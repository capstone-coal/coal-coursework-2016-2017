% assignment: http://eecs.oregonstate.edu/capstone/cs/capstone.cgi?hw=techreview

\documentclass[10pt,draftclsnofoot,onecolumn,journal,compsoc]{IEEEtran}
% for IEEEtran usage, see http://www.texdoc.net/texmf-dist/doc/latex/IEEEtran/IEEEtran_HOWTO.pdf

\usepackage[margin=0.75in]{geometry}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{hyperref}
\usepackage{enumerate}
\usepackage{amssymb}

\renewcommand{\linespread}{1.0}

\title{COAL: Coal and Open-pit surface mining impacts on American Lands}
\author{
  \IEEEauthorblockN{Taylor Alexander Brown, Heidi Ann Clayton, and Xiaomei Wang \\ Group 18} \\
  \IEEEauthorblockA{CS 461: Senior Capstone Fall 2016 \\ Oregon State University}
  % TODO: group number
}
\date{}

\IEEEtitleabstractindextext{
  \begin{abstract}
	Mining is known to cause environmental degradation, but software tools to identify mining impacts are lacking. Researchers studying this problem possess large imaging spectroscopy and environmental quality data sets as well as high-performance cloud-computing resources. This project provides a suite of algorithms using these data and resources to identify signatures of mining and correlate them with environmental impacts over time.
  \end{abstract}
}

\begin{document}

\maketitle
\IEEEdisplaynontitleabstractindextext
\IEEEpeerreviewmaketitle

\newpage

\tableofcontents

\newpage

% Introduction to entire tech review, including authorship of each section
\section{Introduction}
\label{sec:intro}
This document is a review of technology and literature relevant to the COAL project. The technologies discussed in this paper correspond to tasks or subtasks identified in our requirements document \cite{requirements}: 
\begin{itemize}
\item Develop software to process imagery
\item Provide API documentation for developers
\item Provide a website that serves as the project homepage
\end{itemize}
In the past weeks our team has been working with each other and the client to become familiar with the data and the libraries we have selected so we have a more advanced understanding than when we started. However, some material is subject to change as our understanding improves.

Sections \ref{subsec:featureextraction} and \ref{subsec:classification} were authored by Heidi who will take responsibility for the algorithms to identify and classify minerals. Section \ref{subsec:identifymining} was authored by Taylor on behalf of Xiaomei, who will take responsibility for the algorithms to identify regions of mining activity. Sections \ref{subsec:preprocessing}, \ref{subsec:correlate}, and \ref{subsec:changesovertime} were authored by Taylor who will take responsibility for the algorithms to correlate mining with environmental impact and to rank and document changes over time. Section \ref{subsec:apidoc} was written by Heidi who will take responsibility for the API documentation. Sections \ref{subsec:staticsitegenerator} and \ref{subsec:frontendframework} were written by Xiaomei who will take responsibility for implementing the backend and the frontend of the project homepage.

\section{Technologies}

% heidi: requirement 3.4.1 part 1: choose technology for mineral identification and classification
\subsection{Feature extraction in AVIRIS data}
\label{subsec:featureextraction}

\subsubsection{Options}
\begin{enumerate}
\item Spectral Python \cite{spectral}
\item HyperSpy \cite{hyperspy}
\item MATLAB Hyperspectral Toolbox \cite{matlab}
\end{enumerate}
\subsubsection{Goals}
In order to properly analyze the AVIRIS images, COAL needs a way to identify regions in an image that indicate a particular mineral or other pattern. We need to have a tool that can process hyperspectral imagery in order to recognize the spectral signatures of different minerals and land surfaces so that AVIRIS images can be classified. A big part of this is making comparisons to a spectral library in order to identify each pixel. 

\subsubsection{Criteria}
The main criteria being evaluated are if the software library is currently maintained and developed, if the software has explicit support of spectral libraries, and what language is used in the software.

\subsubsection{Comparison}
\begin{tabular}{ | l | l | l | l |}
    \hline
    & Spectral Python & HyperSpy & MATLAB Hyperspectral Toolbox \\ \hline
    Currently maintained & \checkmark & \checkmark & $\times$ \\ \hline
    Has explicit support of spectral libraries & \checkmark & $\times$ & $\times$ \\ \hline
    Language & Python & Python & MATLAB \\
    \hline
\end{tabular} \\ \\
This table shows an overview of the three choices of software libraries against the criteria of being currently maintained, having explicit spectral library support, and the language that is used in the implementation.

\subsubsection{Discussion}
At first glance, it is fairly apparent that the MATLAB Hyperspectral Toolbox does not meet the criteria well at all. It is not currently maintained or developed, with the last update to its GitHub repository being from 2015. There is also a note in the README file that it has been abandoned. MATLAB Hyperspectral Toolbox also does not have explicit spectral library support, which we would like to use to process the spectral signatures. As the name implies, it is also implemented in MATLAB. This is not something that would absolutely stop the team from using a software library, but as the team is more familiar with Python, it is preferred. HyperSpy is a better fit, as it is currently maintained, however it does not have explicit spectral library support built into the library. Spectral Python is the only option that satisfies all criteria. 

\subsubsection{Selection}
We will be using Spectral Python.

\subsection{Classification of AVIRIS data}
\label{subsec:classification}

\subsubsection{Options}
\begin{enumerate}
\item Spectral angle mapper
\item Neural network
\item Gaussian classifier
\end{enumerate}
\subsubsection{Goals}
Before any further data analysis can take place, classifying the AVIRIS data into classes based on mineral concentrations and categories such as vegetation and bodies of water will be needed in order to have variables to correlate (or not) with the environmental effects of mining. 

\subsubsection{Criteria}
The main criteria here are if the classification method can produce mineral classified files and if it is compatible with spectral library data.

\subsubsection{Comparison}
\begin{tabular}{ | l | l | l | l |}
    \hline
    & Spectral angle mapper & Neural network & Gaussian classifier \\ \hline
    Can produce mineral classified files & \checkmark & \checkmark & \checkmark \\ \hline
    Compatible with spectral library data & \checkmark  & $\times$ & $\times$ \\
    \hline
\end{tabular}
\\ \\ This table shows an overview of the three choices of methods to classify the data from the AVIRIS images and spectral library. The only software that meets all of the requirements is the spectral angle mapper.

\subsubsection{Discussion}
All three choices are provided in the Spectral Python library and all three produce mineral classified image files. However, the most important criteria here is that the method can use a spectral library in the classification. Because a spectral library typically has only one sample per mineral, the neural network method and Gaussian method will not work with the way they are implemented in Spectral Python. The spectral angle mapper can use the spectral library as it is with only one sample for most minerals, so it is the best choice.

\subsubsection{Selection}
We will be using the spectral angle mapper for the classification of AVIRIS data.

% xiaomei: requirement 3.4.2: identify mining
\subsection{Identify Mining}
\label{subsec:identifymining}
To identify regions of mining activity, the mineral identification and classification data is further processed and possibly combined with geographic information about geology and mines.

\subsubsection{Options}
Three approaches to identify mining are:
\begin {enumerate}
\item mineral classification, using patterns of mineral deposits \cite{leadville,raymine};
\item GIS comparison, using geographic information such as mining permit boundaries \cite{movingmountaintops} or geologic maps \cite{raymine}; and
\item a combination of mineral classification and GIS comparison, using both sources of data.
\end {enumerate}

\subsubsection{Goals}
The goal of the mining identification step is to accurately locate regions in which mining is taking place in a form that is suitable for both human inspection and further processing. It uses the output of the mineral identification step which located regions containing minerals associated with mining. Because these minerals may be derived from both natural and artificial processes \cite{raymine}, additional logic or data may be necessary to distinguish mines from preexisting geology. Because existing geographic data may be unreliable or incomplete, some combination of mineral classification with geographic information may produce more accurate results.

\subsubsection{Criteria}
Criteria being evaluated include the time needed to research and develop each method, the computational cost of processing the data, and the accuracy of the results.

\subsubsection{Comparison}
\begin{tabular}{ | l | l | l | l |}
    \hline
                           & Research and Development Time & Computational Cost & Accuracy \\ \hline
    Mineral Classification & Low to Medium                 & Low to Medium      & Medium   \\ \hline
    GIS Comparison         & High                          & Medium             & Medium   \\ \hline
    Combination            & High                          & Medium to High     & High     \\
    \hline
\end{tabular}
\newline
\newline
This table compares each option based on the criteria. Because the options are general approaches to algorithm design and not specific libraries, it was necessary to estimate the costs and benefits.

\subsubsection{Discussion}
Mineral classification by itself can be a sophisticated method of identifying traces of mining activity. Minerals with low natural concentrations can be used to identify mining activity, as jarosite was used to map active mining operations in the Ray Mine in Arizona \cite{raymine}. In this case, simply mapping the presence of a mineral may be sufficient to locate mines. In other cases, minerals associated with mining cannot be detected by imaging spectroscopy, such as pyrite in the California Gulch Superfund Site \cite{leadville}. The solution in this case was to search for patterns of secondary minerals weathered from pyrite that are detectable by the sensors. Therefore the mineral classification method can be simple or complex depending on the chemistry of the minerals of interest and the logic required to find them.

GIS comparison can also provide useful insights for identifying mines. One approach that has been used to successfully visualize mining activity is to overlay permit boundaries on top of classified spectroscopy data \cite{movingmountaintops}. This provided a visual comparison of known mines and their spectroscopic signatures. Another approach is to compare geologic maps with the observed minerals and look for discrepancies \cite{raymine}. This allowed naturally-occurring deposits to be distinguished from artificial ones. The comparisons between processed data and known geographic information could be made programmatically or simply displayed for human inspection.

A combination of mineral classification with GIS comparison would provide the most extensive analysis but would require the most research and development time and computational cost. Using both methods would require design choices such as whether to automate or display each result. For example, GIS comparison of geologic maps with observed minerals could be used to automatically eliminate deposits that are thought to be naturally-occurring; alternatively, both the geologic maps and the observations could be combined so that more data is displayed and less is lost. Similar choices would be necessary for mineral pattern detection or permit boundary mapping. Combining multiple data sources would be challenging and could make the results more or less accurate depending on the circumstances.

\subsubsection{Selection}
Based on these criteria, it is recommended to pursue the mineral classification methods first in order to locate regions with mineral deposits that indicate mining. If this can be completed successfully, then evolving a combination of mineral classification and GIS comparison methods would provide an opportunity to improve the analysis.

% taylor: requirement 3.4.3 part 1: preprocessing data to correlate mining with environmental impact
\subsection{Preprocessing Data to Correlate Mining with Environmental Impact}
\label{subsec:preprocessing}
To correlate the regions of mining activity with the regions of environmental degradation, the data sources must be preprocessed to be compatible.

\subsubsection{Options}
Three approaches to make the mining and environmental data compatible are:
\begin{enumerate}
\item conversion of mining activity data to GIS,
\item conversion of environmental impact data from GIS, and
\item conversion of both data sources to a third format.
\end{enumerate}

\subsubsection{Goals}
The goal of the preprocessing step is to transform the input data, assumed here to be spectroscopic data and geographic information, such that the following feature extraction step can find meaningful relationships. Because this is an intermediate step, no data visualization is generated so the formats may be only machine-readable.

\subsubsection{Criteria}% being evaluated (e.g., cost, availability, speed, security, etc)}
Criteria being evaluated include the time needed research and develop each method, the compatibility of the format with existing standards, and the retention or loss of precision produced by each.

\subsubsection{Comparison}
\begin{tabular}{ | l | l | l | l |}
    \hline
                 & Research and Development Time & Standard & Precision        \\ \hline
    To GIS       & High                          & Yes      & Medium           \\ \hline
    From GIS     & Medium                        & No       & Medium to High   \\ \hline
    Third Format & Medium to High                & Maybe    & Medium to High   \\
    \hline
\end{tabular}
\newline
\newline
This table compares each of the possible data formats with estimates for each of the criteria.

\subsubsection{Discussion}
Transforming the spectroscopic data to GIS for combination with the environmental data would produce a standard intermediate format. This format would not necessarily be best suited for the following feature extraction step, and it is speculated that data would inevitably be lost. Furthermore, it would require significant research and development time to transform the processed imagery into a geographical format.

Converting the environmental impact data from GIS so that it is compatible with the processed imagery would generate a nonstandard format, but one that can arguably be better tailored to the application. Research and development time is estimated to be lower for this approach.

Converting both the processed imagery and the geographic information to a third format would provide a choice about compatibility and suitability for further processing. The research and development time for this approach is estimated to be higher.

\subsubsection{Selection}
As we began to design and implement this step, it was concluded that the most straightforward means of combining geographic information and hyperspectral imagery is to convert the GIS data to the same raster format as the imagery. This will facilitate user-defined correlation functions which operate on pixels or regions of each image. The Geospatial Data Abstraction Library (GDAL) handles many standard georeferenced vector and raster file formats, so the output should be broadly compatible.

% taylor: requirement 3.4.3 part 2: feature extraction to correlate mining with environmental impact
\subsection{Feature Extraction to Correlate Mining with Environmental Impact}
\label{subsec:correlate}
To correlate the regions of mining activity with the regions of environmental degradation, relationships must be extracted from the preprocessed data.

\subsubsection{Options}
Having combined the mining and environmental data in the previous step, options for finding meaningful relationships include:
\begin{enumerate}
\item simple logic,
\item statistical analysis, and
\item machine learning approaches.
\end{enumerate}

\subsubsection{Goals}
The goal of this stage is to accurately locate regions where mining coincides with environmental degradation. The result should be suitable for both human analysis as well as further processing.

\subsubsection{Criteria}
Criteria for evaluating these approaches include time needed to research and develop each method, the computational complexity of each, and the accuracy of the results.

\subsubsection{Comparison}
\begin{tabular}{ | l | l | l | l |}
    \hline
                     & Research and Development Time & Computational Cost & Accuracy \\ \hline
    Logic            & Low                           & Low                & Low      \\ \hline
    Statistics       & Medium                        & Medium             & Medium   \\ \hline
    Machine Learning & High                          & High               & Medium to High   \\
    \hline
\end{tabular}
\newline
\newline
This table compares each algorithmic approach with estimates for each of the criteria.

\subsubsection{Discussion}
The simplest way to correlate mining and environmental impact is to identify areas where both are present. Like a Venn diagram, this approach would provide a basic boolean description of regions where both zones intersect. Although it would require very little research and development time to apply logical AND to each pixel, the accuracy is estimated to be low. Furthermore, this method would not be well suited for analyzing results that are probabilistic in nature.

Another way to correlate the data is to use statistical methods to identify zones that are likely to correspond to both mining and environmental data within some level of uncertainty. It would require more research and development to find and implement methods to process the data this way, but the accuracy is estimated to be better. The computational cost of this approach is estimated to be higher.

The most complex way to correlate the data is to use machine learning approaches such as the neural networks which were used to classify the spectroscopic imagery. This would require more research and development time and much more computational complexity than either of the other approaches. It is unknown whether the accuracy of this approach for correlation would exceed that of the statistical method, but it estimated to be as good or better.

\subsubsection{Selection}
Because we are interested in programming the library but not necessarily conducting a detailed environmental analysis, we have decided to pursue the logical approach to environmental correlation as a case study. We plan to identify pixels where waterway pollution corresponds to mining regions. Although our correlation function will be simplistic, our model will enable more sophisticated functions to be developed for environmental research.

% taylor: requirement 3.4.4: rank and document changes over time
\subsection{Rank and Document Changes Over Time}
\label{subsec:changesovertime}
To rank and document changes over time, data from a range of dates should be compared using correlations derived from the previous step.

\subsubsection{Options}
Several approaches to correlating imagery and historical data include:
\begin{enumerate}
\item processing separately and comparing manually,
\item processing separately and comparing automatically, and
\item processing simultaneously and comparing automatically.
\end{enumerate}
Each option describes a high-level approach to designing the algorithm. Implementation details to be determined based on choice.

\subsubsection{Goals}
The goal of this stage is to add a temporal dimension to the spatial correlation between mining and environmental impact. It will require processing multiple historical data sets. The end user will be expected to select environmental data sets and imagery for a given geographical region. These may be processed manually for maximum control or automatically for batch operation. Because this module relies on all previous stages of the data processing pipeline, implementation details remain to be finalized.

\subsubsection{Criteria}
Criteria being evaluated include the time needed to research and develop each method, the computational complexity of each approach, and the quality of the results.

\subsubsection{Comparison}
\begin{tabular}{ | l | l | l | l |}
    \hline
                                                     & Research and Development Time & Computational Cost & Quality \\ \hline
    Process Separately, Compare Manually          & Low                           & High               & Medium      \\ \hline
    Process Separately, Compare Automatically     & Medium                        & High               & Medium to High   \\ \hline
    Process Simultaneously, Compare Automatically & High                          & Very High          & Medium to High   \\
    \hline
\end{tabular}
\newline
\newline
This table compares each approach to algorithm design and the estimated costs and benefits of each.

\subsubsection{Discussion}
The simplest approach is to process the data separately and compare the results manually. Because this simply runs the previous stages over multiple data sets, the research and development time is low. However, the quality of this approach is not high because the end user shoulders responsibility for comparing the results.

A more sophisticated design is to devise a means of combining multiple correlations and producing a unified result. This approach would reuse most of the original data flow and provide a comparison of the results. If variables such as geographic coordinates and atmospheric conditions could be successfully postprocessed by this method, the quality of the results is estimated to be better than the previous approach.

The most complicated approach would be to process multiple historical data sets simultaneously to mediate their transformations. This would require more research and development and likely some degree of refactoring. All of these approaches require a high computational cost, but this one is estimated to have the highest. However, if successfully implemented, this method is estimated to have the highest quality because each of the data sets is available for synchronization from beginning to end.

\subsubsection{Selection}
We now expect to implement a combination of the options discussed above. The user will be required to select the data for analysis. Then the data could be manually processed for maximum control or batch processed for convenience. Thus we expect to facilitate automated comparisons with both separate and simultaneous processing.

% heidi: requirement 3.5: api documentation
\subsection{API Documentation}
\label{subsec:apidoc}

\subsubsection{Options 1, 2, and 3}
\begin{enumerate}
\item Sphinx
\item Doxygen
\item Epydoc
\end{enumerate}

\subsubsection{Goals}
Auto-generated documentation is desirable in the design on COAL 
because there will be code used to initialize data sets from large data files, process said data, and visualize such data. These are complex tasks that will not have the most straight-forward and simple methods of implementation, so clear, comprehensive, and organized documentation would be helpful to anyone who potentially wants to use COAL on a cloud-based platform in the future.

\subsubsection{Criteria}
The main criteria being evaluated when comparing documentation generators is compatibility with Python 2 and Python 3, search engine integration, and use of reStructuredText.

\subsubsection{Comparison}
\begin{tabular}{ | l | l | l | l |}
    \hline
    & Sphinx & Doxygen & Epydoc \\ \hline
    Python 2 and 3 support & \checkmark & \checkmark & $\times$ \\ \hline
    Search engine & \checkmark & \checkmark & $\times$ \\ \hline
    reStructuredText & \checkmark & $\times$ & $\times$ \\
    \hline
\end{tabular}
\\ \\ This table shows an overview of Sphinx, Doxygen, and Epydoc against the criteria. Sphinx meets all three criteria, Doxygen misses the mark on reStructuredText, and Epydoc does meet any of the criteria.

\subsubsection{Discussion}
Epydoc is a widely used documentation generator for Python code. However, it was abandoned in 2009
and therefore does not have Python 3 support. It also does not offer a search engine feature, which is 
an incredibly handy feature to have when trying to find a method or class that meets a certain criteria. It also
uses Markdown instead of reStructuredText, making it fall behind in the ease of use and elegance department. Doxygen fits more of the criteria, but it is a bit tricky since Doxygen by itself does not
support Python documentation. It only does so through the use of a third-party generator like Epydoc. Sphinx fits all of the criteria and is the only choice that uses reStructuredText. reStructuredText provides mechanisms known as roles and directives that will render parts of the documentation to HTML automatically, making the documentation cleaner by not having to embed HTML, which is a common experience in Markdown \cite{nomarkdown}. 


\subsubsection{Selection}
We will be using Sphinx for the auto-generated documentation.

% xiaomei: requirement 3.7 part 1: website backend
% e.g., plain html, static site generators like jekyll, etc.
\subsection{Static site generator}
\label{subsec:staticsitegenerator}

\subsubsection{Options}
\begin {enumerate}
\item Middleman 
\item Jekyll
\item Roots
\end {enumerate}

\subsubsection{Goals}
A static site generator will provide a fast and simple back-end for COAL's homepage. The homepage will not be dependent on real-time content, so we deemed a static site generator appropriate for the back-end.

\subsubsection{Criteria}
\begin {enumerate}
\item Cost to build.
\item Availability: easy to attain.
\item Compatibility
\end {enumerate}

\subsubsection{Comparison}
\begin{tabular}{ | l | l | l | l |}
    \hline
    & Middleman & Jekyll & Roots \\ \hline
    Easily extendable & $\times$ & \checkmark & \checkmark \\ \hline
    Built-in Github support & $\times$ & \checkmark & $\times$ \\
    \hline
\end{tabular}

\subsubsection{Discussion}
Jekyll is defined as “a simple, blog-aware, static site generator” and it is the most widely used today. Jekyll is also very easy to use. It has default integration with GitHub pages, which makes set-up and updates very simple. Consider what we are going to do right now: the blog post page, and compatible with GitHub pages. Jekyll is an ideal choice.

Middleman is not as widely used. Middleman does not have built-in support with GitHub pages, so it is more difficult to set up and update compared to Jekyll. Middleman is also difficult to write extensions for as it is not well documented. Roots is less used than Middleman, but it relies heavily on extensions and is easily extendable \cite{staticsite}.

\subsubsection{Selection}
Jekyll seems to be the best choice after our discussion. Since we are using GitHub pages, it will be the simplest to use and fits our needs.Because one of our need for our website is to create a page for blog posting, so we intend to use Jekyll mainly for that function, which is considered very convenient. 

% xiaomei: requirement 3.7 part 2: website frontend
% e.g., plain stylesheets, frameworks like bootstrap, etc.
\subsection{Front-end framework}
\label{subsec:frontendframework}

\subsubsection{Options}
\begin {enumerate}
\item Bootstrap
\item Skeleton
\item Foundation
\end {enumerate}

\subsubsection{Goals}
Our site should look professional and have a simple, user-friendly interface that is also aesthetically pleasing. In order to accomplish this, a front-end framework is desirable.

\subsubsection{Criteria}
\begin {enumerate}
\item Well designed interaction
\item Professional-looking Interface 
\item Contains required contents
\end {enumerate}

\subsubsection{Comparison}
\begin{tabular}{ | l | l | l | l |}
    \hline
    & Bootstrap & Skeleton & Foundation \\ \hline
    Abundance of themes & \checkmark & $\times$ & \checkmark \\ \hline
    Highly mobile friendly & \checkmark & \checkmark & \checkmark \\ \hline
    Large community support & \checkmark & $\times$ & \checkmark \\
    \hline
\end{tabular}

\subsubsection{Discussion}
Skeleton is fairly bare-bones, so it isn't ideal for when developers want many themes to choose from \cite{bootfoundskeleton}. Skeleton is also a much less active project. At the time of writing this document, the last update made on the official GitHub page for Skeleton was made in December of 2014. Bootstrap and Foundation are both very popular and active projects that compare pretty evenly. Bootstrap has a slightly different approach to the mobile version of a site than Foundation does. Foundation was developed under the assumption that "anything not under a media query is considered mobile \cite{bootstrapvsfoundation}." Bootstrap will only design something for mobile if specified. This is not very important to COAL's homepage though. Both are compatible with Jekyll, our choice for a static site generator. The deciding factor between Bootstrap and Foundation comes down to the preferences and experiences of the team members. We have experience with Bootstrap and enjoy the look and feel of it, so it is our choice for COAL's homepage's front-end.

\subsubsection{Selection}
We will be using Bootstrap for the front-end framework of COAL's homepage.

\section{Conclusion}
In this paper we described and compared technologies for implementing the COAL algorithm suite and associated documentation. The core functionality consists of a data processing pipeline that transforms raw imagery and data into meaningful relationships. We have become familiar with relevant literature, developed detailed sketches of our algorithms, and implemented a framework for most of the modules using the libraries we have chosen. In addition to the algorithms, our project will be supported by API documentation for programmers and a homepage for end users. A discussion of implementation choices for each component was provided along with a recommendation based on a comparison of costs and benefits. The team members responsible for each component were identified in the introduction \ref{sec:intro}. We hope that this discussion has provided the reader with a better understanding of the design choices that have been and have yet to be made as we continue to collaborate with our client on research and development.

\section{Revisions}
\label{sec:revisions}
\subsection{Feature extraction in AVIRIS data}
Revisions to Section \ref{subsec:featureextraction}:
\subsubsection{Revision 1}
Scrapped blob detection methods and compared neural network libraries.

\subsubsection{Revision 2}
Scrapped neural networks, compared explicit support of spectral libraries.

\subsection{Classification of AVIRIS data}
Revisions to Section \ref{subsec:classification}:
\subsubsection{Revision 1}
Rewrote comparison to include other neural net libraries used to classify. 

\subsubsection{Revision 2}
Scrapped neural network libraries and compared classification methods in Spectral Python.

\subsection{Identify Mining}
Revisions to Section \ref{subsec:identifymining}: .

\subsection{Preprocessing Data to Correlate Mining with Environmental Impact}
Revisions to Section \ref{subsec:preprocessing}: Revised to document our choice to convert GIS to raster data.

\subsection{Feature Extraction to Correlate Mining with Environmental Impact}
Revisions to Section \ref{subsec:correlate}: Revised to describe the simple case study chosen by client.

\subsection{Rank and Document Changes Over Time}
Revisions to Section \ref{subsec:changesovertime}: Revised to describe expectation that end user will choose data sets, and updated the selection to be a combination of manual and batch processing.

\subsection{API Documentation}
Revisions to Section \ref{subsec:apidoc}: .

\subsection{Static site generator}
Revisions to Section \ref{subsec:staticsitegenerator}: .

\subsection{Front-end framework}
Revisions to Section \ref{subsec:frontendframework}: .

\begin{thebibliography}{9}
% for IEEE citation reference, see https://www.ieee.org/documents/ieeecitationref.pdf
\bibitem{requirements}
  T. A. Brown, H. Clayton, and X. Wang, Requirements document, Fall 2016.
\bibitem{spectral}
\url{http://www.spectralpython.net/}

\bibitem{hyperspy}
\url{https://github.com/hyperspy/hyperspy}

\bibitem{matlab}
\url{https://github.com/isaacgerg/matlabHyperspectralToolbox}

\bibitem{leadville}
  G. A. Swayze \emph{et al}, ``Using Imaging Spectroscopy to Cost-Effectively Locate Acid-Generating Minerals at Mine Sites: An Example from the California Gulch Superfund Site in Leadville, Colorado,'' paper presented at JPL Airborne Geoscience Workshop, Leadville, Colorado, 1998.

\bibitem{raymine}
  R. N. Clark, \emph{et al}, ``\href{http://speclab.cr.usgs.gov/PAPERS/ray.mine.1.1998/ray.mine.avproc.html}{Mineral Mapping with Imaging Spectroscopy: The Ray Mine, AZ},'' Summaries of the 7th Annual JPL Airborne Earth Science Workshop, R.O. Green, Ed., JPL Publication 97-21. Jan 12-14, pp67-75, 1998.

\bibitem{movingmountaintops}
  D. Nally, ``Moving Mountaintops: Monitoring Surface Mine Expansion and Reclamation Using Landsat Imagery,'' Tufts Univ., Spring 2011.

\bibitem{nomarkdown}
  E. Holscher. (2015, March 15). \emph{Why You Shouldn't Use ``Markdown'' for Documentation} [Online]. Available: \url{http://ericholscher.com/blog/2016/mar/15/dont-use-markdown-for-technical-docs/}

\bibitem{staticsite}
  M. Christensen. (2015, November 16). \emph{Static Website Generators Reviewed: Jekyll, Middleman, Roots, Hugo} [Online]. Available: \url{https://www.smashingmagazine.com/2015/11/static-website-generators-jekyll-middleman-roots-hugo-review/}
  
\bibitem{bootfoundskeleton}  
\emph{Best CSS Frameworks - Bootstrap vs Foundation vs Skeleton?} [Online]. Available: \url{http://customwebsitedevelopement.blogspot.com/2016/01/best-css-frameworks-bootstrap-vs-foundation-vs-skeleton.html}

\bibitem{bootstrapvsfoundation}
  M. Schenker. (2014, September 15). \emph{Bootstrap vs. Foundation: Which Framework is Better?} [Online]. Available: \url{https://bootstrapbay.com/blog/bootstrap-vs-foundation/}
  
\bibitem{dimensionality}
  V. Catterson. (2013, November 27). \emph{Understanding data science: dimensionality reduction with R} [Online]. Available: \url{http://cowlet.org/2013/11/27/understanding-data-science-dimensionality-reduction-with-r.html}

\end{thebibliography}

\end{document}

