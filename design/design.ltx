% assignment: http://eecs.oregonstate.edu/capstone/cs/capstone.cgi?hw=design

\documentclass[10pt,draftclsnofoot,onecolumn,journal,compsoc]{IEEEtran}
% for IEEEtran usage, see http://www.texdoc.net/texmf-dist/doc/latex/IEEEtran/IEEEtran_HOWTO.pdf

\usepackage[margin=0.75in]{geometry}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{hyperref}
\usepackage{enumerate}
\usepackage{amssymb}
\usepackage{pgfgantt}
\usepackage{amsmath}

\renewcommand{\linespread}{1.0}

\title{COAL: Coal and Open-pit surface mining impacts on American Lands}
\author{
  \IEEEauthorblockN{Taylor Alexander Brown, Heidi Ann Clayton, and Xiaomei Wang \\ Group 18} \\
  \IEEEauthorblockA{CS 461: Senior Capstone Fall 2016 \\ Oregon State University}
}
\date{}

\IEEEtitleabstractindextext{
  \begin{abstract}
	Mining is known to cause environmental degradation, but software tools to identify mining impacts are lacking. Researchers studying this problem possess large imaging spectroscopy and environmental quality data sets as well as high-performance cloud-computing resources. This project provides a suite of algorithms using these data and resources to identify signatures of mining and correlate them with environmental impacts over time.
  \end{abstract}
}

\begin{document}

\maketitle
\IEEEdisplaynontitleabstractindextext
\IEEEpeerreviewmaketitle

\newpage

\tableofcontents

\newpage

\section{Introduction}
\label{sec:intro}
This is the design document for COAL. It outlines the design of our system as well as the research and development priorities at each step. The COAL suite of algorithms constitutes a pipeline of data processing and presentation, and is accompanied by API documentation as well as a website for end users and the general public. Specific implementation details remain to be determined based on research, but in general the program reduces a large set of hyperspectral and geographic data into comprehensible conclusions about mining and environmental impact in human and machine-friendly formats.

Each section was authored by the corresponding team member responsible for each component identified in the technology review: Sections \ref{subsec:featureextraction} and \ref{subsec:classification} by Heidi; section \ref{subsec:identifymining} by Xiaomei; sections \ref{subsec:preprocessing}, \ref{subsec:correlate}, and \ref{subsec:changesovertime} by Taylor; section \ref{subsec:apidoc} by Heidi; and sections \ref{subsec:staticsitegenerator} and \ref{subsec:frontendframework} by Xiaomei. The research priorities and design choices for each part of the project are discussed in each section.

\section{Technologies}

% heidi: requirement 3.4.1 part 1: choose technology for mineral identification and classification
\subsection{Feature extraction in AVIRIS data}
\label{subsec:featureextraction}
The process of feature extraction will be done using Spectral Python and spectral libraries. Spectral Python can be installed via Pip or directly from the GitHub repository. The only dependency is NumPy. Spectral Python works with Python 2 and is compatible with Python 3 minus the functions "view\_cube" and "view\_nd", which will not be used in COAL. All minerals have their own unique spectral signature, or relationship between wavelength and reflectance, various samples of which are found in spectral libraries. These will then be used for comparison on each pixel in the classification \ref{subsec:classification} section.

COAL will support using any spectral library that is in the ENVI file format. These can then be passed to the feature extraction and classification modules in COAL to be opened using the "open\_image" function in Spectral Python. This function is relatively general-purpose and will detect what class will represent the file. The spectral library in ENVI format should be detected as a SpectralLibrary object. There will also be an option to only use a subset of the mineral classes in the spectral library. 

The main spectral library that will be used in the case study is the United States Geological Survey (USGS) Digital Spectral Library 06. This spectral library is already provided in ENVI format on an FTP server by USGS. The USGS library has 1365 classes provided with one sample per class. The spectral signatures are found in the "spectra" attribute and the names of the classes are found in the "names" attribute. Classes of interest provided in the USGS spectral library are the "Renyolds\_TnlSldgWet SM93-15w" and "Renyolds\_Tnl\_Sludge SM93-15" classes. These represent sludge produced by coal mining. Other classes may also end up being of interest as the case study goes on.

Along with the USGS Digital Spectral Library 06, COAL should be able to support the ASTER Spectral Library, a compilation of various spectral libraries including USGS provided by JPL. The ASTER Spectral Library is made up of 2400 classes which are taken from the Johns Hopkins Spectral Library, the Jet Propulsion Laboratory Spectral Library, and the USGS Digital Spectral Library 06. What makes using the ASTER library tricky is the format in which it is provided. ASTER is provided as a folder full of ASCII text files with one per mineral class. Luckily, ASTER can be converted into ENVI format using Spectral Python using a SQLite file.

First, a directory containing the ASTER files must be provided along with the name the user wishes to call the generated SQLite file. Then, the "create" method of Spectral Python's "AsterDatabase" class can be used to generate a SQLite file containing all of the information on the individual mineral classes in the ASCII files. The SQLite file will then be queried for the names of the mineral classes and a "BandInfo" object will be created using the wavelength range and number of layers per pixel for an AVIRIS image. These will then be used with the "AsterDatabase" class' "create\_envi\_spectral\_library" method to create an ENVI spectral library.





% heidi
\subsection{Classification of AVIRIS data}
\label{subsec:classification}
After obtaining a functioning spectral library in the feature extraction step, the next step is mineral classification. Essentially, this means that each pixel of the provided AVIRIS image will be analyzed and assigned to the most likely match in the provided spectral library.

First, the AVIRIS image to be classified will be opened with Spectral Python's "open\_image" function. Then, the image will be loaded into memory if the machine COAL is being run on has enough memory to do so. This will increase the efficiency of the classification process, which can take a long time. The bands of the image will then be resampled with the spectra of the spectral library. This will basically normalize the data and take care of discrepancies with units. The discrepancies with units can render the results of the classification process unusable, so this step is very important. 

After resampling the spectral bands, an empty M x N NumPy array will be created, with M and N representing the length and width of the original AVIRIS image. Then, the image will be looped through and each pixel will be processed. The first thing to consider when looking at a pixel is if that pixel actually has data or not. AVIRIS flightlines are surrounded by many "No data" pixels that are represented by either "-.005" or "-50" in their spectral bands. "-.005" is used in images taken by the original AVIRIS device and “-50” is used in images used by AVIRIS-NG, or AVIRIS New Generation.

The pixel will be resampled to get rid of "NaN" values from target bands that do not overlap. Next, the pixel will be run through Spectral Python's "spectral\_angles" function. This computes the angle between the pixel’s spectra and each class in the spectral library. This is where most of the processing time will come from.

Once the list of spectral angles are computed, the argmax will be taken to find the class in the spectral library that most closely matched the current pixel. This can be done easily by using NumPy's "argmax" function. Then, the class number that most closely matches the pixel will be written to the above mentioned M x N NumPy array. One functionality that will be added is the ability to set a threshold for the spectral angles. That is, the ability to set a minimum confidence value for the pixel classification. If the class that most closely matches the pixel has a very low confidence value, it should be written as a "No data" value.

After the all pixels in the image have been successfully processed, the classified array will be saved to a file using the "save\_classification" method from Spectral Python's "ENVI" class. Then, that file can be run through a filter to get rid of all unused classes from the spectral library to clean up the data. Once the data has been classified, it can be further analyzed by the mining identification and environmental correlation steps and opened in a GIS program to get a full visual of all classifications. 


% xiaomei: requirement 3.4.2: identify mining
\subsection{Identify Mining}
\label{subsec:identifymining}

After we get the mineral identification and classification data, we need to identify the regions of mining activity. The data is further processed and possibly combined with geographic information about geology and mines. The goal of identifying mining is to accurately locate regions in which mining is taking place in a form that is suitable for both human inspection and further processing. This step creates input data for further processing of the project and the data evaluation.

To start our process of identifying mining, we need the output from the mineral identification step. The previous step has already located the regions that are containing mineral associated with mining. To distinguish mines from preexisting geology is a challenge; only rely on the existing geography data is not enough for the identification.

There are three methods mentioned after several group discussion sessions for identifying mining: mineral classification, using patterns of mineral deposits; GIS comparison, using geographic information such as mining permit boundaries or geologic maps; and a combination of mineral classification and GIS comparison, using both sources of data. To locate regions with mineral deposits that indicate mining, we have decided that we are going to use the mineral classification methods. Then, we would move to a combination of mineral classification and GIS comparison methods once we are able to complete the first step without much trouble.  

The mineral classification method can be very simple, mapping the presence of a mineral may be sufficient to locate mines: minerals with low natural concentrations can be used to identify mining activity. In this case, the result is expected to be straightforward, and the time spending on the identification is short. However, the outcomes are not always favorable under this method. Still, at the very beginning, we expect to rely on this to produce some useful outputs for further application. The combination of the mineral classification and GIS comparison can be costly; it would require the most research and time. Although the outcome can be most complete, so that is why we use mineral classification method at the beginning; simply mapping the presence of a mineral may be sufficient to locate mines due to minerals with low natural concentrations can be used to identify mining activity. GIS comparison can provide more accurate results. 

% taylor: requirement 3.4.3 part 1: preprocessing data to correlate mining with environmental impact
\subsection{Preprocessing Data to Correlate Mining with Environmental Impact}
\label{subsec:preprocessing}

After mines have been identified, we are interested in analyzing their impact on the environment. Using environmental quality GIS data from agencies such as the Environmental Protection Agency (EPA), United States Geological Survey (USGS), United States Bureau of Reclamation (USBR), and United States Department of the Interior (DOI), we would like to enable research into the environmental, health, and social impacts of mining on watersheds. This stage depends on classified mining data from the preceding stage.

The principal problem is to combine the spectroscopic-derived data with geographic data. Preprocessing this data will require using the metadata from the imagery to associate pixels with geographic coordinates. It will also require loading and handling geographic data. We have found that the Geospatial Data Abstraction Library (GDAL) provides a comprehensive toolkit for processing georeferenced vector and raster data. The team is in the process of identifying a suitable data set for environmental correlation as well as geographic information on waterway boundaries such as NASA's Global Reservoir and Dam (GRanD) collection.

To implement preprocessing, we must first combine the environmental data with the waterway boundaries. This will provide a data set of environmental measurements on watersheds. We must then identify the layer or layers we are interested in analyzing. Once we have combined the waterway boundaries with the environmental data and selected the relevant layer or layers, the data must be cropped to the boundaries of the AVIRIS flightline and converted to a compatible raster format.

Some of these procedures have already been implemented by GDAL, others we are implementing by hand in our module for GIS processing. The GDAL library handles most of the basic GIS operations such as combining the environmental data with waterway boundaries and choosing the desired layer. It was necessary to create a helper function to crop a vector source data set to the dimensions and format of a raster destination data set. It was also necessary to create a helper function that initializes an empty data set to the dimensions of an existing image. These will be called by the subsequent stage to perform the actual correlation.

Ensuring that the input and output of our program is consistent is best accomplished by automated testing. It would be possible to apply test-driven development and write tests to specify program behavior, but we may find it more natural to implement procedures and tests alternately. At the end of the project, our tests will serve as an informal specification and an assurance of correctness. Unit tests have not currently been implemented.

Preprocessing data to correlate mining with environmental impact thus depends on a sequence of steps. We have been researching candidate data sets, using the GDAL library to perform GIS operations programmatically, and implementing helper functions to prepare the geographic data with the AVIRIS imagery in the following stage. Ensuring that these functions meet our requirements will involve automated tests. The end product will be preprocessed geographic information that can be used to make correlations between mining and environmental impact.

% taylor: requirement 3.4.3 part 2: feature extraction to correlate mining with environmental impact
\subsection{Feature Extraction to Correlate Mining with Environmental Impact}
\label{subsec:correlate}
Given preprocessed geographic information and mining-classified imagery, we would like to facilitate correlations between mining and environmental impact. For our case study we would like to identify regions where mining overlaps with water pollution. The result will be a georeferenced raster file which can be used to print maps or to generate more sophisticated analyses. Automated tests will help enforce our assumptions about these procedures, but data that is generated will have to be analyzed in person and adjusted according to utilitarian as well as aesthetic preferences to meet the needs of the research task at hand.

To derive the results, we will allow the application programmer to define a function that combines geographic data with imagery to produce an output. Our case study simply overlays the data sets. To do this, we plan on comparing each mine-classified pixel with the corresponding water pollution pixel to generate a resulting map of pixels where the two conditions overlap. This is a simplistic model and not a true correlation, but it should suffice to allow us to develop and test our library. Environmental data scientists will be able to define more sophisticated functions over the images.

This module will call upon procedures developed in the previous step as well as libraries for processing hyperspectral imagery and geographic information. We anticipate to continue using Spectral Python and GDAL to handle the data, although we may need to turn to additional libraries to facilitate the kind of analysis researchers expect in practice. The libraries we are using are relatively mature, however we have a GitHub Organization in the event that we need to patch third-party libraries. One such patch would be to add GPU acceleration to the machine-learning algorithms included with Spectral Python in order to optimize training and classification, such as by reimplementing the perceptron classifier with the OpenCV library. The GDAL library generates formats compatible with many GIS programs, so we may leave the data visualization up to the user.

The routines for processing geographic data developed in the preceding module are not clearly separated from some of the processing that occurs during the correlation stage. Currently the combine function is implemented in the GIS code and called by both the environment code and the prototype mining identification code. Thus our original intuition that the modules may be separated appears to be justified by the current evolution of the software.

The output formats may include maps and charts, but as mentioned these may be left to the user for post-processing. The GDAL library provides broad compatibility with multiple geographic information formats and thus our code could be extended to support arbitrary formats. The simplistic analysis that we are pursuing as a case study may not lend itself to the sorts of complicated diagrams that we imagined in previous iterations, however environmental data scientists who make use of our code could post-process the output for more sophisticated analyses. In general we expect the output of this module to be a raster file corresponding to the AVIRIS flightline in question.

Automated tests will help ensure the correctness of parts of the program, but the end result will be best analyzed in person. Components that could be tested include those responsible for converting input formats to a predictable binary output format. Sample data can be provided and tested to ensure that results are consistent with expectations. However, the overall usefulness and quality of the data visualizations must be judged subjectively. This may require loading the resulting imagery into GIS applications or other visualization programs in order to get a sense for the shape of the data.

Deriving correlations between mining and economic impacts is the purpose of this feature extraction stage. We have documented and begun to implement a set of algorithms for combining the data sets in an extensible way. The results will be in standard formats suitable for display in GIS applications or for post-processing. Automated tests will aid development, but the quality of the results will also depend on subjective determinations.

% taylor
\subsection{Rank and Document Changes Over Time}
\label{subsec:changesovertime}
Ranking and documenting changes over time is the final step in the data processing pipeline. After identifying minerals, mines, and associated environmental impacts, further analysis is possible by providing a historical context to environmental changes. Implementation of this step could be as simple as instructing the end user to run the program several times over separate data sets, however this would not provide effective usability. A more sophisticated implementation would take as input a historical data set and generate trends automatically. The user will be expected to select a set of images and environmental data sets to examine, and our procedure would automate the analysis. As noted in the preceding section, geographic information formats with broad compatibility allow the user to post-process data as desired.

Before development on this step can begin, the preceding stages of data processing must be complete. The design choices and implementation details made at each step will affect the options available for temporal analysis. Data formats and representations will influence the kinds of results that can be produced. Program structure will determine how reusable certain software components are, a consideration that will be important when combining them to analyze changes over time.

The simplest interpretation of documenting changes over time is to process historical data and generate representations for each time period separately. A note in the user documentation would describe how to execute the program over multiple data sets. As mentioned, the user would be expected to select images and data which are temporally related. The user would have full power to customize the processing step, but for simplicity and usability we may like to implement a means of automating this processing.

More sophisticated methods could be implemented to relieve the burden of repetitive tasks at the cost of additional research and development time. For example, this module could receive arguments or configuration files containing multiple data files and run the analyses automatically. It could even be configured to process new data as it is received from the instruments on an ongoing basis. Designing the program with automation in mind would allow new options for output such as animations and generating temporal graphs which would not be possible if correlations are made in isolation. We expect the output format to be sets of georeferenced raster files containing the results. Post-processing these files could be implemented, for example to generate animations, however it may be more natural to leave this up to the environmental data scientists whose research applications we may not be able to anticipate.

We now expect that this module will simply call upon the previous modules in the pipeline in order to provide automation. Additional third-party components could be used if we implement more sophisticated output processing. However, compatibility with existing GIS applications may satisfy the needs of the end user.

The research and development of this module will produce results which cap off the suite of data processing algorithms provided by COAL. Ranking and documenting environmental changes over time combines the information from all earlier steps into a unified whole for producing temporal analyses to inform environmental data scientists and other interested parties. Design and implementation could be simple, but more sophisticated approaches could be devised to enhance usability with automation and post-processing.

% heidi: requirement 3.5: api documentation
\subsection{API Documentation}
\label{subsec:apidoc}
The Sphinx API documentation will be developed throughout the entirety 
of the project, with each team member documenting their code 
contributions as they develop. The code written will include 
docstrings for all functions that will include, at minimum, a short 
description of the purpose of the function, description of each 
parameter, and the input and output of the function. Further 
docstrings in the code will include explanations of algorithms and 
structures. We will then use reStructuredText (.rst) files to 
integrate any function descriptions with further elaboration and any 
examples, figures, and/or images. We will have a main page for the 
documentation that includes a table of contents, a search bar, and a 
general description of the API. We will also have a page regarding how 
to use COAL's API and a page for each library. The goal of the 
documentation is to be useful to a person who wants to use it in their 
software project but does not necessarily want to know everything that 
is going on in explicit detail. It is a description of the API as a 
black box for the most part, though the source code is available to 
those who want more detail and is linked on the homepage 
\ref{subsec:frontendframework}.

The main page has an auto-generated table of contents and search bar, 
so the only thing we will be writing on the main page is the general 
description of the API. The page regarding using the COAL API will 
include information on dependencies, compatibility with AVIRIS 
imagery, and examples showing how to get started with COAL. Each page 
documenting a library will have a short description of the library at 
the top, followed by a description of each function (documented in the 
code itself with docstrings). If applicable, any concerns regarding 
compatibility with other libraries or performance should be addressed 
here. This is particularly important since image processing takes a lot 
of computing power. The specifics of algorithms and statements are not 
necessary for the auto-generated documentation. We will put comments 
in the source code where necessary for people who want more detail, 
but the API documentation will only include specifics if they raise 
any kind of concern.

Testing will be relatively simple to make sure the API documentation 
is working correctly. Syntax errors are raised by Sphinx if they are 
encountered, so they are easy to check for. We will also be doing 
spell checking on all of our documentation to make sure it is clear 
and professional. We will also check to make sure that the 
documentation is output in a place that is accessible to users. 

Members of this team have used Sphinx documentation before and found 
it elegant, flexible, and easy to read and write. It is also the 
method that Python uses for its documentation, which is frequented by 
members of this team and is seen as nicely formatted and organized. 
Sphinx is also compatible with Python 2 and Python 3, which makes it a 
good match for us if we ever decide we want to switch completely to 
Python 3. 

% xiaomei
\subsection{Static site generator}
\label{subsec:staticsitegenerator}

We will use a static site generator to provide a fast and simple back-end for COAL's homepage. GitHub let users create one site per GitHub account or organization, which is convenient for us due to that we have to keep the page's contents dynamically.

What we will do first is create a GitHub page. The GitHub page can host personal, organization, or project pages directly from a GitHub repository. GitHub pages use github.io domains which are served over HTTPS. The master branch of the repository is responsible for the publishing; this can be selected from the GitHub Pages site repository under the settings option. Once we have it linked, we will start to build our website. The advantage of the static site generator is that it does not need to deal with databases. The static site generator mostly works with JavaScript and HTML and that is why it has fast speed and relatively better performance. The main purpose of our website is to display the resources we need and make it convenient for our team to perform our tasks, so a static site generator is the perfect choice for us. Our site will not require many complicated functionalities, so a more complex back-end is not needed. Also, because we need to change the content on the home page often as the project continues, static site generators tend to have excellent version control for the content. 

As the technology review states, we are going to use Jekyll as our static site generator. Jekyll is a simple, blog-aware, static site generator perfect for personal, project, or organization sites. We chose Jekyll as our static site generator because it has high compatibility with GitHub pages, which makes the work easier to do. To start to use Jekyll, we could simply install it using the command line provided by the official website. Once we get the resources we need, we will create the back end of our home page. We do not have any experience with using Jekyll, but the guide on the Jekyll website is simple to follow. If we do not need to run Jekyll on the local computer, we could only build the directory structure. The existing of the GitHub page makes our job easier. Right now, we only use Jekyll to organize our directory and make our blog post function easier to implement.

Overall, using a static site generator is not a difficult task and there are a lot of resources online. Choosing a static site generator will save us time with maintenance and server resources.


% xiaomei
\subsection{Front-end framework}
\label{subsec:frontendframework}

Due to the continuous nature of our project, we will need a website to show our work and progress both throughout the school year and possibly beyond. There will not be much content to put on the site toward the beginning of research and development, but the content will be filled out as the project progresses. The website serves the purpose of showing our work and presenting our project in a clear and succinct way.

We use plain HTML to build the skeleton of the website, then we put the necessary information on, such as links to our GitHub pages, links to our project description that is located on our course website, the pages to put all of our papers, and links to our developers' GitHub page. All of those links are available on the navigation bar, so it is convenient for us to access them. We will be using a template for a navigation bar that she has used before so that sh can build a simple page before filling everything inside. Also, we put an introduction video on the front page.

Then, based on what is required, or what we want to put on our website later, we may need to alter the content. For the styling, we will use CSS. Our team has discussed that we would mostly use Bootstrap for our front-end framework, for the abundance of its themes and functionalities that we might need later in the project development. 

Bootstrap is easy to use and works well with HTML, CSS, and JS; we choose it because of it saves us time so we can build a basic page very quickly. We also need some pictures related to mining or AVIRIS to make our site more aesthetically pleasing and to provide interest. Taylor has provided one earlier. Due to the reason that most of the images are too colorful to use as a background page for a website, I decide to make it more transparent so it can fit the other elements of the site.

As the project continues, we will update our homepage on a weekly basis as well as the link to our individual weekly updates for required for the computer science senior design course.

\section{Timeline}
The Gantt chart pictured in Figure 1 describes the project timeline. Dependencies among each stage of the project are depicted by arrows and approximate implementation dates are indicated by the date range. Some components may be able to be developed independently while others may require a sequential approach. In general, however, our development will follow the data from initial processing to final representations. API documentation will parallel the development of our source code using comments to automatically generate browsable reference material. The website will provide a hub for developers, end users, and the general public describing our project, and its development will be relatively independent of the algorithmic components.

% academic calendar: http://catalog.oregonstate.edu/ChapterDetail.aspx?key=148
% engineering calendar: http://eecs.oregonstate.edu/industry-relations/calendar
% course timeline: http://eecs.oregonstate.edu/capstone/cs/capstone.cgi?home=1

\newcommand{\firstdayoffallterm}{2016-09-21}      % first day of fall term
\newcommand{\startday}{2016-10-02}                % day groups assigned
\newcommand{\fallprogressreportdue}{2016-12-05}   % finals week of fall term
\newcommand{\alphareleasedue}{2017-02-13}         % week 6 of winter term
\newcommand{\betareleasedue}{2017-03-20}          % finals week of winter term
\newcommand{\winterprogressreportdue}{2017-03-20} % finals week of winter term
\newcommand{\releasedue}{2017-05-15}              % monday prior to tentative expo date
\newcommand{\expoday}{2017-05-19}                 % tentative expo date
\newcommand{\finalreportdue}{2017-06-12}          % finals week of spring term
\newcommand{\lastdayofspringterm}{2017-06-16}     % last day of spring term

\begin{figure}

  % gantt chart: http://mirrors.rit.edu/CTAN/graphics/pgf/contrib/pgfgantt/pgfgantt.pdf
  \begin{ganttchart}[x unit=0.15em, time slot format=isodate, link bulge=4]{\startday}{\lastdayofspringterm}

    % gantt chart title
    \gantttitlecalendar{year, month=shortname} \\

    % gantt chart bars
    \ganttbar[name=mineralid]{Feature Extract Mineral Identification}{\fallprogressreportdue}{\alphareleasedue} \\
    \ganttbar[name=mineralidclass]{Classification Mineral Identification}{\fallprogressreportdue}{\alphareleasedue} \\
    \ganttbar[name=miningid]{Identify Mining}{\alphareleasedue}{\betareleasedue} \\
    \ganttbar[name=impact]{Preprocess Environmental Impact}{\betareleasedue}{\releasedue} \\
    \ganttbar[name=impactfeature]{Feature Extract Environmental Impact}{\betareleasedue}{\releasedue} \\
    \ganttbar[name=changes]{Document Changes}{\betareleasedue}{\releasedue} \\
    \ganttbar{API Documentation}{\fallprogressreportdue}{\releasedue} \\
    \ganttbar{Static site generator}{\betareleasedue}{\finalreportdue} \\
    \ganttbar{Front-end Framework}{\betareleasedue}{\finalreportdue} \\

    % gantt chart links
    \ganttlink{mineralid}{mineralidclass}
    \ganttlink{mineralidclass}{miningid}
    \ganttlink{miningid}{impact}
    \ganttlink{impact}{impactfeature}
    \ganttlink{impactfeature}{changes}

  \end{ganttchart}

  \caption{Gantt Chart: Timeline of Project Tasks}

\end{figure}

\section{Conclusion}
\label{sec:conclusion}
This document has outlined the design of the COAL project and the suite of algorithms and documentation it provides. The major components of the system were broken down and assigned to each team member who described in each section the research priorities and design decisions involved. Processing large datasets of imagery and geographic information require a significant investment to study and develop solutions. However, the results will provide a wealth of information for environmental data scientists and other concerned parties looking for associations between mining and environmental damage. Further revisions of this document will reflect specific choices made as the project matures.

\section{Glossary}
\begin{description}[\IEEEsetlabelwidth{docstring}]
\item[AVIRIS] Airborne Visible/Infrared Imaging Spectrometer
\item[COAL] Coal and Open-pit surface mining impacts on American Lands
\item[docstring] Documentation string. In Python, they are comments that start and end with three quotation marks.
\item[GIS] Geographic Information System
\end{description}

\section{Revisions}
\label{sec:revisions}

\subsection{Feature extraction in AVIRIS data}
Revisions to Section \ref{subsec:featureextraction}: 
\subsubsection{Revision 1}
Scrapped blob detection and wrote about training neural networks.

\subsubsection{Revision 2}
Scrapped neural networks and wrote about spectral libraries.

\subsection{Classification of AVIRIS data}
Revisions to Section \ref{subsec:classification}: 
\subsubsection{Revision 1}
Rewrote most of the section and made it more specific to the development at this time.

\subsubsection{Revision 2}
Scrapped neural networks and wrote about the spectral angle mapper.

\subsection{Identify Mining}
Revisions to Section \ref{subsec:identifymining}: No revisions.

\subsection{Preprocessing Data to Correlate Mining with Environmental Impact}
Revisions to Section \ref{subsec:preprocessing}: Completely revised to describe our new knowledge about geographic information data.

\subsection{Feature Extraction to Correlate Mining with Environmental Impact}
Revisions to Section \ref{subsec:correlate}: Completely revised to describe our environmental case study and the current implementation of the code.

\subsection{Rank and Document Changes Over Time}
Revisions to Section \ref{subsec:changesovertime}: Completely revised to present our better understanding of the batch- and post-processing.

\subsection{API Documentation}
Revisions to Section \ref{subsec:apidoc}: No revisions.

\subsection{Static site generator}
Revisions to Section \ref{subsec:staticsitegenerator}: To clarify what kind of functions we need Jekyll to provide, and to how to get started to use it.

\subsection{Front-end framework}
Revisions to Section \ref{subsec:frontendframework}: Add details about what is on the navigation bar. Add the details of what changes of the website.


\end{document}
