% assignment: http://eecs.oregonstate.edu/capstone/cs/capstone.cgi?hw=report

\documentclass[10pt,draftclsnofoot,onecolumn,journal,compsoc]{IEEEtran}
% for IEEEtran usage, see http://www.texdoc.net/texmf-dist/doc/latex/IEEEtran/IEEEtran_HOWTO.pdf

\usepackage[margin=0.75in]{geometry}
\usepackage[dvipsnames]{xcolor}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{hyperref}
\usepackage{enumerate}
\usepackage{amssymb}
\usepackage{pgfgantt}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{pdfpages}
\usepackage{float}

% title page background image
% TODO rotate and scale ?
\usepackage{eso-pic}
\newcommand\BackgroundPic{%
\put(0,0){%
\parbox[b][\paperheight]{\paperwidth}{%
\vfill
\centering
\includegraphics[width=\paperwidth,height=\paperheight]{161344main_image_feature_681b_ys_full.png}
\vfill
}}}

% color scheme for python code listings
\lstdefinestyle{py_listing}{
  language=Python,
  keywordstyle=\color{Fuchsia}\bf,
  commentstyle=\color{ForestGreen},
  stringstyle=\color{CadetBlue},
  basicstyle=\small\ttfamily,
  numbers=left,
  numberstyle=\scriptsize,
  showspaces=false,
  showstringspaces=false,
  breaklines=true
}
\lstset{style=py_listing}


\renewcommand{\linespread}{1.0}

% formatting for subsubsubsections
% https://tex.stackexchange.com/questions/60209/how-to-add-an-extra-level-of-sections-with-headings-below-subsubsection#60212
% https://tex.stackexchange.com/questions/8361/latex-ieeetran-cls-use-titlesec-package#8362
% FIXME: find a way to put in table of contents
\newcommand{\subparagraph}{}
\usepackage{titlesec}
\titleformat{\paragraph}
{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
\titlespacing*{\paragraph}
{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}
\newcommand{\subsubsubsection}{\paragraph}

\title{COAL: Coal and Open-pit surface mining impacts on American Lands}
\author{
  \IEEEauthorblockN{Taylor Alexander Brown, Heidi Ann Clayton, and Xiaomei Wang} \\
  \IEEEauthorblockA{CS 463: Senior Capstone 2016 -- 2017 \\ Oregon State University}
}
\date{}

\IEEEtitleabstractindextext{
  \begin{abstract}
   Mining is known to cause environmental degradation, but software tools to identify mining impacts are lacking. Researchers studying this problem possess large imaging spectroscopy and environmental quality data sets as well as high-performance cloud-computing resources. This project provides a suite of algorithms using these data and resources to identify signatures of mining and correlate them with environmental impacts over time.
  \end{abstract}
}

\begin{document}
\AddToShipoutPicture*{\BackgroundPic}
\maketitle
\IEEEdisplaynontitleabstractindextext
\IEEEpeerreviewmaketitle
\vfill

\newpage

\vspace*{\fill}
\begin{center}
  \emph{Cover image displays shortwave infrared bands of the Escondida open-pit mine in Chile. \\ Courtesy NASA/GSFC/MITI/ERSDAC/JAROS and U.S./Japan ASTER Science Team.}
\end{center}
\vspace*{\fill}

\newpage

\tableofcontents

\newpage

\section{Introduction}

Coal and Open-pit surface mining impacts on American Lands (COAL) is a Python library for processing hyperspectral imagery from remote sensing devices such as the Airborne Visible/InfraRed Imaging Spectrometer (AVIRIS). COAL was developed as a 2016 -- 2017 senior capstone collaboration between scientists at the Jet Propulsion Laboratory (JPL) and computer science students at Oregon State University (OSU). COAL provides a suite of algorithms for classifying land cover, identifying mines and other geographic features, and correlating them with environmental data sets. COAL is Free and Open Source Software under the terms of the GNU General Public License Version 2.

The COAL project was proposed and led by JPL data scientist Dr. Lewis John McGibbney to study the impacts of mining on the environment. COAL uses imaging spectrometer data, geographic information, and high-performance computing resources to classify minerals, identify mining, and correlate environmental impacts over time. The COAL library was developed with a case study to detect acid mine drainage associated with an open-pit coal mine in New Mexico. COAL delivered a productive method for analyzing spectral and geospatial data with applications beyond the project to environmental data science and other fields.

The COAL capstone team consisted of Taylor Alexander Brown, Heidi Clayton, and Xiaomei Wang. Taylor contributed principal research and development, Heidi contributed development and management of coursework, and Xiaomei contributed discussion and support. Dr. Lewis John McGibbney and Dr. Kim Whitehall of JPL were the clients of the COAL capstone project. Lewis served as a leader as well as developer of the project, providing its initial vision and actively guiding its evolution. Kim supported the project by providing additional feedback and data.

\section{Original Requirements}

%    This needs to be the original document, showing what you thought, at the time, was the project definition.
%    This needs to include the original Gantt chart. 

% commit 4151a0a
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Introduction}
Our team name for this project is ``COAL'': Coal and Open-pit surface mining impacts on American Lands.

\subsubsection{Purpose}
The purpose of this document is to describe what COAL will provide as a piece of software. This document will touch on the interfaces, features, and constraints of COAL. This document is intended for developers, potential users of COAL, and any stakeholders in projects that will make use of or reference COAL.

\subsubsection{Scope}
\label{req:scope}
COAL will be a suite of algorithms that identifies, classifies, and quantifies the effects of open-pit mining on the surrounding environment. This will be done by first using spectroscopy data to identify mining activity and then using data provided by GRaND to attempt to correlate these identified sites with environmental impacts over time. These algorithms will be provided as reusable components that can be used in cloud-based systems such as NASA Ames Stereo Pipeline. There will also be a website that will present the algorithms and related data visualization as well as links to the source code, project wiki, API documentation, and developer profiles to anyone interested in using COAL for their cloud-based platform or research.

\subsubsection{Definitions, acronyms, and abbreviations}
\label{req:defs}
\begin{description}[\IEEEsetlabelwidth{NASA Ames Stereo Pipeline}]
\item[AGU] American Geophysical Union
\item[API] Application Programming Interface
\item[AVIRIS] Airborne Visible/Infrared Imaging Spectrometer
\item[COAL] Coal and Open-pit surface mining impacts on American Lands
\item[GRaND] Global Reservoir and Dam Database
\item[Imaging Spectroscopy] ``[A]cquisition of images where for each spatial resolution element in the image a spectrum of the energy arriving at the sensor is measured [1].''
\item[LSE] Land and Solid Earth
\item[MTM] Mountain-top Mining
\item[NASA Ames Stereo Pipeline] A cloud-based platform that provides tools for processing stereo imagery from satellites, rovers, aerial photography, and historical images [2].
\end{description}

\subsubsection{References}

\begin{enumerate}[ {[}1{]} ]
\item "Imaging spectroscopy," in Jet Propulsion Laboratory, 2007. [Online]. Available: \url{http://aviris.jpl.nasa.gov/html/aviris.spectroscopy.html}. Accessed: Nov. 4, 2016.
\item "Neo-geography Toolkit," in National Aeronautics and Space Administration. [Online]. Available: \url{https://ti.arc.nasa.gov/tech/asr/intelligent-robotics/ngt/stereo/}. Accessed: Nov. 4, 2016.
\end{enumerate}

\subsubsection{Overview}
The rest of this document contains an overall description of the project, a list of specific requirements, and signatures from the client and students.

\subsection{Overall description}

\subsubsection{Product perspective}
The COAL suite of algorithms will be compatible with cloud-based platforms. Users will interact with COAL's homepage in order to view a detailed analysis of COAL's algorithms and data visualization using spectroscopy and mineral maps. COAL's algorithms and data analysis will be interfaced with Python data analysis and visualization libraries. COAL's algorithms will have time and space complexity that is efficient enough for users to process very large data sets (e.g. 5+ GB). There are no hardware components to this product nor are there communications interfaces.

\subsubsection{Product functions}
Analyze imaging spectroscopy and environmental quality data sets to generate human-readable reports and visualizations. Provide API documentation, wiki, homepage, source code, and research to allow others to use, improve, and replicate this work.

\subsubsection{User characteristics}
The intended users are data scientists at research institutions. Users are expected to have access to image spectroscopy and environmental quality data sets which they wish to study. Users are also expected to have access to high-performance cloud computing resources and experience deploying applications to them. In general, the intended users are highly educated, experienced, and technical, but not necessarily proficient in programming.

\subsubsection{Constraints}
COAL will be efficient enough to process large data sets on high-performance cloud servers in a practical amount of time for research applications.

\subsubsection{Assumptions and dependencies}
The system requirements (such as operating system, build environment, and programming language interpreter version) are to be determined based on the requirements of the third-party libraries chosen to implement it.

\subsection{Specific Requirements}

\subsubsection{Research background literature and libraries}
\label{req:subsubsection:research}
On an ongoing basis for the duration of the project, review background literature and libraries and provide relevant references in project documentation. Literature may include videos, books, websites, and journal articles. Libraries include third-party software modules used and any new versions and functionalities that are added to them.

\subsubsection{Problem statement}
\label{req:subsubsection:problem}
Provide documents that define the scope of the software suite from both the client perspective and the student perspective. The client problem statement provides a high-level overview of the client's goals for the project. The student problem statement describes the project from a computer science perspective, giving an overview of the problem domain and the steps taken to solve it.

\subsubsection{Requirements document}
\label{req:subsubsection:requirements}
Provide a document that describes in detail what tasks are to be completed and when.

\subsubsection{Develop Software}
Develop a suite of imagery processing algorithms using existing libraries and, if necessary, custom code. The behavior of each algorithm should be verified with automated tests. The algorithms should be efficient enough and effective enough for data scientists to justify adopting and deploying them to solve real-world problems. The results should include visualizations that allow data scientists to infer meaningful relationships from the data sets.

\subsubsubsection{Mineral identification and classification}
\label{req:subsubsubsection:mineral}
Implement algorithms to identify and classify minerals and their spatial relationships from imaging spectroscopy data.

\subsubsubsection{Identify mining}
\label{req:subsubsubsection:mining}
Implement algorithms to identify regions of mining activity from the mineral identification and classification data.

\subsubsubsection{Correlate mining with environmental impact}
\label{req:subsubsubsection:environment}
Implement algorithms to correlate the regions of mining activity with data on regions of environmental degradation.

\subsubsubsection{Rank and document changes over time}
\label{req:subsubsubsection:changes}
Implement algorithms to derive spatial and temporal relationships between the regions of mining activity and the regions of environmental degradation.

\subsubsection{API documentation}
\label{req:subsubsection:apidoc}
Provide API documentation for developers in a human-readable format that is automatically generated from the codebase.

\subsubsection{Wiki}
\label{req:subsubsection:wiki}
Provide a wiki with documentation for end users containing comprehensive usage instructions and relevant background information. Also provide a development blog with weekly updates from the development team.

\subsubsection{Homepage}
\label{req:subsubsection:website}
Provide a website that serves as the project homepage. The homepage should describe the project in general terms and demonstrate example imagery and data. Links to relevant sites including the source code repository, API documentation, Wiki, and developer profiles should be included. It should be accessible, interactive, and self-contained.

\subsubsection{Release Software}
\label{req:subsubsection:release}
Release the software as a collection of reusable modules which data scientists may deploy to cloud platforms. All code is to be released under the Apache License, Version 2.0.

\subsubsection{Publish Research Paper}
\label{req:subsubsection:paper}
Publish a research paper and present it to a scientific society such as the AGU. The paper should explain the research problem, describe the software developed to solve it, and present as a case study an analysis of imagery from the AVIRIS project.

% academic calendar: http://catalog.oregonstate.edu/ChapterDetail.aspx?key=148
% engineering calendar: http://eecs.oregonstate.edu/industry-relations/calendar
% course timeline: http://eecs.oregonstate.edu/capstone/cs/capstone.cgi?home=1

\newcommand{\firstdayoffallterm}{2016-09-21}      % first day of fall term
\newcommand{\startday}{2016-10-02}                % day groups assigned
\newcommand{\fallprogressreportdue}{2016-12-05}   % finals week of fall term
\newcommand{\alphareleasedue}{2017-02-13}         % week 6 of winter term
\newcommand{\betareleasedue}{2017-03-20}          % finals week of winter term
\newcommand{\winterprogressreportdue}{2017-03-20} % finals week of winter term
\newcommand{\releasedue}{2017-05-15}              % monday prior to tentative expo date
\newcommand{\expoday}{2017-05-19}                 % tentative expo date
\newcommand{\finalreportdue}{2017-06-12}          % finals week of spring term
\newcommand{\lastdayofspringterm}{2017-06-16}     % last day of spring term

\begin{figure}

  % gantt chart: http://mirrors.rit.edu/CTAN/graphics/pgf/contrib/pgfgantt/pgfgantt.pdf
  \begin{ganttchart}[x unit=0.15em, time slot format=isodate, link bulge=4]{\firstdayoffallterm}{\lastdayofspringterm}

    % gantt chart title
    \gantttitlecalendar{year, month=shortname} \\

    % gantt chart bars
    \ganttbar{Research literature and libraries}{\startday}{\expoday} \\
    \ganttbar[name=problem]{Problem Statement}{\startday}{2016-10-26} \\
    \ganttbar[name=requirements]{Requirements Document}{2016-10-26}{2016-11-04} \\
    \ganttbar[name=mineralid]{Mineral Identification}{\fallprogressreportdue}{\alphareleasedue} \\
    \ganttbar[name=miningid]{Identify Mining}{\alphareleasedue}{\betareleasedue} \\
    \ganttbar[name=impact]{Correlate Impact}{\betareleasedue}{\releasedue} \\
    \ganttbar[name=changes]{Document Changes}{\betareleasedue}{\releasedue} \\
    \ganttbar{API Documentation}{\fallprogressreportdue}{\releasedue} \\
    \ganttbar{Wiki}{\fallprogressreportdue}{\releasedue} \\
    \ganttbar{Homepage}{\betareleasedue}{\finalreportdue} \\
    \ganttbar{Release Software}{\releasedue}{\finalreportdue} \\
    \ganttbar{Publish Research Paper}{\releasedue}{\finalreportdue}

    % gantt chart links
    \ganttlink{problem}{requirements}
    \ganttlink{mineralid}{miningid}
    \ganttlink{miningid}{impact}
    \ganttlink[link mid=0.25]{miningid}{changes}

  \end{ganttchart}

  \caption{Gantt Chart: Timeline of Project Tasks}

  \label{req:figure:gantt}

\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Changes to Requirements}

%    What new requirements were added? What existing requirements were changed? What existing requirements were deleted? Why?
%
%    Use the following table format:
%    1	Requirement	What happened to it	Comments
%    2	Requirement	What happened to it	Comments
%    3	Requirement	What happened to it	Comments
%
%    What was the final Gantt chart? This should be a record of what happened when! 

% just talk about what's changed, don't paste in whole document

Our requirements themselves were unchanged throughout the course of this project. Though there were many changes to the design document and the technology review document, as discussed in later sections, the requirements document stayed largely the same.

There were only two small changes that were made during Spring 2017. First off, in the "Scope" section \ref{req:scope} there was a reference to Global Reservoir and Dam Database (GRanD). We removed this reference and instead just mentioned water datasets in general. Along with this, we removed the definition of GRanD from the "Definitions, acronyms, and abbreviations" \ref{req:defs} section. GRanD ended up not being applicable to our case study so ended up using the National Hydrography Dataset (NHD) instead as discussed later.

The original version of the project timeline was described in the requirements document in Figure \ref{req:figure:gantt}. It was updated in the design document in Figure \ref{design:figure:gantt}. The final version of the Gantt chart reflecting the actual project timeline is recorded in Figure \ref{reqchanges:figure:gantt}. Modifications were made as the project evolved, and research and development extended from the beginning of winter term through the code freeze on 2017-05-01.

Several additional changes were made at the end of the project after the revised requirements document was submitted. The team contributed to research throughout the course of the project, however our client decided to take responsibility for drafting and publishing the research paper after the end of the course. In addition, after an initial release of COAL it was realized that the license needed to change from the Apache License Version 2 to the GNU General Public License Version 2 as required by the licensing of the Spectral Python library.

The table in Figure \ref{reqchanges:figure:table} details the outcome of each requirement.


\newcommand{\codefreeze}{2017-05-01}     % code freeze

\begin{figure}

  % gantt chart: http://mirrors.rit.edu/CTAN/graphics/pgf/contrib/pgfgantt/pgfgantt.pdf
  \begin{ganttchart}[x unit=0.15em, time slot format=isodate, link bulge=4]{\firstdayoffallterm}{\lastdayofspringterm}

    % gantt chart title
    \gantttitlecalendar{year, month=shortname} \\

    % gantt chart bars
    \ganttbar{Research literature and libraries}{\startday}{\codefreeze} \\
    \ganttbar[name=mineralid]{Mineral Classification}{2017-02-02}{\codefreeze} \\ % issue 18 through code freeze
    \ganttbar[name=miningid]{Mining Identification}{2017-02-07}{\codefreeze} \\ % issue 19 through code freeze
    \ganttbar[name=impact]{Environmental Correlation}{2017-02-02}{\codefreeze} \\ % issue 20 through code freeze
    \ganttbar{API Documentation}{2017-03-10}{\codefreeze} \\ % issue 30 through code freeze
    \ganttbar{Wiki}{\startday}{\expoday} \\
    \ganttbar{Website}{2017-03-04}{\lastdayofspringterm} \\ % website issue 10 and ongoing
    \ganttbar{Research Paper}{2017-02-10}{\lastdayofspringterm} % issue 27 ongoing

  \end{ganttchart}

  \caption{Gantt Chart: Actual Timeline of Project Tasks}

  \label{reqchanges:figure:gantt}

\end{figure}


\begin{figure}
  \begin{tabular}{|l | l | p{11.5cm} |}
    \hline
    {\bfseries Section}                    & {\bfseries Requirement }  & {\bfseries Outcome and Comments} \\ \hline
    \ref{req:subsubsection:research}       & Background Research       & Research into literature and libraries was ongoing throughout the duration of the project. \\ \hline
    \ref{req:subsubsection:problem}        & Problem Statement         & The original problem statement submitted by the client and the student problem statement were included in the fall progress report. \\ \hline
    \ref{req:subsubsection:requirements}   & Requirements Document     & The requirements document was drafted during fall term and revised during spring term. Additional course documents were not considered to be requirements for the client's purposes. \\ \hline
    \ref{req:subsubsubsection:mineral}     & Mineral Classification    & The mineral classification algorithm was implemented using Spectral Angle Mapper (SAM) classification. \\ \hline
    \ref{req:subsubsubsection:mining}      & Mining Identification     & The mining identification algorithm was implemented by identifying minerals associated with coal mining in the mineral classification data. \\ \hline
    \ref{req:subsubsubsection:environment} & Environmental Correlation & The environmental correlation algorithm was implemented by intersecting locations of mines with hydrography data. \\ \hline
    \ref{req:subsubsubsection:changes}     & Temporal Analysis         & Temporal analysis was implemented by the client as a Science Data System (SDS) to process AVIRIS data over time. A temporal dataset was not available for our case study. \\ \hline
    \ref{req:subsubsection:apidoc}         & API Documentation         & API documentation was configured using Sphinx and ReadTheDocs.io and automatically generated from Python docstrings. \\ \hline
    \ref{req:subsubsection:wiki}           & Wiki                      & The wiki included development documentation and team member weekly updates, however user documentation was mainly implemented in the API docs and website. \\ \hline
    \ref{req:subsubsection:website}        & Website                   & The website was implemented with Jekyll on GitHub pages and designed with Bootstrap. It became the public face of COAL and a source of user documentation. \\ \hline
    \ref{req:subsubsection:release}        & Release Software          & The Python COAL library \verb!pycoal! was released on the Python Package Index (PyPI) on an ongoing basis. It was deployed on a cloud server and will be deployed on the high-performance computing environment XSEDE. The licensing was eventually changed from Apache2 to GPLv2. \\ \hline
    \ref{req:subsubsection:paper}          & Research Paper            & Capstone developers contributed to research throughout the project and the client chose to draft the paper after the project. \\
    \hline
  \end{tabular}

  \caption{Table of Requirements and Changes}
  \label{reqchanges:figure:table}

\end{figure}




\section{Original Design}

%% Your original design document, as well as a discussion of what had to change over the course the year.
% document: finesse original text into unified document
% discussion

% same commit 4151a0a
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Introduction}
\label{design:sec:intro}
This is the design document for COAL. It outlines the design of our system as well as the research and development priorities at each step. The COAL suite of algorithms constitutes a pipeline of data processing and presentation, and is accompanied by API documentation as well as a website for end users and the general public. Specific implementation details remain to be determined based on research, but in general the program reduces a large set of hyperspectral and geographic data into comprehensible conclusions about mining and environmental impact in human and machine-friendly formats.

Each section was authored by the corresponding team member responsible for each component identified in the technology review: Sections \ref{design:subsec:featureextraction} and \ref{design:subsec:classification} by Heidi; section \ref{design:subsec:identifymining} by Xiaomei; sections \ref{design:subsec:preprocessing}, \ref{design:subsec:correlate}, and \ref{design:subsec:changesovertime} by Taylor; section \ref{design:subsec:apidoc} by Heidi; and sections \ref{design:subsec:staticsitegenerator} and \ref{design:subsec:frontendframework} by Xiaomei. The research priorities and design choices for each part of the project are discussed in each section.

\subsection{Technologies}

% heidi: requirement 3.4.1 part 1: choose technology for mineral identification and classification
\subsubsection{Feature extraction in AVIRIS data}
\label{design:subsec:featureextraction}

In order to process hyperspectral data, machine learning techniques will have to be used to process the raw AVIRIS images \cite{aviris}. Further processing could be done using blob detection, as described below.

AVIRIS images cover a lot of land mass with a lot of different features, so a method of feature extraction is needed to automatically pick out the different regions on the image. In blob detection, an area of an image that is different from the surrounding pixels is known as a blob. This will make classification easier as instead of needing to classify every single pixel every single time, the classification algorithm will only need to process each blob, or “feature,” which typically contains many pixels. The blob detection algorithm that will be used is automatic scale detection. It is one of the more expensive methods of blob detection, but it is highly accurate, so it is the one that has been chosen as discussed in the technology review. The automatic scale detection algorithm is based on the Laplacian of Gaussian. As the name suggests, this is a convolution of the Laplacian and Gaussian filters \cite{autoscaleprinc}. The Gaussian filter is applied first to smooth out noise and then the Laplacian filter is used to detect the shapes in the image \cite{lapofgaus}. The equation for a Gaussian convolution is given below, where t represents scale \cite{autoscaleprinc}.

\[ g(x, y) = \frac{1}{2\pi t}e^{-\frac{(x^{2} + y^{2})}{2t}}\] 

The equation for a Laplacian filter is given below, where t represents scale \cite{autoscaleprinc}.


\[\Delta^{2}L = t(L_{xx} + L_{yy}) \]


When the Laplacian filter is applied to the Gaussian filter, the equation becomes the one shown below \cite{lapofgaus}.

\[LoG(x, y) = -\frac{1}{\pi t^{4}}[1 - \frac{x^{2} + y^{2}}{2t}] e^{\frac{-x^{2} + y^{2}}{2t^{2}}}\]

In automatic scale selection, the algorithm is looking for the scale that produces the largest Laplacian response in the blob center \cite{ppt}. When looking at a kernel, the scale that is the “best fit” so to speak, is the maximum value in the kernel. Figure 1 is an example of this. The scale that would be chosen is five. Looking through the values, all of them correspond to specific behavior in the section of the image it represents. On sections that are fairly homogeneous, the individual values in the kernel will be zero or close to zero. When a difference occurs, the values will start to become negative on the lighter side of the blob and positive on the darker side \cite{lapofgaus}. Many implementations of the Laplacian of Gaussian method use a reversed sign. That is, the minimum is chosen instead. This does not alter the correctness of the algorithm, but we will be using a the positive signed implementation where the scale chosen is the largest number \cite{lapofgaus}.

\begin{figure}
\centering
\begin{tabular}{ | l | l | l | l | l |}
    \hline
    -1 & 0 & 0 & 0 & 0\\ \hline
    0 & 1 & 2 & 1 & 0\\ \hline
    0 & 3 & 5 & 1 & 0\\ \hline
    0 & 2 & 2 & 3 & 0\\ \hline
    0 & 0 & 0 & 0 & -2\\
    \hline
\end{tabular}
\caption{An example of a kernel to be analyzed by automatic scale detection.}
\end{figure}

When graphed on a Cartesian coordinate system, the scale chosen is the local maximum. As illustrated in figure 2. with an example blob being the center of the flower, the largest and best fitting blob corresponds to the local maximum when graphed on the Cartesian coordinate system.

\begin{figure}
\centering
\includegraphics[scale=.5]{blob.jpg}
\caption{A demonstration of the largest scale blob in the center of the flower corresponding with the local maximum of the curve \cite{ppt}.}
\end{figure}

One important factor that needs to be considered for this design item is that the values being generated inside the Laplacian of Gaussian kernel can get very, very large \cite{lapofgaus}. It is very important that the environment used when running the automatic scale detection algorithm can support a large range of numbers, both positive and negative. The possibility of overflow is likely and this could very heavily skew the results of blob detection. The development team does not have experience with running this type of algorithm, but support for a minimum of 32-bit numbers is what will be used as a starting point.

% heidi
\subsubsection{Classification of AVIRIS data}
\label{design:subsec:classification}
After the data has been processed using feature extraction as detailed in the previous section, the resulting data will be put through a process of classification. The classification will take the AVIRIS images, which are essentially a map of reflected light, and produce an image where each pixel is identified as belonging to a certain class \cite{introarcmap}. This serves the purposes of making the data more readable to the development team and any other person who is using COAL and classifying the data in a useful way to aid in the identifying minerals, identifying mining, and correlating the effects with environmental impact design items. The classes will be types of land surfaces. This includes water (e.g. turbid versus less turbid), vegetation (e.g. dense versus less dense), basaltic lava flow, snow, and any types of minerals found in the image \cite{aviris}. The algorithm used for classification will be a neural network.

The classification will be done by recording the differences of reflectance in the pixels of the image. For example, snow is highly reflective in the visible light spectrum but not in the infrared spectrum \cite{introarcmap}. We will use both supervised classification and unsupervised classification to classify the different types of land surfaces in the images. That is, we will be training the neural network algorithm to look for specific minerals and land surfaces but will also try training the algorithm to classify pixels into any class deemed significant. In the training stage, COAL will use AVIRIS data on sites where the land surfaces are known. These images with known land surfaces will train the neural network algorithm to classify the land surfaces of AVIRIS images where the land surfaces are not known.

When testing the progress of the training on the neural network algorithm, the algorithm will be run on images with known land surfaces that were not used in the training stages. If the algorithm correctly classifies at least 80\% of the pixels in at least 80\% of the images used for testing, the training will be considered sufficient. Otherwise, more samples will be required in the training and the algorithm will need to be tested again. In general, small sample sizes do not produce reliable results when making predictions, so making sure the training has used enough AVIRIS images is important. 

After the training stage, AVIRIS images with unknown land surfaces will be processed with the trained algorithm. The algorithm will classify each pixel into a class representing a land surface and that data will be used for further analysis (see design items \ref{design:subsec:identifymining}, \ref{design:subsec:preprocessing}, and \ref{design:subsec:correlate}) and possibly be output into an image to be used on the homepage. 

Something to consider when training and using this algorithm for analysis is spectral separability. Some types of minerals and other land surfaces have very similar relationships between reflectance and wavelength and can be cause the algorithm’s accuracy to suffer if they are separated into different classes \cite{separability}. It is up to the developers to determine if the particular minerals or other land surfaces that are not very separable should be put into the same class or kept separate depending on if the minerals or other land surfaces in question are important enough by themselves in some way to warrant a slightly lower accuracy.


% xiaomei: requirement 3.4.2: identify mining
\subsubsection{Identify Mining}
\label{design:subsec:identifymining}

After we get the mineral identification and classification data, we need to identify the regions of mining activity. The data is further processed and possibly combined with geographic information about geology and mines. The goal of identifying mining is to accurately locate regions in which mining is taking place in a form that is suitable for both human inspection and further processing. This step creates input data for further processing of the project and the data evaluation.

To start our process of identifying mining, we need the output from the mineral identification step. The previous step has already located the regions that are containing mineral associated with mining. To distinguish mines from preexisting geology is a challenge; only rely on the existing geography data is not enough for the identification.

There are three methods mentioned after several group discussion sessions for identifying mining: mineral classification, using patterns of mineral deposits; GIS comparison, using geographic information such as mining permit boundaries or geologic maps; and a combination of mineral classification and GIS comparison, using both sources of data. To locate regions with mineral deposits that indicate mining, we have decided that we are going to use the mineral classification methods. Then, we would move to a combination of mineral classification and GIS comparison methods once we are able to complete the first step without much trouble.  

The mineral classification method can be very simple, mapping the presence of a mineral may be sufficient to locate mines: minerals with low natural concentrations can be used to identify mining activity. In this case, the result is expected to be straightforward, and the time spending on the identification is short. However, the outcomes are not always favorable under this method. Still, at the very beginning, we expect to rely on this to produce some useful outputs for further application. The combination of the mineral classification and GIS comparison can be costly; it would require the most research and time. Although the outcome can be most complete, so that is why we use mineral classification method at the beginning; simply mapping the presence of a mineral may be sufficient to locate mines due to minerals with low natural concentrations can be used to identify mining activity. GIS comparison can provide more accurate results. 

% taylor: requirement 3.4.3 part 1: preprocessing data to correlate mining with environmental impact
\subsubsection{Preprocessing Data to Correlate Mining with Environmental Impact}
\label{design:subsec:preprocessing}

After mines have been identified, we are interested in analyzing their impact on the environment. Using environmental quality GIS data from agencies such as the EPA, we would like to be able to correlate mining operations with environmental degradation. This stage depends on the previous steps in the data processing pipeline and will require additional research to analyze and solve the problem.

Research and development of this step will focus on identifying and analyzing available literature and tools. The principal problem is to combine the spectroscopic-derived data with geographic data. Preprocessing this data will require using the metadata from the imagery to associate pixels with geographic coordinates. It will also require loading and handling geographic data. Researching means of accomplishing both of these processes will thus be an important and ongoing task.

In addition to literature, we will be identifying and learning to use tools and libraries for handling the data. Learning to use GIS applications such as ArcGIS is therefore a high priority. Using proprietary GIS software will require the team to arrange suitable licensing. Library methods for analyzing the imagery metadata and the geographic information will probably be used, so identifying suitable libraries will also be an important focus.

Having researched literature and tools, our development focus will turn to implementation. Decisions will need to be made about how to structure the application and how to pass the preprocessed data to the following feature extraction step. It may make more sense to combine both preprocessing and feature extraction of environmental impact into a single module, or we may find that the components are best separated into distinct procedures.

Ensuring that the input and output of our program is consistent is best accomplished by automated testing. It would be possible to apply test-driven development and write tests to specify program behavior, but we may find it more natural to implement procedures and tests alternately. At the end of the project, our tests will serve as an informal specification and an assurance of correctness.

The goal which the tests will verify is that the data is output in a suitable format for feature extraction. The spectrometry-derived data identifying mines will be associated with geographical locations. We may also find it desirable to simplify the data by discarding unnecessary information. The geographic information having been loaded into the application may likewise be simplified. We will need to identify specific layers of interest to pass on to the next step. Combining both of the data or keeping them separate will be a factor in the design as well.

Preprocessing data to correlate mining with environmental impact thus depends on a sequence of steps. Initial research and development must be undertaken to analyze available literature and documentation as well as to find tools and libraries to facilitate our programming. Implementing the algorithms will make our assumptions explicit and require decisions about structure and format. Ensuring they meet our requirements will involve automated tests. The end product will be preprocessed imagery and geographic information that can be used to make correlations between mining and environmental impact.

% taylor: requirement 3.4.3 part 2: feature extraction to correlate mining with environmental impact
\subsubsection{Feature Extraction to Correlate Mining with Environmental Impact}
\label{design:subsec:correlate}
Given preprocessed mine imagery and geographic information, feature extraction derives correlations between mining and environmental impact. Research will occupy a preliminary and ongoing part of implementing this module. Implementation will require design decisions as well as more complete specification of the desired output data. The result will be different representations and visualizations of data, such as tabular and graphical displays. Automated tests will help enforce our assumptions about the procedures, but data that is generated in human-readable formats will have to be analyzed in person and adjusted according to utilitarian as well as aesthetic preferences.

The research priority of this stage is to determine the best ways of deriving and presenting the results. One approach that has been described in our readings is to simply overlay the data sets in a geographical representation. This allows an end user to determine by eye the regions where mining and environmental problems overlap. However, we need to find out whether there are means of improving the accuracy of the correlations or removing superfluous results. Can we assign confidence values to sites where mining and pollution appear to be linked, or are the results more naturally boolean in character? What other ways of presenting the data improve the end user experience of processing the data? These are some of the problems and questions our research will seek to answer.

In addition to researching derivation and presentation, implementation will require referencing the documentation of software libraries used in this module. It may also depend on patching contributed libraries to correct bugs or add features. Imaging libraries, geographic information libraries, statistical libraries, and data visualization libraries are all likely candidates to contribute to our project. Identifying suitable libraries, becoming proficient with them, and fixing them are some of the steps implementation will entail.

The libraries we choose will influence the design choices of the module as well as the amount and nature of the code we write. As mentioned previously, this step and the preceding preprocessing step may be combined into a single module or they may be separated. Implementation of this module requires procedures for handling the data and generating representations for the end user. Separating these procedures would facilitate modularity and testability. They could be unified in the application program by means of command-line parameters or preference files selecting, for example, the desired graphical representation or the algorithm used to generate it.

The output as described may include tabular and graphical representations such as map overlays and charts. It will be necessary to specify the data formats to generate and to produce them to best meet the needs of the environmental data scientists using them. Tables, bar charts, line charts, geographical maps, and diagrams are all possible representations of different quantities in the result set. Probabilistic data using confidence values would require a means of conveying the different probabilities as well as potentially cutting off results that are below a certain level of accuracy.

Automated tests will help ensure the correctness of parts of the program, but the end result will be best analyzed in person. Components that could be tested include those responsible for converting input formats to a predictable binary output format. Sample data can be provided and tested to ensure that results such as tabular representations are consistent with expectations. However, the overall usefulness and quality of the data visualizations must be judged subjectively.

Deriving and displaying correlations between mining and economic impacts is the purpose of this feature extraction stage. Research into literature and libraries will be conducted, algorithms will be implemented, and graphical representations will be selected to convert preprocessed data into user-friendly formats. Automated tests will aid development, but the results will also depend on subjective determinations.

% taylor
\subsubsection{Rank and Document Changes Over Time}
\label{design:subsec:changesovertime}
Ranking and documenting changes over time is the final step in the data processing pipeline. After identifying minerals, mines, and associated environmental impacts, further analysis is possible by providing a historical context to environmental changes. Implementation of this step could be as simple as instructing the end user to run the program several times over separate datasets, however this would not provide effective usability. A more sophisticated implementation would take as input a historical data set and generate trends automatically. Whatever approach is chosen, research into literature and libraries will again be a significant part of developing this feature.

Before development on this step can begin, the preceding stages of data processing must be complete. The design choices and implementation details made at each step will affect the options available for temporal analysis. Data formats and representations will influence the kinds of displays that can be produced. Program structure will determine how reusable certain software components are, a consideration that will be important when combining them to analyze changes over time.

The simplest interpretation of documenting changes over time is to process historical data and generate representations for each time period separately. A note in the user documentation would describe how to execute the program and possibly means of automating it. Shell scripts could be provided, for example, to apply the environmental correlation procedure to a collection of historical images and geographic information files. However, the first difficulty occurs when associating AVIRIS flight times with environmental measurements which were most likely produced at separate times and at separate intervals. The end user would have to make decisions about the nearest applicable data set, a requirement which would need to be noted in the documentation as well.

More sophisticated methods could be implemented to relieve the burden of some of these tasks at the cost of additional research and development time. For example, this module could receive command-line arguments or configuration files containing multiple data files and run the analyses automatically. Designing the program this way would allow new options for output such as generating temporal graphs which would not be possible if correlations are made in isolation. Research and development would be required to handle the time dimension of the data and generate it in a suitable and attractive format. Possibilities for output using this approach include animations which display the progression of environmental changes or images which display the same information statically.

It is likely that the libraries identified in previous stages will be used in this step, however new data representations such as animations could entail additional third-party components. Besides the specifics of implementation, more general points about the format of the data visualization will be necessary to produce a useful and attractive end product. Objective criteria for determining data visualization include pertinent trends or conventions in environmental literature which would be desirable to emulate. Subjective criteria include choices about color and contrast which in addition to affecting the aesthetic quality of the application may also influence usability and accessibility.

The research, design choices, and implementation of this module will produce results which cap off the suite of data processing algorithms provided by COAL. Ranking and documenting environmental changes over time combines the information from all earlier steps into a unified whole for producing temporal analyses to inform environmental data scientists and other interested parties. Design and implementation could be simple, but depending on the time left available for developing this step after the others, more sophisticated approaches could be devised to enhance usability.

% heidi: requirement 3.5: api documentation
\subsubsection{API Documentation}
\label{design:subsec:apidoc}
The Sphinx API documentation will be developed throughout the entirety 
of the project, with each team member documenting their code 
contributions as they develop. The code written will include 
docstrings for all functions that will include, at minimum, a short 
description of the purpose of the function, description of each 
parameter, and the input and output of the function. Further 
docstrings in the code will include explanations of algorithms and 
structures. We will then use reStructuredText (.rst) files to 
integrate any function descriptions with further elaboration and any 
examples, figures, and/or images. We will have a main page for the 
documentation that includes a table of contents, a search bar, and a 
general description of the API. We will also have a page regarding how 
to use COAL's API and a page for each library. The goal of the 
documentation is to be useful to a person who wants to use it in their 
software project but does not necessarily want to know everything that 
is going on in explicit detail. It is a description of the API as a 
black box for the most part, though the source code is available to 
those who want more detail and is linked on the homepage 
\ref{design:subsec:frontendframework}.

The main page has an auto-generated table of contents and search bar, 
so the only thing we will be writing on the main page is the general 
description of the API. The page regarding using the COAL API will 
include information on dependencies, compatibility with AVIRIS 
imagery, and examples showing how to get started with COAL. Each page 
documenting a library will have a short description of the library at 
the top, followed by a description of each function (documented in the 
code itself with docstrings). If applicable, any concerns regarding 
compatibility with other libraries or performance should be addressed 
here. This is particularly important since image processing takes a lot 
of computing power. The specifics of algorithms and statements are not 
necessary for the auto-generated documentation. We will put comments 
in the source code where necessary for people who want more detail, 
but the API documentation will only include specifics if they raise 
any kind of concern.

Testing will be relatively simple to make sure the API documentation 
is working correctly. Syntax errors are raised by Sphinx if they are 
encountered, so they are easy to check for. We will also be doing 
spell checking on all of our documentation to make sure it is clear 
and professional. We will also check to make sure that the 
documentation is output in a place that is accessible to users. 

Members of this team have used Sphinx documentation before and found 
it elegant, flexible, and easy to read and write. It is also the 
method that Python uses for its documentation, which is frequented by 
members of this team and is seen as nicely formatted and organized. 
Sphinx is also compatible with Python 2 and Python 3, which makes it a 
good match for us if we ever decide we want to switch completely to 
Python 3. 

% xiaomei
\subsubsection{Static site generator}
\label{design:subsec:staticsitegenerator}

We will use a static site generator to provide a fast and simple back-end for COAL's homepage. GitHub let users create one site per GitHub account or organization, which is convenient for us for that we have to keep our page content dynamically. %unclear what this sentence saying
What we will do first is create a GitHub page. The GitHub page can host personal, organization, or project pages directly from a GitHub repository. GitHub pages use github.io domains which are served over HTTPS. The master branch of the repository is responsible for t'she publishing; this can be selected from the GitHub Pages site repository under the settings option. Once we have it linked, we will start to build our website. The advantage of the static site generator is that it does not need to deal with databases. The static site generator mostly works with Javascript and HTML and that is why it has fast speed and relatively better performance. The main purpose of our website is to display the resources we need and make it convenient for our team to perform our tasks, so a static site generator is the perfect choice for us. Our site will not require many complicated functionalities, so a more complex back-end is not needed. Also, because we need to change the content on the home page often as the project continues, static site generators tend to have excellent version control for the content. 

As the technology review states, we are going to use Jekyll as our static site generator. Jekyll is a simple, blog-aware, static site generator perfect for personal, project, or organization sites. We chose Jekyll as our static site generator because it has high compatibility with GitHub pages, which makes the work easier to do. To start to use Jekyll, we will simply download %download what?
from the GitHub pages or the official website. Once we get the resources we need, we will create the back end to our home page. We do not have any experience with using Jekyll, but the guide on the Jekyll website is simple to follow. 

Overall, using a static site generator is not a difficult task and there are a lot of resources online. Choosing a static site generator will save us time with maintenance and server resources.


% xiaomei
\subsubsection{Front-end framework}
\label{design:subsec:frontendframework}

Due to the continuous nature of our project, we will need a website to show our work and progress both throughout the school year and possibly beyond. There will not be much content to put on the site toward the beginning of research and development, but the content will be filled out as the project progresses. The website serves the purpose of showing our work and presenting our project in a clear and succinct way.

We use plain HTML to build the skeleton of the website, then we put the basic information on, such as developer profiles, project information, and relevant links (Github, class website, etc.). %expand here, like a lot. how will you structure the pages in terms of columns? what exact information will be in the developer profiles? also, mention a page including links to research papers on image processing, aviris, and general mining here instead of above like it was
Xiaomei will be using a template for a navigation bar that she has used before so that sh can build a simple page before filling everything inside. 

Then, based on what is required, or what we want to put on our website later, we may need to alter the content. For the styling, we will use CSS. Our team has discussed that we would mostly use Bootstrap for our front-end framework, for the abundance of its themes and functionalities. %not super important to include, but what specific theme did you use/do you maybe plan on using?
Bootstrap is easy to use and works well with HTML, CSS, and JS; we choose it because of it saves us time so we can build a basic page very quickly. We also need some pictures related to mining or AVIRIS to make our site more aesthetically pleasing and to provide interest. There are many images of AVIRIS and its flight lines, so this should not be an issue. We will need to search for some techniques to fit different blocks for various using and make them look nice. %various using?

As the project continues, we will update our homepage on a weekly basis as well as link to our individual weekly updates for required for the computer science senior design course.

\subsection{Timeline}
The Gantt chart pictured in Figure \ref{design:figure:gantt} describes the project timeline. Dependencies among each stage of the project are depicted by arrows and approximate implementation dates are indicated by the date range. Some components may be able to be developed independently while others may require a sequential approach. In general, however, our development will follow the data from initial processing to final representations. API documentation will parallel the development of our source code using comments to automatically generate browsable reference material. The website will provide a hub for developers, end users, and the general public describing our project, and its development will be relatively independent of the algorithmic components.

% academic calendar: http://catalog.oregonstate.edu/ChapterDetail.aspx?key=148
% engineering calendar: http://eecs.oregonstate.edu/industry-relations/calendar
% course timeline: http://eecs.oregonstate.edu/capstone/cs/capstone.cgi?home=1

%\newcommand{\firstdayoffallterm}{2016-09-21}      % first day of fall term
%\newcommand{\startday}{2016-10-02}                % day groups assigned
%\newcommand{\fallprogressreportdue}{2016-12-05}   % finals week of fall term
%\newcommand{\alphareleasedue}{2017-02-13}         % week 6 of winter term
%\newcommand{\betareleasedue}{2017-03-20}          % finals week of winter term
%\newcommand{\winterprogressreportdue}{2017-03-20} % finals week of winter term
%\newcommand{\releasedue}{2017-05-15}              % monday prior to tentative expo date
%\newcommand{\expoday}{2017-05-19}                 % tentative expo date
%\newcommand{\finalreportdue}{2017-06-12}          % finals week of spring term
%\newcommand{\lastdayofspringterm}{2017-06-16}     % last day of spring term

\begin{figure}

  % gantt chart: http://mirrors.rit.edu/CTAN/graphics/pgf/contrib/pgfgantt/pgfgantt.pdf
  \begin{ganttchart}[x unit=0.15em, time slot format=isodate, link bulge=4]{\startday}{\lastdayofspringterm}

    % gantt chart title
    \gantttitlecalendar{year, month=shortname} \\

    % gantt chart bars
    \ganttbar[name=mineralid]{Feature Extract Mineral Identification}{\fallprogressreportdue}{\alphareleasedue} \\
    \ganttbar[name=mineralidclass]{Classification Mineral Identification}{\fallprogressreportdue}{\alphareleasedue} \\
    \ganttbar[name=miningid]{Identify Mining}{\alphareleasedue}{\betareleasedue} \\
    \ganttbar[name=impact]{Preprocess Environmental Impact}{\betareleasedue}{\releasedue} \\
    \ganttbar[name=impactfeature]{Feature Extract Environmental Impact}{\betareleasedue}{\releasedue} \\
    \ganttbar[name=changes]{Document Changes}{\betareleasedue}{\releasedue} \\
    \ganttbar{API Documentation}{\fallprogressreportdue}{\releasedue} \\
    \ganttbar{Static site generator}{\betareleasedue}{\finalreportdue} \\
    \ganttbar{Front-end Framework}{\betareleasedue}{\finalreportdue} \\

    % gantt chart links
    \ganttlink{mineralid}{mineralidclass}
    \ganttlink{mineralidclass}{miningid}
    \ganttlink{miningid}{impact}
    \ganttlink{impact}{impactfeature}
    \ganttlink{impactfeature}{changes}

  \end{ganttchart}

  \caption{Gantt Chart: Timeline of Project Tasks}

  \label{design:figure:gantt}

\end{figure}

\subsection{Conclusion}
\label{design:sec:conclusion}
This document has outlined the design of the COAL project and the suite of algorithms and documentation it provides. The major components of the system were broken down and assigned to each team member who described in each section the research priorities and design decisions involved. Processing large datasets of imagery and geographic information require a significant investment to study and develop solutions. However, the results will provide a wealth of information for environmental data scientists and other concerned parties looking for associations between mining and environmental damage. Further revisions of this document will reflect specific choices made as the project matures.

\subsection{Glossary}
\begin{description}[\IEEEsetlabelwidth{docstring}]
\item[AVIRIS] Airborne Visible/Infrared Imaging Spectrometer
\item[COAL] Coal and Open-pit surface mining impacts on American Lands
\item[docstring] Documentation string. In Python, they are comments that start and end with three quotation marks.
\item[GIS] Geographic Information System
\end{description}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Changes to Design}
The design document had a lot of changes throughout both Winter 2017 and Spring 2017. When we first had to write the design document, we were still in the research phase and did not know how we were going to approach many parts of COAL, so we wrote about some possibilities that we ended up not using.

The first iteration of the design document mentions blob detection and neural networks for the "Feature extraction in AVIRIS data" \ref{design:subsec:featureextraction} and "Classification of AVIRIS data" \ref{design:subsec:classification} sections. The method of feature extraction talks about blob detection and the classification method talks about neural networks. A second iteration scraps blob detection and talks about neural networks for feature extraction as well and makes a few edits to the classification method to make it specific to Spectral Python. The last iteration of the design document reflects a big change in direction we had to make. We spoke to the maintainer of Spectral Python and realized our data was not compatible with the neural network method, so we switched to a spectral angles mapper. This meant that the final feature extraction section is now about the use of spectral libraries and the final classification section is now about the spectral angles mapper.

The mining identification section of the original design document was not revised, however the general principles it described remained accurate. The prediction that ``mapping the presence of a mineral may be sufficient to locate mines: minerals with low natural concentrations can be used to identify mining activity'' was ultimately validated by our identification of minerals associated with acid mine drainage and coal mine waste.

The sections pertaining to environmental correlation and temporal analysis were completely revised as research progressed. Sources of geographic data such as the National Hydrography Dataset and software tools such as GDAL and QGIS were identified throughout the course of the project. The general principles describing the data processing pipeline were maintained, however the specific details were not known during the first draft of the design document. The division between preprocessing and feature extraction was found to be arbitrary and not ultimately relevant, although the environmental correlation module was divided into a number of subroutines. The temporal analysis module had to be reinterpreted as temporal data was not available for our case study. Our client ended up implementing a science data system (SDS) to complement COAL that would allow for bulk processing of AVIRIS data over time.

The sections regarding API documentation and the website were revised slightly but were not significantly changed.

\section{Original Technology Review}

%% Your tech review, in its original form. Did you change your mind about any technologies? What had to change?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Introduction to entire tech review, including authorship of each section
\subsection{Introduction}
\label{sec:intro}
This document is a review of technology and literature relevant to the COAL project. The technologies discussed in this paper correspond to tasks or subtasks identified in our requirements document \cite{requirements}: 
\begin{itemize}
\item Develop software to process imagery
\item Provide API documentation for developers
\item Provide a website that serves as the project homepage
\end{itemize}
At this point in the project our team is working with the client to become familiar with the data and the available literature, so some material is subject to change as our understanding improves.

Sections \ref{subsec:featureextraction} and \ref{subsec:classification} were authored by Heidi who will take responsibility for the algorithms to identify and classify minerals. Section \ref{subsec:identifymining} was authored by Taylor on behalf of Xiaomei, who will take responsibility for the algorithms to identify regions of mining activity. Sections \ref{subsec:preprocessing}, \ref{subsec:correlate}, and \ref{subsec:changesovertime} were authored by Taylor who will take responsibility for the algorithms to correlate mining with environmental impact and to rank and document changes over time. Section \ref{subsec:apidoc} was written by Heidi who will take responsibility for the API documentation. Sections \ref{subsec:staticsitegenerator} and \ref{subsec:frontendframework} were written by Xiaomei who will take responsibility for implementing the backend and the frontend of the project homepage.

\subsection{Technologies}

% heidi: requirement 3.4.1 part 1: choose technology for mineral identification and classification
\subsubsection{Feature extraction in AVIRIS data}
\label{subsec:featureextraction}

\subsubsubsection{Options}
\begin{enumerate}
\item Automatic scale selection 
\item Spoke filter
\item Template matching
\end{enumerate}
\subsubsubsection{Goals}
In order to properly analyze the AVIRIS images, COAL needs a way to identify regions in an image that indicate a particular mineral or other pattern. Blob detection is a good way of doing this, as it is a way of separating out regions that differ from their surroundings.

\subsubsubsection{Criteria}
The main criteria being evaluated are sensitivity to noise, ability to detect many different shapes, and speed.

\subsubsubsection{Comparison}
\begin{tabular}{ | l | l | l | l |}
    \hline
    & Automatic scale selection & Spoke filter & Template matching \\ \hline
    Sensitive to noise & $\times$ & \checkmark & \checkmark \\ \hline
    Good for non-homogeneous shapes & \checkmark & $\times$ & $\times$ \\ \hline
    Speed & slow & fast & fast \\
    \hline
\end{tabular} \\ \\
This table shows an overview of the automatic scale selection, spoke filter, and template matching blob detection methods against noise sensitivity, ability to detect blobs of any shape, and speed of detection \cite{blobdetection}.

\subsubsubsection{Discussion}
The spoke filter and template matching methods are both fairly fast, however they do not meet the crucial requirement of being able to detect blobs of many different shapes. The spoke filter method does a poor job of blob detection on any non-circular blob. The template matching method only matches blobs that match a template pattern. Both are also sensitive to noise, which is often present in AVIRIS data. Automatic scale selection is the slowest method of the three options as, indicated by the name, it uses multiple scales. However, it is also insensitive to noise and can detect all kinds of shapes, not just circular shapes or shapes provided by a template. In this case, the slow speed is outweighed by the flexibility \cite{blobdetection}.

\subsubsubsection{Selection}
We will be using automatic scale selection.

\subsubsection{Classification of AVIRIS data}
\label{subsec:classification}

\subsubsubsection{Options}
\begin{enumerate}
\item Conjugate-gradient backpropagation (CGBP)
\item Gaussian maximum likelihood (GML)
\item Minimum Euclidean distance (MED)
\end{enumerate}
\subsubsubsection{Goals}
Before any further data analysis can take place, classifying the AVIRIS data into classes based on mineral concentrations and categories such as vegetation and bodies of water will be needed in order to have variables to correlate (or not) with the environmental effects of mining. 

\subsubsubsection{Criteria}
The main criteria being evaluated are accuracy based on tests performed in previous research and any aspects of the algorithms that could affect the accuracy during the training phase.

\subsubsubsection{Comparison}
\begin{tabular}{ | l | l | l | l |}
    \hline
    & CGBP & GML & MED \\ \hline
    Test accuracy & 94.083\% & 90.48\% & 71.675\% \\ \hline
    Confounding factors & $\times$  & \checkmark & $\times$ \\
    \hline
\end{tabular}
\\ \\ This table shows an overview of CGBP, GML, and MED against the average overall test accuracy and presence of any confounding factors (yes or no) in a set of experiments conducted on AVIRIS data collected on Icelandic lands \cite{aviris}. GML is the only option that has a confounding factor. 

\subsubsubsection{Discussion}
In the experiments performed on AVIRIS data, there were no significant confounding factors that had an effect on the accuracy of the data when using the CGBP method. Just looking at the accuracy of the GML method, it looks like a fairly good choice, however this is a limitation to its use when using very high dimensional data like AVIRIS. The GML method relies on having "nonsingular (invertible)
class-specific covariance matrices for all classes \cite{aviris}." Since there are often many dimensions to the data, there are cases when the sample size is smaller than the number of classes, which would make the data break the requirement of being nonsingular. The MED method performed the lowest in terms of accuracy and like the CGBP method, had no confounding factors that limited its usefulness. 

\subsubsubsection{Selection}
We will be using the conjugate-gradient backpropagation method for classifying AVIRIS data.

% xiaomei: requirement 3.4.2: identify mining
\subsubsection{Identify Mining}
\label{subsec:identifymining}
To identify regions of mining activity, the mineral identification and classification data is further processed and possibly combined with geographic information about geology and mines.

\subsubsubsection{Options}
Three approaches to identify mining are:
\begin {enumerate}
\item mineral classification, using patterns of mineral deposits \cite{leadville,raymine};
\item GIS comparison, using geographic information such as mining permit boundaries \cite{movingmountaintops} or geologic maps \cite{raymine}; and
\item a combination of mineral classification and GIS comparison, using both sources of data.
\end {enumerate}

\subsubsubsection{Goals}
The goal of the mining identification step is to accurately locate regions in which mining is taking place in a form that is suitable for both human inspection and further processing. It uses the output of the mineral identification step which located regions containing minerals associated with mining. Because these minerals may be derived from both natural and artificial processes \cite{raymine}, additional logic or data may be necessary to distinguish mines from preexisting geology. Because existing geographic data may be unreliable or incomplete, some combination of mineral classification with geographic information may produce more accurate results.

\subsubsubsection{Criteria}
Criteria being evaluated include the time needed to research and develop each method, the computational cost of processing the data, and the accuracy of the results.

\subsubsubsection{Comparison}
\begin{tabular}{ | l | l | l | l |}
    \hline
                           & Research and Development Time & Computational Cost & Accuracy \\ \hline
    Mineral Classification & Low to Medium                 & Low to Medium      & Medium   \\ \hline
    GIS Comparison         & High                          & Medium             & Medium   \\ \hline
    Combination            & High                          & Medium to High     & High     \\
    \hline
\end{tabular}
\newline
\newline
This table compares each option based on the criteria. Because the options are general approaches to algorithm design and not specific libraries, it was necessary to estimate the costs and benefits.

\subsubsubsection{Discussion}
Mineral classification by itself can be a sophisticated method of identifying traces of mining activity. Minerals with low natural concentrations can be used to identify mining activity, as jarosite was used to map active mining operations in the Ray Mine in Arizona \cite{raymine}. In this case, simply mapping the presence of a mineral may be sufficient to locate mines. In other cases, minerals associated with mining cannot be detected by imaging spectroscopy, such as pyrite in the California Gulch Superfund Site \cite{leadville}. The solution in this case was to search for patterns of secondary minerals weathered from pyrite that are detectable by the sensors. Therefore the mineral classification method can be simple or complex depending on the chemistry of the minerals of interest and the logic required to find them.

GIS comparison can also provide useful insights for identifying mines. One approach that has been used to successfully visualize mining activity is to overlay permit boundaries on top of classified spectroscopy data \cite{movingmountaintops}. This provided a visual comparison of known mines and their spectroscopic signatures. Another approach is to compare geologic maps with the observed minerals and look for discrepancies \cite{raymine}. This allowed naturally-occurring deposits to be distinguished from artificial ones. The comparisons between processed data and known geographic information could be made programmatically or simply displayed for human inspection.

A combination of mineral classification with GIS comparison would provide the most extensive analysis but would require the most research and development time and computational cost. Using both methods would require design choices such as whether to automate or display each result. For example, GIS comparison of geologic maps with observed minerals could be used to automatically eliminate deposits that are thought to be naturally-occurring; alternatively, both the geologic maps and the observations could be combined so that more data is displayed and less is lost. Similar choices would be necessary for mineral pattern detection or permit boundary mapping. Combining multiple data sources would be challenging and could make the results more or less accurate depending on the circumstances.

\subsubsubsection{Selection}
Based on these criteria, it is recommended to pursue the mineral classification methods first in order to locate regions with mineral deposits that indicate mining. If this can be completed successfully, then evolving a combination of mineral classification and GIS comparison methods would provide an opportunity to improve the analysis.

% taylor: requirement 3.4.3 part 1: preprocessing data to correlate mining with environmental impact
\subsubsection{Preprocessing Data to Correlate Mining with Environmental Impact}
\label{subsec:preprocessing}
To correlate the regions of mining activity with the regions of environmental degradation, the data sources must be preprocessed to be compatible.

\subsubsubsection{Options}
Three approaches to make the mining and environmental data compatible are:
\begin{enumerate}
\item conversion of mining activity data to GIS,
\item conversion of environmental impact data from GIS, and
\item conversion of both data sources to a third format.
\end{enumerate}

\subsubsubsection{Goals}
The goal of the preprocessing step is to transform the input data, assumed here to be spectroscopic data and geographic information, such that the following feature extraction step can find meaningful relationships. Because this is an intermediate step, no data visualization is generated so the formats may be only machine-readable.

\subsubsubsection{Criteria}% being evaluated (e.g., cost, availability, speed, security, etc)}
Criteria being evaluated include the time needed research and develop each method, the compatibility of the format with existing standards, and the retention or loss of precision produced by each.

\subsubsubsection{Comparison}
\begin{tabular}{ | l | l | l | l |}
    \hline
                 & Research and Development Time & Standard & Precision        \\ \hline
    To GIS       & High                          & Yes      & Medium           \\ \hline
    From GIS     & Medium                        & No       & Medium to High   \\ \hline
    Third Format & Medium to High                & Maybe    & Medium to High   \\
    \hline
\end{tabular}
\newline
\newline
This table compares each of the possible data formats with estimates for each of the criteria.

\subsubsubsection{Discussion}
Transforming the spectroscopic data to GIS for combination with the environmental data would produce a standard intermediate format. This format would not necessarily be best suited for the following feature extraction step, and it is speculated that data would inevitably be lost. Furthermore, it would require significant research and development time to transform the processed imagery into a geographical format.

Converting the environmental impact data from GIS so that it is compatible with the processed imagery would generate a nonstandard format, but one that can arguably be better tailored to the application. Research and development time is estimated to be lower for this approach.

Converting both the processed imagery and the geographic information to a third format would provide a choice about compatibility and suitability for further processing. The research and development time for this approach is estimated to be higher.

\subsubsubsection{Selection}
Since this is an intermediate step, standards compatibility is less important than research and development time, and precision is more important. Based on these estimates, it is recommended to preprocess data by converting the geographic information so it can be combined with the processed imagery.

% taylor: requirement 3.4.3 part 2: feature extraction to correlate mining with environmental impact
\subsubsection{Feature Extraction to Correlate Mining with Environmental Impact}
\label{subsec:correlate}
To correlate the regions of mining activity with the regions of environmental degradation, relationships must be extracted from the preprocessed data.

\subsubsubsection{Options}
Having combined the mining and environmental data in the previous step, options for finding meaningful relationships include:
\begin{enumerate}
\item simple logic,
\item statistical analysis, and
\item machine learning approaches.
\end{enumerate}

\subsubsubsection{Goals}
The goal of this stage is to accurately locate regions where mining coincides with environmental degradation. The result should be suitable for both human analysis as well as further processing.

\subsubsubsection{Criteria}
Criteria for evaluating these approaches include time needed to research and develop each method, the computational complexity of each, and the accuracy of the results.

\subsubsubsection{Comparison}
\begin{tabular}{ | l | l | l | l |}
    \hline
                     & Research and Development Time & Computational Cost & Accuracy \\ \hline
    Logic            & Low                           & Low                & Low      \\ \hline
    Statistics       & Medium                        & Medium             & Medium   \\ \hline
    Machine Learning & High                          & High               & Medium to High   \\
    \hline
\end{tabular}
\newline
\newline
This table compares each algorithmic approach with estimates for each of the criteria.

\subsubsubsection{Discussion}
The simplest way to correlate mining and environmental impact is to identify areas where both are present. Like a Venn diagram, this approach would provide a basic boolean description of regions where both zones intersect. Although it would require very little research and development time to apply logical AND to each pixel, the accuracy is estimated to be low. Furthermore, this method would not be well suited for analyzing results that are probabilistic in nature.

Another way to correlate the data is to use statistical methods to identify zones that are likely to correspond to both mining and environmental data within some level of uncertainty. It would require more research and development to find and implement methods to process the data this way, but the accuracy is estimated to be better. The computational cost of this approach is estimated to be higher.

The most complex way to correlate the data is to use machine learning approaches such as the neural networks which were used to classify the spectroscopic imagery. This would require more research and development time and much more computational complexity than either of the other approaches. It is unknown whether the accuracy of this approach for correlation would exceed that of the statistical method, but it estimated to be as good or better.

\subsubsubsection{Selection}
At present, we have insufficient information to come to a firm conclusion. Further research is recommended to determine which approach is best suited to our needs. Because the logical approach is so simple, the little cost that has been estimated may be worth the small benefit of analyzing data this way. The statistical approach is hypothesized to be the best suited for this step, however machine learning should be chosen if research determines it is more accurate and achievable under our time constraints.

% taylor: requirement 3.4.4: rank and document changes over time
\subsubsection{Rank and Document Changes Over Time}
\label{subsec:changesovertime}
To rank and document changes over time, data from a range of dates should be compared using correlations derived from the previous step.

\subsubsubsection{Options}
Several approaches to correlating imagery and historical data include:
\begin{enumerate}
\item processing separately and comparing manually,
\item processing separately and comparing automatically, and
\item processing simultaneously and comparing automatically.
\end{enumerate}
Each option describes a high-level approach to designing the algorithm. Implementation details to be determined based on choice.

\subsubsubsection{Goals}
The goal of this stage is to add a temporal dimension to the spatial correlation between mining and environmental impact. It will require processing multiple historical datasets which may be disjoint: For example, imaging spectroscopy data gathered from different dates may have different geographic coordinates or atmospheric conditions. The end user should be able to select a geographical region of interest and view a sequence of correlations that compensate for varying observation conditions.

\subsubsubsection{Criteria}
Criteria being evaluated include the time needed to research and develop each method, the computational complexity of each approach, and the quality of the results.

\subsubsubsection{Comparison}
\begin{tabular}{ | l | l | l | l |}
    \hline
                                                     & Research and Development Time & Computational Cost & Quality \\ \hline
    Process Separately, Compare Manually          & Low                           & High               & Medium      \\ \hline
    Process Separately, Compare Automatically     & Medium                        & High               & Medium to High   \\ \hline
    Process Simultaneously, Compare Automatically & High                          & Very High          & Medium to High   \\
    \hline
\end{tabular}
\newline
\newline
This table compares each approach to algorithm design and the estimated costs and benefits of each.

\subsubsubsection{Discussion}
The simplest approach is to process the data separately and compare the results manually. Because this simply runs the previous stages over multiple data sets, the research and development time is low. However, the quality of this approach is not high because the end user shoulders responsibility for comparing the results.

A more sophisticated design is to devise a means of combining multiple correlations and producing a unified result. This approach would reuse most of the original data flow and provide a comparison of the results. If variables such as geographic coordinates and atmospheric conditions could be successfully postprocessed by this method, the quality of the results is estimated to be better than the previous approach.

The most complicated approach would be to process multiple historical datasets simultaneously to mediate their transformations. This would require more research and development and likely some degree of refactoring. All of these approaches require a high computational cost, but this one is estimated to have the highest. However, if successfully implemented, this method is estimated to have the highest quality because each of the datasets is available for synchronization from beginning to end.

\subsubsubsection{Selection}
More research is recommended to determine which options are best suited for the project. The third approach is arguably the most elegant since it encapsulates the entire process, however the second approach is likely to be more achievable within the timeframe. The first approach puts more responsibility on the end user than desirable, however with a low implementation cost it may be worth implementing for experimental purposes.

% heidi: requirement 3.5: api documentation
\subsubsection{API Documentation}
\label{subsec:apidoc}

\subsubsubsection{Options 1, 2, and 3}
\begin{enumerate}
\item Sphinx
\item Doxygen
\item Epydoc
\end{enumerate}

\subsubsubsection{Goals}
Auto-generated documentation is desirable in the design on COAL 
because there will be code used to initialize data sets from large data files, process said data, and visualize such data. These are complex tasks that will not have the most straight-forward and simple methods of implementation, so clear, comprehensive, and organized documentation would be helpful to anyone who potentially wants to use COAL on a cloud-based platform in the future.

\subsubsubsection{Criteria}
The main criteria being evaluated when comparing documentation generators is compatibility with Python 2 and Python 3, search engine integration, and use of reStructuredText.

\subsubsubsection{Comparison}
\begin{tabular}{ | l | l | l | l |}
    \hline
    & Sphinx & Doxygen & Epydoc \\ \hline
    Python 2 and 3 support & \checkmark & \checkmark & $\times$ \\ \hline
    Search engine & \checkmark & \checkmark & $\times$ \\ \hline
    reStructuredText & \checkmark & $\times$ & $\times$ \\
    \hline
\end{tabular}
\\ \\ This table shows an overview of Sphinx, Doxygen, and Epydoc against the criteria. Sphinx meets all three criteria, Doxygen misses the mark on reStructuredText, and Epydoc does meet any of the criteria.

\subsubsubsection{Discussion}
Epydoc is a widely used documentation generator for Python code. However, it was abandoned in 2009
and therefore does not have Python 3 support. It also does not offer a search engine feature, which is 
an incredibly handy feature to have when trying to find a method or class that meets a certain criteria. It also
uses Markdown instead of reStructuredText, making it fall behind in the ease of use and elegance department. Doxygen fits more of the criteria, but it is a bit tricky since Doxygen by itself does not
support Python documentation. It only does so through the use of a third-party generator like Epydoc. Sphinx fits all of the criteria and is the only choice that uses reStructuredText. reStructuredText provides mechanisms known as roles and directives that will render parts of the documentation to HTML automatically, making the documentation cleaner by not having to embed HTML, which is a common experience in Markdown \cite{nomarkdown}. 


\subsubsubsection{Selection}
We will be using Sphinx for the auto-generated documentation.

% xiaomei: requirement 3.7 part 1: website backend
% e.g., plain html, static site generators like jekyll, etc.
\subsubsection{Static site generator}
\label{subsec:staticsitegenerator}

\subsubsubsection{Options}
\begin {enumerate}
\item Middleman 
\item Jekyll
\item Roots
\end {enumerate}

\subsubsubsection{Goals}
A static site generator will provide a fast and simple back-end for COAL's homepage. The homepage will not be dependent on real-time content, so we deemed a static site generator appropriate for the back-end.

\subsubsubsection{Criteria}
\begin {enumerate}
\item Cost to build.
\item Availability: easy to attain.
\item Compatibility
\end {enumerate}

\subsubsubsection{Comparison}
\begin{tabular}{ | l | l | l | l |}
    \hline
    & Middleman & Jekyll & Roots \\ \hline
    Easily extendable & $\times$ & \checkmark & \checkmark \\ \hline
    Built-in Github support & $\times$ & \checkmark & $\times$ \\
    \hline
\end{tabular}

\subsubsubsection{Discussion}
Jekyll is defined as “a simple, blog-aware, static site generator” and it is the most widely used today. Jekyll is also very easy to use. It has default integration with GitHub pages, which makes set-up and updates very simple.
Middleman is not as widely used. Middleman does not have built-in support with GitHub pages, so it is more difficult to set up and update compared to Jekyll. Middleman is also difficult to write extensions for as it is not well documented. Roots is less used than Middleman, but it relies heavily on extensions and is easily extendable \cite{staticsite}.

\subsubsubsection{Selection}
Jekyll seems to be the best choice after our discussion. Since we are using GitHub pages, it will be the simplest to use and fits our needs.

% xiaomei: requirement 3.7 part 2: website frontend
% e.g., plain stylesheets, frameworks like bootstrap, etc.
\subsubsection{Front-end framework}
\label{subsec:frontendframework}

\subsubsubsection{Options}
\begin {enumerate}
\item Bootstrap
\item Skeleton
\item Foundation
\end {enumerate}

\subsubsubsection{Goals}
Our site should look professional and have a simple, user-friendly interface that is also aesthetically pleasing. In order to accomplish this, a front-end framework is desirable.

\subsubsubsection{Criteria}
\begin {enumerate}
\item Well designed interaction
\item Professional-looking Interface 
\item Contains required contents
\end {enumerate}

\subsubsubsection{Comparison}
\begin{tabular}{ | l | l | l | l |}
    \hline
    & Bootstrap & Skeleton & Foundation \\ \hline
    Abundance of themes & \checkmark & $\times$ & \checkmark \\ \hline
    Highly mobile friendly & \checkmark & \checkmark & \checkmark \\ \hline
    Large community support & \checkmark & $\times$ & \checkmark \\
    \hline
\end{tabular}

\subsubsubsection{Discussion}
Skeleton is fairly bare-bones, so it isn't ideal for when developers want many themes to choose from \cite{bootfoundskeleton}. Skeleton is also a much less active project. At the time of writing this document, the last update made on the official GitHub page for Skeleton was made in December of 2014. Bootstrap and Foundation are both very popular and active projects that compare pretty evenly. Bootstrap has a slightly different approach to the mobile version of a site than Foundation does. Foundation was developed under the assumption that "anything not under a media query is considered mobile \cite{bootstrapvsfoundation}." Bootstrap will only design something for mobile if specified. This is not very important to COAL's homepage though. Both are compatible with Jekyll, our choice for a static site generator. The deciding factor between Bootstrap and Foundation comes down to the preferences and experiences of the team members. We have experience with Bootstrap and enjoy the look and feel of it, so it is our choice for COAL's homepage's front-end.

\subsubsubsection{Selection}
We will be using Bootstrap for the front-end framework of COAL's homepage.

\subsection{Conclusion}
In this paper we described and compared technologies for implementing the COAL algorithm suite and associated documentation. The core functionality consists of a data processing pipeline that transforms raw imagery and data into meaningful relationships. At this point in our group's research we have collected the most literature on the beginning stages, so later stages are described at a less granular level. In addition to the algorithms, our project will be supported by API documentation for programmers and a homepage for end users. A discussion of implementation choices for each component was provided along with a recommendation based on a comparison of costs and benefits. The team members responsible for each component were identified in the introduction \ref{sec:intro}. We hope that this discussion has provided the reader with a better understanding of the design choices our team faces as we continue to collaborate with our client on research and development.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Changes to Technology Review}
Similar to the design document, we had to write the technology review before we had a solid idea of what direction we wanted to go. Therefore, we made a lot of changes along the way. In addition, responsibility for implementing some of the requirements was shifted or shared between team members.

The first iteration mentions methods of blob detection in the "Feature extraction in AVIRIS data" \ref{subsec:featureextraction} section and the "Classification of AVIRIS data" \ref{subsec:classification} section talks about statistical methods used in tandem with a neural network in a study done with AVIRIS data. These were then changed to talk about different neural network technologies. For both, Spectral Python was used. The final iteration still chooses Spectral Python for feature extraction, but the discussion talks about spectroscopy libraries that explicitly support spectral libraries. The final classification section compares different methods of classification within Spectral Python and chooses the spectral angle mapper as the final choice. 

Mining identification (section \ref{subsec:identifymining}) was ultimately completed by using patterns of mineral deposits corresponding to coal sludge and acid mine drainage. Geographic information was used as a complementary source of data but was not used for automated processing. Using spectral samples from the USGS Digital Spectral Library 06 as proxies for the presence of coal mine alteration allowed us to identify mines from mineral-classified files without significant computational cost.

Environmental correlation (sections \ref{subsec:preprocessing} and \ref{subsec:correlate}) was eased because it turned out that both the mining data and the environmental data were both already in a georeferenced format. It was still necessary to convert vector data into a raster format so that the two datasets could be superimposed programmatically. We avoided developing application-specific data formats and instead preferred to generate data products in the same ENVI raster format used by the original AVIRIS files. After combining the data sets, we used simple logic to find where mine alteration corresponded to water resources. This was elaborated slightly by calculating the intersection within a certain proximity of stream flow lines. More sophisticated statistical and machine learning approaches could improve accuracy, however there was not time to implement these methods.

The temporal analysis (section \ref{subsec:changesovertime}) was ultimately implemented as a science data system (SDS) by our client who is a specialist in data science. This toolkit allows scientists to batch process AVIRIS data using the COAL library using cloud and supercomputing resources. It was left to end users to define the preferred data processing strategy appropriate for their specific application.

No change was needed for the API documentation or the website technology (sections \ref{subsec:apidoc}, \ref{subsec:staticsitegenerator}, and \ref{subsec:frontendframework}), although the Sphinx documentation was also pushed to the hosting service Read the Docs by our client.

\section{Weekly Blog Posts}

%% Your team weekly blog posts. These should be formatted nicely and clearly distinct from one another. This should include all members posts, clearly distinct. I recommend organizing these very carefully so that it doesn't look absurd. 

%% term date / person

\subsection{Fall 2016}
\subsubsection{2016-10-14}
\subsubsubsection{Taylor}
After I received the project assignment, I contacted my teammates Heidi and Xiaomei to coordinate our introduction to the client, Dr. Lewis John McGibbney at JPL. We were invited to collaborate via email, Google Groups, and GitHub. After making introductions, we arranged to meet using a video conference since our clients are out of state and since I commute to campus. It was necessary to reschedule our initial meeting because of time conflicts.

During our first meeting each of us said a few things about ourselves and our interests. Dr. McGibbney was joined at JPL by Dr. Kim Whitehall. After introductions we discussed basic logistics and considered the problem statement in simple terms. At a high level, our project aims to process sensor data to derive useful observations about the impact of mining operations. Our minutes were kept in a document in Google Drive along with other resources. We made plans to meet weekly, with Xiaomei, Heidi, and I reserving a study room in the library.

The clients set up a GitHub Organization for our project repository and any others that we contribute to in the course of development. After the meeting we focused on creating and typesetting the problem statement as well as doing preliminary research into existing literature and tools that would aid our project. There were some permission issues with GitHub so it was not possible to push the documents or the blog to the project repository this week. Finally, we arranged to meet with our TA Vedanth Narayanan (Vee) after working around each of our schedules.

\subsubsubsection{Heidi}
This week we had our first client meeting with Dr. McGibbney from NASA JPL as well as his colleague Dr. Whitehall. We mostly went over introductions and what our strengths were with regards to software. We have a fairly diverse team (C/C++, Python, back-end web APIs, front-end web development) and I think that will work in our favor. We also had to coordinate a weekly meeting time with our client and our TA which proved difficult since we're all busy and one member of the team is only available twice a week. We also got started with our problem statement document that we managed to get turned in at the last minute. So far, my impression is that we are working with people who have a lot of knowledge and experience and I'm looking forward to learning a lot and getting a basic workflow worked out in the future.

\subsubsubsection{Xiaomei}
During the first week, after I receive my project assignment, I meet with my teammates and get to know each other. We didn't have the chance to meet our client until a later time during the week two. Before that, we couldn't have much things done.

Next week, we will meet our client on Thursday, and meet our TA after that. We'll make our plan for the rest of the term, and hopefully to get more information from our client.

We do not have access to wiki until the end of the week three, that is why this is posted late.

\subsubsection{2016-10-21}
\subsubsubsection{Taylor}
This week we read up on literature provided by the client, continued to figure out the tools we use for collaboration including GitHub and Google Apps, and met to discuss the science and technology that will be relevant to our work.

The literature we reviewed included information on LANDSAT imagery and data processing, a description of the effects of mining in the Amazon, a case study about mineral mapping with imaging spectroscopy, and a short documentary on the world's largest mines. A software tool we are investigating is the Python library scikit-spectra which is described in a research paper. We are also identified the USGS Digital Spectral Library 06 as a likely data source for our project.

Shortly after meeting with the client to discuss these resources and collaboration tools, we met with our TA to make introductions and go over basic expectations. At the time of this writing we are still having technical issues posting the blog, so Heidi, Xiaomei, and I are using a repository on my personal GitHub while we troubleshoot.

\subsubsubsection{Heidi}
We had our second client meeting this week, this time during our agreed upon weekly meeting time. It's helpful to meet in person for our video conferences so we can go over our to-do list for the next seven days before our next meeting. Dr. McGibbney sent us a lot of information about spectroscopy libraries and research using them. We also previewed AVIRIS data that we will likely be using when we get further along in our project as well as some LANDSAT images. From our conversation today, we learned one of our challenges is going to be the fact that the data we will be analyzing and using for our project will be plentiful and in different formats, so we will need to be able to find a consistent way to aggregate it. We agreed to use the website Overleaf to edit our TeX documents. We also agreed that we will have a draft of the requirements document done by Wednesday so that there are a couple of days for Dr. McGibbney to give us feedback and for us to change the document accordingly. We also discussed getting a general workflow schedule started within the next couple of weeks and making sure that our wiki (this one) is set up. We had our first TA meeting right after our client meeting and discussed tips for making Capstone a better experience (try to be friends since we'll be working together until the end of Spring, don't be shy about constructive criticism or confrontation when it's needed) and what our TA Vee expects from us. We also discussed our progress on the problem statement, requirements document, and the wiki. Overall, the readings that I did this week as well as the information in the client meeting are making me more excited about working on this project.

\subsubsubsection{Xiaomei}
This week, my team and I received many documents from our client, all related to the project and possibly the tools we are going to use. Also, we meet our client on Friday and talk about the requirement documents that is due next week and some questions about what the project would look like. We also spoke of the documents that we received, and what's important for the team. After meet with my team and out client, we head to meet our TA and talk to him about our project and everything we have done last three weeks.

Next week, we expect to finish our requirement document. We have scheduled our regular meeting time with our client and our TA. So, we expect to get to start our project. Last three weeks seems like a familiar process for me. I think I will review the reading from our client last week again, and dip into the idea deeper. I am looking forward the project with excitement.
\subsubsection{2016-10-28}
\subsubsubsection{Taylor}
This week the team submitted the final Problem Statement and merged it into our repository. I used the Problem Statement and conversations with team members to create the initial version of the Requirements document. We met with the client and went through the document and discussed other action items to take care of. Afterwards we had our usual meeting with the TA. I created a skeleton of the website, set up a GitHub project management board for the team to try out, and finished the rough draft of the Requirements document for submission. We may use the service Overleaf to collaborate on our LaTeX documents.

\subsubsubsection{Heidi}
This week was a bit messy since we were all very busy and there was some miscommunication among us about due dates and signatures for our documents. We didn't think we would get our revised problem statement turned in on time on Wednesday, but Dr. McGibbney happened to get back to us almost immediately with a signature, so we ended up not being late after all. I was perhaps a bit hasty when I CC'd our professors about it in an email. Besides that bump in the road, this week has been productive and we're on a good schedule. We had a basic outline of our requirements document done by our meeting with Dr. McGibbney on Thursday so we were able to get some feedback and expand on some points in it. I'll be turning in the hard copy today (Friday) and we'll have our final draft done next week. We decided on some action items to be done by our next meeting, including reading up on Sphinx documentation and reStructured text, setting up a website and a blog where we can make group updates (not individual like this wiki). We decided to use Github for project management and went over using Overleaf for editing our TeX documents. We also obtained some AVIRIS flight line data to be able to study. Overall, I think we solidified what direction we are going in even more and again, I'm excited to be able to work on this project.

\subsubsubsection{Xiaomei}
This week, we start to work on the requirement document, which is due the next Friday. Also, more importantly, we discussed the workflows.

Depending on the feedback from Kevin, we provided a refined version of the problem statement on Wednesday. We meet with our client and our TA on Friday and discuss various of things. First, we decide to use Overleaf as a collaborate tool for our team. Second, we decide what kind of API documentation we are going to use, which is Sphinx and reStructured text. At last, we talked about what our task is.

Next, we will still work on the requirement document. Then, we will start work on the website, which is kind of what our final product is.
\subsubsection{2016-11-04}
\subsubsubsection{Taylor}
Heidi and I met with Dr. Winters to discuss issues with the Problem Statement. We had submitted the overview of the project provided by the client which was very detailed but did not meet all the requirements of the assignment. We agreed therefore to submit by the end of the term a Student Problem Statement containing the development perspective on the project in addition to the Client Problem Statement.

During our client meeting we discussed the work we have been assigned and the literature and libraries we should review. Based on these conversations I gained a better understanding of imaging spectroscopy and methods to analyze the data such as neural networks. My main objective this week after completing the writing assignments is to review the provided literature for a discussion of the research and a demo of the libraries at the next meeting.

During our TA meeting we discussed the Problem Statement, the Requirements Document, and team collaboration.

Heidi and I collaborated via Overleaf to revise the rough draft of the Requirements Document and format it according to IEEE 830-1998. We have submitted it to the client for approval or revision, after which we will turn in the signed hard copy.

\subsubsubsection{Heidi}
This week, we received feedback on our problem statement that we need to revise it and write it more from the perspective of a developer rather than the broad issue. Taylor and I met with Dr. Winters to further discuss the feedback and plan on turning in a final revised version next week. This week I created a Sphinx documentation skeleton for our project that is now in the repository. At our client meeting, we looked at a research paper that discusses statistical methods of classifying and extracting features of AVIRIS data. The paper is dense with information and at a very advanced level, so it will likely be challenging to fully grasp, but it looks fascinating at a cursory glance. We will be reading over said paper for the next few weeks and discussing it. The majority of the work we put in this week was to the requirements document. It's difficult to provide such hard specifics on a research project, but in our opinion the document turned out very nice. Plus, the LaTeX formatting is very pretty. I'm looking forward to diving into more of the "how" of this project next week and beyond.

\subsubsubsection{Xiaomei}
This week, we finished the requirement document, which is more like a to-do list than an actual description of the project. We finally start to use Overleaf as our collaborator tool for our Tex document. I feel like I did not contribute much to this document. I read the original abstract, and I am somehow still confused by all the to-dos. I watched some latex guild videos online so I can contribute more about the document in the future. After this week's meeting, I realized I still have plenty of things to learn; for one thing, I am not familiar with this kind of topic (geology-related? and land, and graphs). I cannot fully understand many of the researchers that our client sent us when I read them. Next week, I will take a look at our website and start to put staffs inside, such as the links to the research papers.
\subsubsection{2016-11-11}
\subsubsubsection{Taylor}
This week Xiaomei and I met after lecture Tuesday to talk about our git workflow and the website.

The primary task the client assigned to us this week was to review the research literature he posted to Google Drive. This provided more detailed background information on the spectroscopic imagery we will be working on and the experiences past researchers have had processing the data. These resources put us into a better position for analyzing the problems we are trying to solve.

Our client meeting was canceled this week. I was still able to meet remotely with Heidi and Xiaomei who picked up my study room reservation since I was unable to make it to Corvallis. After discussing the Tech Review with them, they met with the TA and forwarded me the feedback on the document which we will address at a future date.

I posted a draft outline of the Tech Review to Overleaf which Heidi, Xiaomei, and I are collaborating on.

\subsubsubsection{Heidi}
This week we mostly focused on reading articles that Dr. McGibbney provided to us at our last meeting and discussing the technology review. Both Taylor and Dr. McGibbney were ill on Thursday, which is when we have our weekly meeting, so we did not meet with our client this week. It has proven to be a bit of a challenge to break up requirements into parts that each member of our team is responsible for in the technology review, but so far I am optimistic. We met briefly over a video conference to discuss which parts of the project we are responsible for, and I got the API documentation and the mineral identification and classification part of COAL. I feel like our project is going in a good direction and I'm looking forward to moving past the planning stage.

\subsubsubsection{Xiaomei}
This week, we start to work on the technique review. With the help of Taylor, I discussed the method of Github pull request on Tuesday. The client meeting is cancelled this week, so we didn't talk much new about the project. But our group meet to talk about the tech review on Friday and set each section.
\subsubsection{2016-11-18}
\subsubsubsection{Taylor}
This week we collaborated on the Tech Review and submitted a fairly complete draft. However, we are working with the client to revise the content based on our research. The sections I authored were relatively vague since we have not discussed much about the later stages of our data processing pipeline.

I was unable to attend lecture Tuesday, so I appreciated Heidi keeping up to date. We met on Thursday to create a first draft of the design document which is currently an outline in bullet points. Afterwards we discussed our progress with the TA.

Friday we met with the client and discussed the writing assignments, literature, and some more technical details. I have been assigned to look into USGS spectral libraries and continue literature review. I would also like to take a closer look at the AVIRIS data we are using as a case study.

After the meeting I provided some information we could use as we consider setting up a shared development server. I also emailed some reflections on the literature I have read to date.

Unfortunately I feel I have been spending more time on the writing assignments than working on the actual project. I volunteered to write the Student Problem Statement as discussed with Kirsten, and although there is plenty of time until the end of the term I did not meet my goal of finishing it last week.

\subsubsubsection{Heidi}
This week was a bit stressful. The technology review took much longer to write than we expected and we had to email Dr. Winters for an extension. Most of this week was spent reading up on spectroscopy and feature extraction. On Thursday we got together to make an outline of the design document and plan meetings to finish the design document and the final presentation. We reserved 8 hours in total at the library, which I think will just barely give us enough time to finish our assignments with further collaboration via email. It's going to be even more stressful next week and dead week trying to get everything in order, but I think we'll pull through and likely be glad we have some documents to help us get started next term. I'm trying to stay optimistic about this, I think it's going to be a fun project!

\subsubsubsection{Xiaomei}
This week, we finished the technology review, which outlined "how" would we start our project. We list every possible technology we likely to use and discuss their advantages and disadvantages. Then, we choose the most viable one. We know this is a research project and many things are different. So, we might change the technologies during our project.

This Thursday, our group meet at the class time and start to work on the design document, which is kind of similar to the technology review. We make an outline. Then we schedule several meeting time for next week and a week after that to prepare for the final paper and the final presentation.
\subsubsection{2016-11-25}
\subsubsubsection{Taylor}
This week I met with Heidi and Xiaomei after lecture. This gave us the opportunity to discuss and plan for upcoming assignments. I shared a beamer presentation I gave earlier this year which could serve as a template for our demo.

We did not meet with the client or TA on account of the holiday. The client responded to a long email regarding literature that I wrote at the end of last week and to which I have yet to reply. I plan on getting caught up on the writing assignments as well as doing more research. Reading the background literature and becoming versed in the details of the technology will be a significant challenge, but one that I look forward to. I expect to make a serious dent in my print quota since I prefer to read and take notes on paper.

I will be meeting with the team next week as well, and I hope we can schedule more group meetings to the extent that they are productive.

\subsubsubsection{Heidi}
This week was Thanksgiving, so we did not have our regular video conference on Thursday. Instead, my teammates and I met at the library for three hours on Tuesday to discuss and work on the design document. I got one of my sections done and started working on adding to the glossary. We all touched bases on what we'll be getting done before next week so we can have ample time to get our client's feedback before the deadline and get our presentation finished by the end of next week so we don't have to work on it during finals week. Overall, this week was fairly productive despite the holiday break.

\subsubsubsection{Xiaomei}
This week, I only have two-day class, but our group still meet on Tuesday and start working on the design document. Due to the holiday, we did not meet with our client or our TA. We talked about our part in design document (the same as the tech review) and then scheduled the next week's meeting. Also, we expect to finish our final presentation before the finals week so that we need to start right after the Thanksgiving holiday.
\subsubsection{2016-12-02}
\subsubsubsection{Taylor}
This week the team met several times to coordinate our work. On Monday we met to discuss the term project and edit the design document and presentation. Tuesday night I added a number of illustrations to the presentation from our research literature and other sources. Wednesday we met to record voiceovers for the presentation but ran into trouble with sound equipment. We agreed to record our audio separately after which I will dub them into the presentation.

I completed my sections of the design document as well as the introduction, timeline, and conclusion sections. We met with the TA and discussed some of the challenges we were having with the coursework. We were advised to include in our retrospective reflections on what worked and what didn't work this term, and whether we or the instructors could have done anything to improve. We will be arranging a new time to meet with the TA next term.

The client meeting was delayed until Friday which added time pressure to the design document submission. Lewis, Heidi, and I were in attendance. Because the deadline was extended we had no trouble submitting the design document after getting client approval. We identified areas to improve the document as we revise it in future cycles. During the meeting we reflected on the writing assigned this term as well as the timeline for project implementation in the following terms. It will be a challenge to complete the project in the remaining term and a half we have to concentrate on implementation, but I believe it is doable as long as we keep a tight schedule. We also shared a copy of the presentation which will be added to the website. Finally, Lewis provided a brief demo of a notebook exercising library functions that analyze AVIRIS data.

The remaining work for the term includes finishing the fall progress report along with the presentation. The student problem statement will be included with the submission along with the client problem statement that was submitted at the start of the term.

\subsubsubsection{Heidi}
This week, we worked on getting the design document done and starting the presentation. The design document was difficult for me to write to be honest. My section on the API documentation wasn't so hard to write about, but the sections on feature extraction and classification were difficult. There was a lot of very dense reading to do and trying to put it into words while keeping the feature extraction and classification concepts separate was a challenge. We were working up until the deadline on that but it's in the repo and a hard copy is in Kelley 2098, so we got it done on time. The presentation slides were finished during two meetings we had in the library this week. We also got started on recording the voice-over and Taylor will be putting the slides and audio together when they're all done. I'm going to be putting together a template for the final written report tonight and sharing it with my group on Overleaf and we will be writing it gradually until we have our next meeting on Tuesday. We're all glad to be almost done with these assignments and making progress toward doing the bulk of the programming and image processing next term.

\subsubsubsection{Xiaomei}
This week, our team meet three times. On Monday, we meet to discuss the presentation and the final report paper. Moreover, on Wednesday, we start to put contents inside our powerpoint and recording the audio. The design document is due this week; I worked on this for the most of the time, and my section includes: identify mining, front end, and static site generator. I need to meet the five hundred word limit for each section, and that poses the most difficulties for me. We decide to finish off the final report the next week.

\subsection{Winter 2017}

\subsubsection{2017-01-13}

\subsubsubsection{Taylor}
This week I watched two MIT OpenCourseWare lectures on neural networks to prepare for implementation.

https://www.youtube.com/watch?v=uXt8qF2Zzfo\&index=12\&list=PLUl4u3cNGP63gFHB6xb-kVBiQHYe\_4hSi

https://www.youtube.com/watch?v=VrMHA3yX\_QI\&index=13\&list=PLUl4u3cNGP63gFHB6xb-kVBiQHYe\_4hSi

I posted an extensive summary and discussion to the mailing list about the theory and its applications to our project. I sketched a preliminary concept of using networks to classify our data:

    Mineral ID Neural Net
    Mining ID Neural Net

I printed out and have begun reviewing the rest of the literature in our shared Google Drive, and I have been looking at other resources as well.

Heidi, Xiaomei, and I met on Friday to discuss considerations for implementation. We had a good discussion about neural networks and the data formats we will be using to train and execute them.


\subsubsubsection{Heidi}
This week Taylor, Xiaomei, and I met in the library to discuss where to start on our development. Taylor sent us some MIT lectures on neural networks and we discussed the flow of data and what is achievable this term. We think we're in a pretty good spot and can get mineral identification, mining identification, and associating mining with environmental effects done this term. We looked at the neural network library Caffe and decided to tinker with it to see if it is something we want to use along with or instead of spectral Python. We also looked at USGS spectral library data and compared it with AVIRIS data. They aren't in the same format, so that will have to be accounted for when we start processing images. USGS data has more layers and represents numbers in 32-bit floating point numbers while AVIRIS uses integers. We think this will not end up being a very big obstacle though. Overall, it was great to touch base with the rest of my team members to get motivated for the rest of this term. Our client is on the East coast and wasn't able to make it to our meeting via video conference, but it wasn't much of an issue. We still have a few open questions, one of the biggest being how much work the neural network will do for us and how much we might have to tweak things.

\subsubsubsection{Xiaomei}
Week One of Winter Term January 13th, 2017

Our team met on Friday at the Valley Library and discussed what we expect for this term. Also, early this week, Taylor sent out the lecture from MIT that talk about neural networks; we are looking forward to going through it during the first one or two weeks. During the meeting, we also discussed the neural networks and how it is going to help with the project. We have not met with our TA nor our client this week, and it is likely that we cannot meet with our TA next week as well; fortunately, we plan just to go through what we have left and what we might do next week.

\subsubsection{2017-01-20}

\subsubsubsection{Taylor}
This week I set up a development machine with a GPU at home to hopefully speed up our neural network computations.

Once my development environment was set up, I downloaded the binary AVIRIS data files and began an in-depth analysis. I posted an extensive discussion about these "Bits and Pieces" to our mailing list. I also drafted a more detailed outline of our algorithms in pseudocode.

I became more familiar with the Spectral Python library which provides implementations for some of the algorithms I mentioned. We may need to extend the algorithms provided by the library for performance or convenience.

Xiaomei, Heidi, and I met briefly on Friday during which we reviewed the high-level objectives of our project and discussed some lower-level implementation details.

I downloaded data from the USGS Digital Spectral Library which it turns out is compatible with the AVIRIS format. We intend to use this library as training data for our neural network, however some preprocessing may remain to be implemented.

After reading Chapters 1-8 of "Python Essential Reference, Fourth Edition" I got a better understanding of some unfamiliar details of the Python interface. I believe that in particular we will be able to leverage generators for efficient iteration, the NumPy library for handling numerical datatypes, and the package interface to structure the modules in our codebase.

After we do a bit of exploratory programming, we will have collected most of the loose threads we need to weave together to implement mineral classification. We are keeping in mind the subsequent mining, environment, and temporal stages, but these will not receive our full attention until later.

\subsubsubsection{Heidi}
This week was mostly spent further clarifying and exploring the Spectral Python library as well as the USGS data. We learned that some of the USGS data for minerals are separated into different classifications, which is a bit strange but not something we believe we can't work with. Further looking into the Spectral library reveals that there are in fact implementations of some of the algorithms that Taylor mentions in his pseudocode, however we're not sure if they are 100\% suitable for us the way they are or if we will need to tweak them yet. This week wasn't super eventful, but I'm hoping that we can get some code committed in the next couple of weeks. Regrettably, we've all been busy with other things so meeting is hard, but we have still been managing to meet on Fridays.

\subsubsubsection{Xiaomei}
January 20th, 2017. Week 2.

No client or TA meeting this week.

Mainly, our group got together this week about an hour to discuss the big pictures of our project. Taylor had some fabulous ideas about how we deal with identifying the mining. We looked some process and now have a pretty nice idea about how this process would go in the future. Not many details were done by my part this week, but I understand better from the meeting we hold this Friday. I expect more next week since we are going to have our first TA meeting and our second class.


\subsubsection{2017-01-27}
\subsubsubsection{Taylor}
This week I studied the Spectral Python library in more detail and developed in a feature branch detailed skeleton code and sketches of our algorithms. The team met on Monday to touch bases with the TA and to review the code, and on Friday to meet with the client to discuss the state of the project.

The mineral classification skeleton I posted breaks down the mineral classification stage into several reusable components. I was able to identify library methods to do most of the things we are interested in for processing the files and training the classifiers. A number of questions remain to be answered and filled in, such as the parameters to the neural network or variables to index the multidimensional arrays. Heidi will be taking charge of the remaining development of this module and I will assist in a supporting role. The team will probably have to coordinate its efforts to train the network for classification.

In order to represent our algorithms visually, I posted several sketches for documentation:

    Mineral Training
    Mineral Classification
    Mining Classification
    Mining Correlation
    Temporal Correlation

In addition to novel data formats, one of the challenging things about this project is working with large data sets represented as multidimensional arrays. Spectral Python is implemented on top of NumPy, so images implement both interfaces. This allows us to think in terms of images or in terms of arrays, whichever is easier.

Since we seem to have a handle on the image processing and classification, I will be turning my attention to GIS. The Mining Classification and Mining Correlation stages rely on projecting imagery onto geographic information files in order to derive correlations. I hope to post another detailed skeleton using these which my teammates and I can fill in as we go along.


\subsubsubsection{Heidi}
This week we had a video conference with our client for the first time this term which was nice. We were able to go over the high level details of our project and confirm that the pace we were going at wasn't too lacking. This week, I'll be adding a page to the wiki on mineral classification. I'm planning on doing this Monday night. Week 4 is going to be a very busy week for me as I have a midterm and a MECOP event to attend, but I'll be attempting to try to get the code skeleton that was just pushed to the COAL master branch to run and do something. Further discussion about Spectral Python this week confirms that it has most of what we're looking for, so I won't be venturing much further from that for now. We have been working under the assumption that we will be using USGS data for training our neural network classifier, which is still correct, but we will also be adding another data set. Our client is getting his hands on this data and will be updating us when we can start using it. So far, everything is in order.

\subsubsubsection{Xiaomei}
Friday, January 27th, 2017. Week 3.

Client Meeting: We have our first client meeting this week. This had been very helpful because we are going to start the actual coding part very soon and there are something we would like to clarify. Taylor explained the basic skeleton of the identify mining and the training process. I need to keep the website up to date and start the conference paper with outlines.

TA Meeting: This is the first TA meeting this term. We basically describe where we are and what we have done.

\subsubsection{2017-02-03}
\subsubsubsection{Taylor}
Friday, February 3, 2017. Week 4

Starting from next week, I am going to take up the role of leading the group: to ensure all todos and watch the deadlines; to ensure the agenda is correct and in a timely fashion, and keep updating the project dashboard.

I have worked on the skeleton of the conference paper sometimes this week, and Lewis mentioned today that he would start a project on the Overleaf for the same purpose. Also, between now and next Friday, I need to update our website to fit the requirements as much as possible. We need to be aware that the midterm report is on its way as well.

During the regular meetings on Friday, we made sure what each of us is going to do for the next week. As I mentioned above, I mainly focus on the website update, and I also need to take a look at the mineral identification part. Moreover, I need to understand what Taylor and Heidi are doing due to the purpose that I need to keep track of things and stay up to date.


\subsubsubsection{Heidi}
This week was a busy week for all of us. I created a brief draft of a wiki page on mineral classification and I plan on adding to it this week and beyond. We had another meeting with our client and talked about what we'll be working on before our next meeting. I'll be working on the code for mineral classification. There is a branch in my forked repository called "mineral-id-dev" that I will be working off of. We also discussed the midterm report we have due in two weeks. We set aside some time on Monday for discussing document revisions as well as what we will be doing for the demo. Our team leader for the next two weeks is Xiaomei, who will be responsible for keeping the project board up to date and creating the agenda for our next meeting. The next leader will be Taylor and then it will go to me for the last two weeks of the term.

\subsubsubsection{Xiaomei}
Friday, February 3, 2017. Week 4

Starting from next week, I am going to take up the role of leading the group: to ensure all todos and watch the deadlines; to ensure the agenda is correct and in a timely fashion, and keep updating the project dashboard.

I have worked on the skeleton of the conference paper sometimes this week, and Lewis mentioned today that he would start a project on the Overleaf for the same purpose. Also, between now and next Friday, I need to update our website to fit the requirements as much as possible. We need to be aware that the midterm report is on its way as well.

During the regular meetings on Friday, we made sure what each of us is going to do for the next week. As I mentioned above, I mainly focus on the website update, and I also need to take a look at the mineral identification part. Moreover, I need to understand what Taylor and Heidi are doing due to the purpose that I need to keep track of things and stay up to date.


\subsubsection{2017-02-10}

\subsubsubsection{Taylor}
This week I made progress implementing the GIS routines with the Geospatial Data Abstraction Layer (GDAL) library. I posted a more detailed sketch illustrating the algorithm. This will allow us to use geographic data to train the mining classifier with mine permit boundaries and to implement the environmental correlator with pollution data. I still need to find suitable files to test and debug these functions. My work-in-progress contributions to mining identification and environmental correlation have been posted to GitHub. One of my goals this coming week is to find the necessary files and finish implementing these functions.

I also have collaborated with Heidi to pursue mineral classification and with Xiaomei to implement the website.

The team will be working together in person and online to revise the documents and compile the midterm report. We have also begun thinking about our research paper and potential publications to submit it to.

Lewis, Xiaomei, and I met Friday to review our development. An additional spectral library (ASTER) has been shared with the team for use as an additional data source for training the mineral classifier. We spent some time talking about what kinds of environmental data to use as a case study, and we are leaning towards analyzing impacts on water resources. Since our concern is to implement the library but not conduct the actual data science, we agreed that a simple "overlay" analysis would be sufficient to demonstrate mining/environment correlation. For this, we are considering finding a way to use NASA's Global Reservoir and Dam (GRanD) database.


\subsubsubsection{Heidi}
This week I started to work on the code for the mineral classification. I made a few revisions to the code and put in some Sphinx-compatible doc strings but nothing is functional yet. I was ill and unable to make our team's weekly meeting but emailed some points I wanted to bring up. This coming week, we will mostly be working on the midterm report. I set up and shared the OneNote notebook with the team and most of the document revisions will also be on me. We have a busy week and will probably be working down to the last minute but that was true most of last term and we made it through that so I feel like we'll get everything done.

\subsubsubsection{Xiaomei}
During the regular meeting of this week, we talk more about the website and the GIS work. This week, I was responsible for developing the website by adding the features of: fix various links; change and style the background pictures; add videos to the front page; add downloadable pages to links to all our paperwork for the class and so on.

There is a deadline for the class next week, which we need to revise our past documents and write a midterm report by next Friday. We plan to meet during the weekdays of next week to address those assignments. Apart from that, I need to add the Blog part to our website, which needs me to work with Jykell and might need some more time.

*fix date

\subsubsection{2017-02-17}
\subsubsubsection{Taylor}
This week we had to concentrate on our winter progress report and presentation. Heidi and I met Wednesday, and the whole team met Friday, to coordinate development. The contributions for which I was responsible included my sections of the progress report, slides, recordings, and revisions. In addition, I built the documents, committed and merged the changes, created the video, uploaded and submitted the assignment, and added the documents to the deliverables folder of our team's Google Drive.

The writing displaced our development time this week. I am concerned about the timeline so I intend to redouble my efforts to contribute to my components as well as those of my teammates as necessary. In the coming week I hope to collaborate with Heidi on a concrete implementation of mineral classification, and to continue to develop my methods for processing geographic information. The repository has been renamed, and future revisions will use a classical object-oriented API.

\subsubsubsection{Heidi}
This week we mostly worked on getting our midterm obligations finished. I ended up completely changing two out of three of my sections in the design document and the technology review. It feels good to have them revised, as they were pretty inaccurate before as we did not have a handle on how we were going to implement COAL until after those documents were due. We also put together a new slideshow and recorded presentation as well as a written midterm report. I was ill the entire week, so finding the energy to finish all of these things was hard and I was pretty exhausted by Friday night, but it is nice to have them out of the way so we can continue to write our code. Taylor and I are meeting for a few hours on Monday in order to work on the mineral classification code. We are also going to implement it as methods of a class as per our client's request.

\subsubsubsection{Xiaomei}
This week we mainly are working on the midterm paper and presentation. I revised some content of our design document and technology review from the last term; since my part of mining identification have not got started yet, there are not many to change, but I do expect to rewrite it in the future.

I record plenty of changes about our website, including the new features I would be implemented next week. I think I am going to finish all of the core content of the website next week and get into the mining identification until new requests emerge.

\subsubsection{2017-02-24}
\subsubsubsection{Taylor}
The team met with the TA on Monday to review our progress and code. Afterwards, Heidi, Xiaomei, and I went to the library to work on coding. Heidi and I pair-programmed on mineral classification, and Xiaomei worked on the website.

I had a conversation with the TA on Tuesday describing the contributions each team member has made throughout the project.

This week I took responsibility for the mining classification module which was assigned to Xiaomei and submitted a work-in-progress pull request containing a skeleton of the algorithm. Because mining classification and environmental correlation require interacting with geographical information, I focused on acquiring and processing GIS data.

I have been documenting my findings on the mailing list. I was able to locate the region scanned by the flightline by converting the coordinate system in the header file to latitude and longitude. This allowed me to identify it as the San Juan Mine in northwest New Mexico. I then found photos of the mine and allegations of water contamination by the Sierra Club. I then acquired and shared topographic quadrangles from the USGS and mining permit boundaries from the New Mexico Energy, Minerals and Natural Resources Department. Since the permit boundaries appear to be larger than the excavation, we may need to select the training region by hand. I was able to load the maps and the flightline into a GIS application (I experimented with GRASS GIS, QGIS, and ArcMap), however the image rotation is not detected so I have been searching for a way to rotate it with GDAL. Once the geographic information is consistent, I can finish implementing projections and transformations.

Heidi, Xiaomei, and I met on Friday to review our progress. We agreed to prioritize the website and mineral classification, and I will continue developing the mining classification and environmental correlation.

\subsubsubsection{Heidi}
This week I spent my time working on the code for mineral.py. I managed to get the skeleton filled out and at least syntactically correct on Thursday. It wasn't too difficult but I did struggle with finding the correct parameters for the structure of the neural network. I went with a structure used on AVIRIS data in another research paper, but I may change it around when I'm testing. Next week I'll be working on tests. This might end up being quite the undertaking considering the complexity of some of Spectral's objects and methods. This will also be when the team will find out how much consideration we have to give to timing. The libraries and AVIRIS images are huge files, so it could be a concern.

\subsubsubsection{Xiaomei}
Week 7; 2/24/2017

This week I mainly worked on the website as well. Our client did not attend our regular meeting this week, so we just sit down and talk about what we are going to do next week.

The website might need a big overhaul because I want to incorporate the Jekyll blogs; this would result in the entire change of structures of our website.

\subsubsection{2017-03-03}
\subsubsubsection{Taylor}
We met with the TA on Monday but the team meeting we had scheduled afterwards had to be canceled because other group members were not available. Heidi and I collaborated through GitHub and the mailing list throughout the week as she implements tests and functionality for mineral identification. Our goal is to have preliminary mineral classified data by the end of next week so I can start on mining identification in earnest.

This week I was able to solve the GIS rotation issue after requesting help from the GIS Stack Exchange. It turned out to be a bug in GDAL. I fixed the bug and posted my patch to the GDAL mailing list. The maintainer made a few modifications and improvements and committed the fix to the GDAL source. The fix is scheduled to appear in version 2.2.0.

Once I was able to rotate our imagery, I generated a smaller RGB data set to use for development purposes. I created a vector shapefile corresponding to the boundaries of the excavation which I can hopefully use to generate training data. I found that the GRanD database we hoped to use for water resource mapping was not applicable to our data set.

I found additional AVIRIS-NG flightlines over the San Juan mine in New Mexico as well as over a coal mine in Colorado. We are waiting for JPL to give us access to these data products which we hope to use for training and analysis. Although the files are huge, it turns out we don't have a lot of mining imagery to work with.

We met with the client on Friday to review our progress. We now have access to an AWS machine to do our data processing. This is an ordinary cloud server rather than the supercomputer we had considered, but we hope it will suffice for our purposes. Since the website is still not usable, I have taken responsibility for its development from Xiaomei and have had to take time away from my other classes to implement her module.

\subsubsubsection{Heidi}
This week I worked on further tests and the mineral classification code. Using the ASTER library proved to be a bit of a challenge because Spectral Python does not have a default method for turning ASTER into a training data set. It must first be turned into a .db file and then manually manipulated so it can be turned into ENVI format. I'm not entirely sure how I'm going to get it into ENVI format, but I'm sure I will get it figured out. I may submit an issue to Spectral Python's GitHub page for a method of turning ASTER into a training data set. I plan on having some sort of classifier done by the end of the week so Taylor can further work on his parts.

\subsubsubsection{Xiaomei}
Week 8; 3/3/2017

This week, I finished most of the website and have time to clean up some issues that were assigned to me before on the GitHub, including clean up the directory and replace the name of our project. During our regular meeting, we stated what we have finished this week.

Of the coming week, I would start to write/revise the API documentation and there might be more requirements for revising the website; I would be looking into that as well.

\subsubsection{2017-03-10}
\subsubsubsection{Taylor}
After taking responsibility for the website I spent the weekend implementing the site with Bootstrap and Jekyll. It now incorporates the information requested by the client, although I will still need to add blog posts and a wiki page on editing it.

We met with the TA on Monday to discuss our progress. We are still behind schedule because of delays implementing mineral classification and other modules. The goal was to have preliminary data by Friday so I could begin mining identification.

I found water quality data which we can use for environmental correlation. The data points are sparse rather than continuous, so we have had to revise our assumptions.

I merged in an outline of GIS processing routines, mining identification, and environmental correlation, however these will need to be heavily revised once mineral data is available. I also merged in code to add a Python linter to our build process and to clean up some incorrect changes made to the docs directory.

We met the client on Friday. He is implementing a science data system to automate data processing and make it available on the Web.

Mineral identification wasn't finished by Friday, but Heidi came up with some helpful findings after contacting the maintainer of the Spectral Python library, and I have taken on implementing the rest of the mineral classification algorithm. I am currently processing the mineral data on our cloud server.

\subsubsubsection{Heidi}
This week I worked more on the mineral classification code. We decided to switch to another algorithm and leave neural networks behind after discussing our project with someone over on Spectral Python's GitHub page after I created an issue regarding creating a training set out of a spectral library. We are going forward with a spectral angles algorithm that calculates the spectral angles between the pixels and each entry in the spectral library and uses argmin to obtain a class. Timing proved to be an issue this week. When trying to normalize the data using a principal components algorithm in Spectral Python, it took about half an hour. Normalizing the library only took about a second or less, but the 6.3GB image took a very long time.

\subsubsubsection{Xiaomei}
Since Taylor finished our website during the last weekend, this week I mainly learned about API documentation and how to deal with it because I am basically new to this topic.

\subsubsection{2017-03-17}
\subsubsubsection{Taylor}
This week I implemented the first working version of mineral classification and ran the classifier over several images. I found evidence of coal mining waste across the surface of the San Juan Mine, but the other images I tested gave ambiguous results.

Now that mineral classified data is finally available, I can implement mining identification and environmental correlation. These will probably end up being less complicated than first imagined because of limitations of the data: The mineral data is limited by the number of samples, and the environmental data is sparse rather than continuous.

I met with Heidi and Xiaomei on Monday and on Friday to work on the presentation and the poster. I finished the poster and submitted it Friday. We did not meet with the TA or the client this week.

\subsubsubsection{Heidi}
This week Taylor was able to help out with getting the mineral classification code to a functional level and was able to generate an image. Again, timing proved to be an issue with this because it took between 6 and 11 hours to generate one classified image. I'm really thankful Taylor could help me out as I've been swamped with assignments this week and had to prioritize, unfortunately. I mainly tried to get recordings for our presentation finished and worked on getting the poster done with Taylor and Xiaomei on 3/17. Our poster looks really cool and presents our project well with a lot of visual interest in my opinion. We also tried to plan our meetings for next term but I am unfortunately the only member of the team who has all of their classes registered. I plan on working on tests and documentation for the classification over Spring break.

\subsubsubsection{Xiaomei}
This last week I mainly focus on the prensations as well as the final paper. I had to work on the API documentations and finished them by the final's week. I have been talking with our client Lewis about what I'm going to do for the next term and possibly during the spring break: I would start working on the conference paper.

I want to take more look at what Heidi and Taylor were working on during the spring break, this would help me on the conference paper, and I might also do some contribution to it.


\subsection{Spring 2017}

\subsubsection{2017-04-07}
\subsubsubsection{Taylor}
This week I began processing some of the new data that was acquired during Spring Break. The team attended lecture and met the TA on Wednesday. We met the client on Friday. We have set milestones for our remaining development leading up to Expo.

\subsubsubsection{Heidi}
This week we met with our client for the first time since the end of Spring break. Lewis uploaded a bunch of new AVIRIS images to our AWS machine and Taylor is running them through a script to classify all of them. It was decided at the meeting that Xiaomei will start working on the research paper and I will write tests during week 2 for classification to verify that the classifications are valid and the metadata is correct.

\subsubsubsection{Xiaomei}
The first week does not have much to say due to this is the first week after the spring break.

First, we have decided the time for the TA meeting and the weekly regular group meeting. The TA meeting is mostly about the information of expo.

Second, my main task for this final term is the research paper. I would be expected to get start on it next week.

\subsubsection{2017-04-14}
\subsubsubsection{Taylor}
This week I continued processing our new AVIRIS data and started to analyze the results.

I met with the TA and Xiaomei on Wednesday to discuss our Expo requirements and upcoming assignments.

I helped write an XSEDE allocation request to obtain supercomputing resources to speed up our image processing. I found that my contribution to GDAL will be released in version 2.2.0 on 2017-05-01. I implemented the mining classifier and used QGIS to analyze its classified input and output. This will be part of our 0.2 release. I also gave feedback on our research paper effort and helped implement mineral classification tests.

Heidi, Xiaomei, and I met on Friday to discuss development and upcoming class requirements.

\subsubsubsection{Heidi}
This week I worked on mineral classification tests. I had a bit of trouble extracting sub-images but I figured it out. Taylor finished a working draft of mining identification and Xiaomei is set to work on the research paper. I will be finishing up tests and then working on ASTER implementation. We are set for a 0.2 release.

\subsubsubsection{Xiaomei}
Start the research paper. It seems that we have plenty of time to finish the paper. Right now, we decided the paper would be submitted to the Elsevier's Computers and Geosciences.

author information pack.

I'm mostly working on the Abstract/Introduction part of the paper this week, I personally would like to post them next Monday, for the reason so that I had time to make some final changes.

We might need a time frame to plan the progress we're desired. This is probably one of the things that would be settled next week. Other than that, I would wait for more directions.

Taylor has provided more information about what we are right now regarding of the whole project, which would be very helpful.

The TA meeting this week has discussed and decided for the requirements of our Expo during May.

\subsubsection{2017-04-21}
\subsubsubsection{Taylor}
This week I implemented tests for mining classification, uploaded the winter report files to our repository and Google Drive, implemented a method to convert AVIRIS images into a suitable RGB format for GIS analysis, added a method to remove superfluous metadata from processed imagery, resolved attribution issues in our materials, added updated visualizations to the website, and added instructions for website development into the website readme. I also continued to process and analyze incoming classified imagery.

On Wednesday we met with the TA to discuss upcoming deadlines and project goals. On Friday Heidi, Xiaomei, and I met to coordinate our work and prepare for the unexpected code freeze.

In the remaining time I will be working to resolve all the remaining issues so that the initial version of our codebase is in the best possible shape. I will also have to worry about the class work and the research paper the team is writing.

\subsubsubsection{Heidi}
This week I finished up tests for mineral classification. It wasn't too hard, just sort of tedious trying to extract sub-images from the full AVIRIS images that would make good test cases, i.e. one with all "No data" pixels, one with both "No data" and other classes, and one with no "No data" pixels, making sure at least one is an AVIRIS-NG image. I will move onto ASTER and working on the API documentation. Unfortunately, our deadline has been moved two weeks earlier than was stated for the last 6 months, so we will be in a huge rush next week.

\subsubsubsection{Xiaomei}
This week, I continued work on the research paper. I created a basic structure on the overleaf (later transferred to google doc). I wrote abstract and introduction mostly based on what we already have on this kind of topic.

The structure is expected to change as we write more about the paper. The major problem right now that I am facing is that there is no clear thesis for our paper at the current stage. I could continue to add more content and revised altogether in a later date.

One thought I had for the research paper is to follow our workflow, to write as the steps we took to proceed our current progress. This is how I lay out our structure, also, the explanation about the background information would also be important.

\subsubsection{2017-04-28}
\subsubsubsection{Taylor}
As a result of delays last term in implementing the prerequisite modules and the unexpected code freeze which pushed back the completion date by two weeks from what has been documented on the course website, I have been significantly strained to finish processing the data and deriving the environmental correlation (not to mention meeting the requirements of my other classes). The time pressure has also made it more difficult to coordinate development and availability with our client. In an effort to hasten turnaround time, I created a development branch for more rapid iteration.

After Heidi updated the mineral classification tests, I added subset and threshold functionality to the mineral classifier to support an alternate mine classification workflow. I finished processing the high-resolution AVIRIS imagery which provided the best quality mineral classification data of the San Juan Mine. I took initial steps to fix the API documentation which wasn't building, after which Heidi improved the theme and functionality. Heidi implemented experimental ASTER support for which I wrote unit tests. In order to improve performance of mineral classification on the large-memory servers we hope to support, I added an option to load entire images into memory. As part of environmental correlation I compiled and shared the latest version of GDAL and generated RGB imagery which added a new perspective for interpreting the remote-sensing data. I updated the visualizations in the website carousel to include our latest imagery. I also updated the poster multiple times after we learned at our TA meeting that that deadline too was being pushed forward from Monday to Friday.

Due to the time pressure, we canceled our meeting on Friday and rescheduled it for Monday. In the remaining time before the code freeze I hope to finish the environmental correlation module and add multiple blog posts and documentation to the website. After principal development has completed, I will be turning my attention to helping author the research paper as well as the various additional homework projects that have just been announced.

\subsubsubsection{Heidi}
This was a stressful week for us. There was a code freeze announced last week despite us not being aware of this during the rest of the year, meaning our workflow was progressing on the assumption we would have a due date of May 15th for code changes. I pushed a working ASTER implementation, though there are some improvements needed, i.e. removal of classes with no-data spectra. I also made changes to the Sphinx documentation on readthedocs, changing the theme and adding information on dependencies as well as changing page titles and other small fixes. We are focusing on getting our poster submitted and getting Taylor's components finished before our Monday code freeze.

\subsubsubsection{Xiaomei}
I continue working on the research paper. During the first part of the week, I try to add some content every day. Most of the content I've added since now is based on the initial structure I built before. So the most part I worked on this week is a general introduction and some background information.

Lewis has provided a more detailed and more official structure later this week, and we have an example paper to follow. This makes the work easier in regarding of the direction of the paper.

Next week, if no exception, I would still work on the paper like before. More research is needed as well.

\subsubsection{2017-05-05}
\subsubsubsection{Taylor}
I managed to get the environmental correlation module done before the deadline. The solution was inelegant but it was successfully able to locate mining classified pixels within a given distance of known streams and water bodies. To implement this I had to build GDAL and QGIS from source, and I shared steps and executable code with the team. In addition to implementing the environmental correlation module, before the code freeze I revised the website to include detailed documentation and blog posts containing our poster and my write-up about the GDAL bug I fixed. The latest version of the code was merged in and tagged as 0.5.0. The team met on Monday and on Friday, but our client was unable to attend either day. As far as we know, everything is on track for Expo, and we'll be doing a live critique in the coming week.

\subsubsubsection{Heidi}
We managed to get environmental correlation and some last bits and pieces together by the code freeze deadline. The only thing we have left is to figure out the problem with the pip installation. We also submitted our final poster draft after getting it approved by our professors. We all mostly worked on a writing assignment given in class, and since there is a code freeze, nothing much has been done regarding this project this week.

\subsubsubsection{Xiaomei}
Week 5 For me, there aren't much for me this week. I continue to do research and put things together for the research paper. Due to the other work I need to do, I haven't spent much time compare to the last two weeks.

However, I anticipated doing more work for the research paper next week. We also have the live critique with another team in the class next Wednesday. I might start to prepare for the Expo as well.

\subsubsection{2017-05-12}
\subsubsubsection{Taylor}
This week we did a live critique of our Expo presentation with Dr. Winters. The team met on Friday to discuss the upcoming midterm report. Progress on the research paper is ongoing, but slow.

\subsubsubsection{Heidi}
This week we met to work on the midterm report. We haven't spoken to our client in a while, so the project has been quiet. Expo is next week and our project is being highlighted, which is super cool. We also met with another group to get feedback on our project pitch at Expo to make sure it made sense to the average person and was vaguely interesting. We got some good feedback and I think we're on track for Expo.

\subsubsubsection{Xiaomei}
During the week six, I mainly focus on the course work: the midterm report and the presentation.

My part for the midterm report is the Problem\&Solution as well as the write-up apart. For presentation, I'm responsible for the Expo pitch and the review of our midterm report part.

Next week, most of my time for this class is contributed to the preparation of the Expo on Friday. I expect to be there very early to receive the poster and set up the basics. The schedule is online, so it won't be so hard.

\subsubsection{2017-05-19}
\subsubsubsection{Taylor}
This final weekly update after Expo summarizes my experience with the project and considers what I might have liked to have known at the beginning.

We gave our Expo presentation on Friday 2017-05-19 which was well-received. Our project didn't draw the largest number of viewers, but those it did were very interested in our work. The team spoke at length with OSU graduate students, industry representatives, and students and community members of all ages. For our collaborative efforts in the Free and Open Source Software and scientific communities, as well as for the potential applications of our remote sensing efforts, we were recognized with an award for collaboration by representatives of the CH2M HILL company. Although we were able to display interesting visualizations with bright colors, the relative intangibility of our library was a common source of questions. It was necessary to summarize the "big picture" of our work — using remote sensing to study environmental impacts of coal mining on water resources — to get our point across. We were surprised by just how technical some of our contacts were, some of whom were familiar with the GDAL library and geospatial tools.

In general, this project was extremely challenging due to the heavy load of coursework, the extremely high learning curve, and the long hours needed to research and develop the library. I personally prioritized this project over many of my other classes this year, so although I was finally able to satisfy the high standard I held for this project I have had trouble staying on top of my other courses and managing my schedule. I recognized this as a remarkable professional opportunity and put in at least as much time as the part-time development job I previously held. Ultimately I think it was a great success, and our client has stated we went "Far, far, far beyond expectations." We still need to tie up a few loose ends and push through to draft the research paper, so we can't quite yet call it quits.

At the beginning of the course I did not realize how much coursework we would have to do outside of research and development, and in general I didn't expect this to be as huge a time commitment as it turned out to be. Come major development in winter term, in hindsight I would have liked to stay more on top of the scheduling which ended up being a large source of stress when we were behind schedule. The preliminary and skeleton code I contributed and the ongoing background research was a successful strategy, however too much time was spent waiting for unrealistic development responsibilities to be met. A criticism I have of the course in general was the rigidity with which our individual requirements were evaluated, since it was more realistic to change responsibilities according to need. This, along with inadequate communication, led to tension within the team at times. I was also frustrated by what I perceived to be a disproportionate burden on implementation, but I was ultimately able to take pride in my contributions. In general our team has maintained its cohesiveness and civility despite mistakes on all of our parts, including mine. I wish the project had not been so stressful, and I was frustrated by incomplete communication of the course organization, but these costs were repayed by meeting the very high standard we set.

It was a surprise and an honor to be tasked with this project, as I was initially quite skeptical and apprehensive about the way capstone projects were distributed. My personal interests in programming language design did not come into play in this project, but the potential for Free and Open Source Software contributions has advanced my career goals and the geospatial analysis gave me practical insight into a fascinating and prestigious field. I was honored to work with my clients at JPL and my teammates and instructors at OSU.

\subsubsubsection{Heidi}
This week, we turned in our midterm report with minutes to spare. Video processing is hard. But the main highlight of the week is that we finally got to present our project at the Engineering Expo. We actually had many people come to ask about our project who worked in or were interested in processing imagery and environmental data similar to pycoal. I would say we talked to more technical people than not. We all arrived on time and got set up quickly, so there were no bumps. We also won an award that was presented at the end of Expo. We were awarded for our collaborations outside of our group, e.g. reaching out to the Spectral Python maintainer, Taylor's work with the GDAL bug fix, contacting our client's JPL colleagues to get data, etc. It was very cool to be recognized and the CH2M engineers who were involved seemed very interested in our project.

This project as a whole has been quite the journey. None of has experience with hyperspectral imagery or GIS processing before we started, so we really had to start from scratch both in knowledge and code since our project was also built from the ground up. We learned a huge amount in the last three terms. It was also a great lesson in remote collaboration.

It would have been cool to have a description of all the documents we need to write provided at the beginning as it could be a good way to steer our early conversations.

\subsubsubsection{Xiaomei}
If you were to redo the project from Fall term, what would you tell yourself?

Our project seems very much like a research project at the beginning. But, if I have a chance to redo our project, I would start to get familiar with Python again, because I haven't used Python for a very long time. Second, I would do more research about the remote sensing and its application. I feel like our project could have a much larger use, so how to correlates its implementation with the current goal (such as environmental correlation) is a good thing to think about at the start phase of the project.

What's the biggest skill you've learned?

Although I didn't participate much about the code part, I still learn much from the discussion and through study the final version of the code, I learn how the algorithm is implemented. Besides, I was responsible for the project website at the first half of the project, in order to implement the function we need, I put most of my time on it; hence I learned most of the skills of building a website.

What skills do you see yourself using in the future?

As I just said, I think the website skill would help me a lot. Due to the reason that I probably won't be work in the similar field as this project in the future, the more general skill would be the best help for me.

What did you like about the project, and what did you not?

Compare to some of the other projects, what I like about this project is that in this project we actual would build something in the end. Although at the beginning there were plenty of background information that I need to get familiar with, I think the final product we offer is good.

On the other hand, from my perspective, what I dislike is that I was not very interested in the mining and remote sensing filed.

What did you learn from your teammates?

Their ability to schedule everything is perfect. I learned a lot from how they work and how the project was scheduled. I think my teammates have a very good professional skill set as well. How they do their research and where to find relevant information is a good skill that I've learned as well.

If you were the client for this project, would you be satisfied with the work done?

I think we stick to our schedule and all the things are done before the code freeze and the Expo.

If your project were to be continued next year, what do you think needs to be working on?

First, we can expand our focus on the environmental correlation part. This is the part of why we do our project at the first place: we want to identify the mining and see how it impacted the environment. This can affect plenty of fields, such as policy making if the application is appropriate and efficient.

In conclusion, I think we did a good job at the Expo. But we still have some work to finish. I expect to put a good ending on this project.

\section{Final Poster}
The following page contains our final poster that we presented at the 2017 Engineering Expo. 

% do something else here?

\includepdf{expo-poster.pdf}

%% Your final poster, scaled and color-printed on a single 8.5"x11" paper. If you don't have access to a color printer, I will print it for you. Let me know by the Thursday of dead week if you need it printed. 

\section{Documentation}

\subsection{Implementation}

The Python COAL library is implemented with a series of classes that provide an image processing pipeline. Hyperspectral imagery and a spectral library are used to classify minerals, mining proxy classes in a mineral classified image are used to identify mines, and mine classifications are combined with geographic data to derive environmental correlations. Imaging spectrometer data is represented as an $M\times{}N\times{}B$ matrix of numeric values corresponding to the reflectance value observed for each spectral band measured by the sensor at a given location. Mineral classification uses the spectral angle mapper algorithm to compare the value of each band in each pixel to known values in the spectral library to predict the most likely mineral on the land surface. Mining identification uses spectral library classes known to be associated with mining to filter relevant pixels from the mineral classified image. Environmental correlation generates a rasterized proximity map from the geographic vector data and compares it to the mining classified image to detect mining impacts such as acid mine drainage within a given distance from geographic features such as stream flow lines. The COAL science data system is used to batch process images over time.

\subsection{Requirements}
COAL has been tested on x86\_64 GNU/Linux and is expected to work without modification on any Unix-like system. COAL will perform best on systems with large amounts of RAM, hard disk space, and CPU resources.

\subsection{Dependencies}
COAL supports Python versions 2.6+ and 3.3+. COAL depends on several other software packages:
\begin{itemize}

\item NumPy is required for numerical processing and can be installed via the Python Package Index (PyPI).

\item Spectral Python is required for spectral image processing and can be installed via PyPI.

\item GDAL version 2.2.0 or later is required for GIS processing and can be built from the source on GitHub.

\item QGIS is recommended for viewing COAL data products and can be installed from sources found on the QGIS website or via a package manager.

\end{itemize}


\subsection{Installation}
COAL can be installed from PyPI by running \verb!pip install pycoal! as root. It can also be installed from source from our GitHub page or from Conda by running \verb!conda install -c conda-forge pycoal=0.5.2!.

\subsection{Usage}
\subsubsection{Data}
COAL uses several kinds of input data to classify minerals, identify mines, and correlate environmental impacts:

\begin{itemize}
\item A spectral library in ENVI format is required to classify minerals. The USGS Digital Spectral Library 06 used in our case study is recommended and can be accessed via FTP from the United States Geological Survey (USGS). Experimental support for the ASTER spectral library is also available.

\item COAL was designed to analyze AVIRIS images in ENVI format. Imaging spectrometer data from JPL can be downloaded or requested via the AVIRIS and AVIRIS-NG websites.

\item A geographic dataset is used to derive environmental correlations. The National Hydrography Dataset used in our case study is available from USGS as part of The National Map.
\end{itemize}

\subsubsection{Examples}

\subsubsubsection{Mineral Classification}
The Mineral Classification API provides methods for generating visible-light and mineral classified images. Mineral classification can take hours to days depending on the size of the spectral library and the available computing resources, so running a script in the background is recommended. The following script provides an example of mineral classification:

\begin{lstlisting}
#!/usr/bin/env python
import pycoal

# path to spectral library
libraryFilename = "s06av95a_envi.hdr"

# path to orthocorrected, scaled-reflectance image
inputFilename = "ang20150420t182050_corr_v1e_img.hdr"

# path to save RGB image
rgbFilename = "ang20150420t182050_corr_v1e_img_rgb.hdr"

# path to save mineral classified image
classifiedFilename = "ang20150420t182050_corr_v1e_img_class.hdr"

# create a new mineral classification instance
mineralClassification = pycoal.mineral.MineralClassification(libraryFilename)

# generate a georeferenced visible-light image
mineralClassification.toRGB(inputFilename, rgbFilename)

# generate a mineral classified image
mineralClassification.classifyImage(inputFilename, classifiedFilename)
\end{lstlisting}

\subsubsubsection{Mining Identification}
The Mining Identification API filters mineral classified images to identify specific classes of interest, by default proxies for coal mining in the USGS Digital Spectral Library 06. The following script provides an example of mining identification:

\begin{lstlisting}
#!/usr/bin/env python
import pycoal

# path to mineral classified image
mineralFilename = "ang20150420t182050_corr_v1e_img_class.hdr"

# path to save mining classified image
miningFilename = "ang20150420t182050_corr_v1e_img_class_mining.hdr"

# create a new mining classification instance
miningClassification = pycoal.mining.MiningClassification()

# generate a mining classified image
miningClassification.classifyImage(mineralFilename, miningFilename)
\end{lstlisting}

\subsubsubsection{Environmental Correlation}
The Environmental Correlation API finds pixels in a mining classified image that are within a certain number of meters from features in a vector layer such as flow lines in the National Hydrography Dataset (NHD). The following script provides an example of environmental correlation:

\begin{lstlisting}
#!/usr/bin/env python
import pycoal

# path to mining classified image
miningFilename = "ang20150420t182050_corr_v1e_img_class_mining.hdr"

# path to hydrography data
vectorFilename = "NHDNM/Shape/NHDFlowline.shp"

# path to save environmental correlation image
correlationFilename = "ang20150420t182050_corr_v1e_img_class_mining_NHDFlowline_correlation.hdr"

# create a new environmental correlation instance
environmentalCorrelation = pycoal.environment.EnvironmentalCorrelation()

# generate an environmental correlation image of mining pixels within 10 meters of a stream
environmentalCorrelation.intersectProximity(miningFilename, vectorFilename, 10.0, correlationFilename)
\end{lstlisting}

\subsection{Additional Documentation}
More detailed documentation on installation and usage as well as API documentation can be found on our website at \url{http://capstone-coal.github.io/docs}.

\section{Learning New Technologies}

Websites that were helpful included documentation from libraries such as Spectral Python, NumPy, and GDAL as well as references on Python and AVIRIS. Helpful publications included \emph{Python Essential Reference: Fourth Edition} by David M. Beazley, \emph{Mineral Resources Economics and the Environment} by Stephen E. Kesler, and the many research papers cited throughout the project. People on campus who helped with COAL capstone course organization included our professors Kevin McGrath and Kirsten Winters and our teaching assistant Vedanth Narayan. People off campus who contributed data and feedback to COAL included our clients Lewis John McGibbney and Kim Whitehall, Thomas Boggs and other Spectral Python developers, Even Rouault and other GDAL developers, and David R. Thompson and Sarah Lundeen from JPL who helped us acquire AVIRIS imagery.

\section{What We Learned}
% INDIVIDUAL: see brainstorming

% T
\subsection{Taylor}

%What technical information did you learn? %What non-technical information did you learn?

Information I learned in the course of this project included processing hyperspectral imagery, image classification, geographic information systems (GIS), background on mining and its impacts on the environment, and various software libraries and applications. Processing imaging spectrometer data is a niche skill for collaborating with research institutions such as JPL. It was necessary to read multiple research papers and pages of software library documentation to understand and begin to manipulate the data format. Thankfully we were able to leverage much existing code rather than implementing our own data processing routines. Implementing image classification for mineral and mine identification allowed us to review several strategies including machine learning and statistical methods. We benefited from both our own research and feedback from external sources. I was principally responsible for handling the GIS aspects of this project, and I learned a great deal both about backend libraries such as GDAL as well as GIS applications including QGIS, GRASS, and ArcMap. My GDAL bug fix provided a great opportunity to contribute to another Free and Open Source Software library. It was necessary to learn background information on coal mining and its effects on water resources. I was able to draw from some previous geology coursework as well as information provided by the client and available online. Specific software libraries and applications we became most familiar with in the course of this project included NumPy, Spectral Python, GDAL, and QGIS. We also made use of a cloud server and multiple remote collaboration applications. In addition, I learned to acquire data from sources such as the AVIRIS project, the USGS Digital Spectral Library 06, and The National Map.

%What have you learned about project work? %What have you learned about project management?

Project managment techniques employed in this project included issue tracking, a project board, code review, remote work, meeting agendas and notes, and automated testing. Our development team was geographically distributed so we collaborated almost exclusively online. The COAL mailing list was the first source of contact with our client and remained our principal means of coordinating development. Software development was principally managed using Git and the project management tools provided by GitHub and Google Apps. We used an issue tracker to manage implementation and changes, eventually creating and resolving more than 100 separate issues and discussing and merging changes via pull requests. I set up a project board including ``To Do'', ``Blocked'', ``In Progress'', ``Review'', and ``Done'' columns to provide a visualization of our workflow which was reminiscent of Agile procedures I learned in previous work. We followed a Git branching and code review workflow wherever possible to communicate our changes with each other and our client. Our distributed work techniques included setting a regular meeting agenda and taking notes for our video conferences that served as our principal source of face-to-face interaction with our client. And although we did not practice test-driven development precisely, we implemented unit tests to provide a sanity check for our library.

%What have you learned about working in teams? %If you could do it all over, what would you do differently?

Our team evolved according to each member's specific strength. Since this project was managed rather loosely, we had the opportunity to set our own goals but also needed to be proactive in setting our own timeline. The relative freedom permitted by this project allowed our research to proceed in the most natural direction given the data that was available. Previous jobs I have held were much more tightly managed, so this was a welcome change. However, since so much was left up to student developers our success depended on each of us learning how best to collaborate with each other. My strategy of continuous background research and development was successful, however rather than waiting for teammates to implement components individually it might have been better to collaborate more actively at every step of development. I was ultimately able to take pride in my significant contribution to parts of this project, however the burden of implementation and the limited time frame was not without cost. If I were doing it all over again, I would try to find a way to balance my time and take a more active role even earlier in the project. Although we were required to write a large number of documents at the beginning of the project, it might have been preferable to focus on research and development earlier and write our results after we were more familiar with the problem domain.

% H
\subsection{Heidi}
This project was a unique experience for me and I learned a lot from it. My previous work experience while interning at a fairly large company was very different from the project structure we had for COAL. In particular, I think I learned a lot about project leadership and collaboration. Our client steered us in the right direction and did code reviews, but most of the leadership was on us. That means that we had to make sure we were setting up meetings, keeping detailed agendas, and updating our GitHub project board ourselves. These things were generally not my responsibility when I was an intern. 

My experience while interning taught me a lot about developing while working on a team and I think that helped me a lot while working on this project. Code review, very frequent communication, and never being shy about asking teammates for help or opinions are concepts that were very important for the success of this project in my view.

I had previously worked remotely with engineers at another company location, but that was a lot different from the remote work we did here. Before, working remotely was a lot simpler because the engineers and I were at work 8+ hours a day and responses were often fairly fast. Our client is in California and has a lot on his plate besides this project, so keeping in contact with him was difficult when we didn't plan in advance. We were all busy as well because we were taking other classes, so there was sometimes a lot of delay when trying to work with each other too. This made me realize the importance of making sure to get as much as I could out of our weekly meetings. 

I also learned about reaching out to software maintainers. I'm usually pretty reserved so I've never contacted a software maintainer on GitHub before. I learned pretty quickly that I should probably start doing it a lot more. The short correspondence my team and I had with the Spectral Python maintainer was integral to COAL and without it, I doubt our project would have been a success. 

On the technical end, we all had to learn a ton about hyperspectral imagery and imaging spectroscopy. This is a concept that was completely alien to me. We all had to read a lot of research papers on the subject and watch a lot of online lectures about related material. I was already experienced in Python, but I got to use libraries that I have never used before, i.e. NumPy and Spectral Python, so that was a huge learning experience for me. I was also very rusty on using Sphinx for auto-generated documentation, so I definitely got a refresher working on this project.

In terms of what I would do differently, I think that we were often working up until the very last minute to finish documents for class. This added unnecessary stress and I think it would not have been hard to do our work earlier. It taught me a lot about the stress of work deadlines but I don't think it was an ideal experience. I would also offer to help my teammates more if I could do this all over. I'm satisfied with the work that I did, but I feel like I could have reached out more to my teammates when I had free time to help them more. 

% X
\subsection{Xiaomei}

I have learned a lot from this project. For starters, because I have never worked with the concept of the remote sensing technology as well as the image classification, only the touch of the surface could have benefited me vastly.

First, at the beginning of the project, I read a lot of research papers and works on the related field, this includes AVIRIS data guidelines, digital evaluation models, GIS application, monitoring surface mines, mining identification, and remote sensing. Not all the literature I have read played a role in later development; however, they still benefit me. I now have a better understanding of all the information provided about mining.

During the development of the project, I learned a lot about technique side of the project. First, I now understand what is hyperspectral imagery, and what are the differences between the hyperspectral imagery and regular images. We can only use the hyperspectral imagery to aid us during the project development due to its various traits. Second, I learned the concept of the spectrometry, an analytical technique. Third, I learned how flight data was collected (most by NASA using their special devices) and the post-processed procedures. Also, because we use Python for our project, I acquire more knowledge about Python than before.

During the classification of the hyperspectral imagery. We explored various possible techniques, including neural networks and machine learning. I learned a lot of the related theories behind those two methods. I watched some lectures about neural networks online even though we decided to abandon it for our project. 

I have been responsible for our project website for a while during the first phase of the project development. I have to learn about HTML and CSS, even though I have used them before. Also, I used Bootstrap and Jekyll for the first time; this gives a precious experience for me to build a website.

Along the work-flow of our project, I found out that a scientific method to organize the project is a vital part for developers' experience. Even though we haven't strictly stick to one development style like waterfalls, but our work-flow is kind of similar to agile. Our client performed like a project leader, and his experience of development has helped us plenty. I learned the work style from him, and it would definitely benefit me in the future career. The most important thing during a project development is to have a clear schedule and stick to it. We have a project board during our development time; it was new to me and 
it also played a crucial role.

Last, although we never meet our client in person, we have video conferences each week at a fixed time. Those regular meetings really smooth the process of the development, and we have time to get together to find and also solve problems. The good communications are a vital part for us to stay in good shape throughout the project.

If I have a chance to redo our project, there are several aspects I want to approaches differently. First, at the very beginning of the project, when we have no idea what to do, and we have to write all kind of document, we can now know exactly how we are going to approach the problem. Second, We have spent a little bit times on the neural networks because we assumed we are going to use it to classify our images; however, we switch to a statistic method at the middle of the project. Now we know what method we are going to use so it can save us a lot of time. Third, I was responsible for the website at the beginning, if I can do it again, I would know what kind of technique I would use and maybe put more style to it. Fourth, if we can save a lot more time, I can spend more hours on the project and hopefully stick to the time-line better. Last, I want to communicate with my client and teammates better at the first half of our project; I think this would work for me to have a better involvement.

Overall, I think our project is great, and I have learned so much from it. I am glad to be part of it, and I hope I would take what I have learned and use them in the future.

%%%%%%%%%%%%%%%%%%

%    What technical information did you learn?

%% hyperspectral imagery
%%% spectrometry
%%% how data was collected and post-processed (orthorect, scaled refl)
%%% matrix operations
%% classification including
%%% machine learning (MLP NN)
%%% active learning
%%% SAM
%% geographic information systems
%% mining itself -- mining practices, effects of mining (acid mine drainage)
%% specific libraries -- numpy, spy, gdal, ....
%% sphinx + read the docs
%% web: jekyll & bootstrap

%    What non-technical information did you learn?

%    What have you learned about project work?

%    What have you learned about project management?

%% issue tracking
%%% assignee
%%% milestone
%% project board
%%% to do | blocked | in progress | review | done
%% code review
%% meeting agenda & notes ( & board)
%% distributed work
%%% mailing list
%%% github
%%% videoconferences

%% automated testing

%    What have you learned about working in teams?

%    If you could do it all over, what would you do differently?

\section{Appendix I: Code Listings}

% same as midterm report
% copy and paste

The following code snippets are from commit \verb!f81e492c! which was the final version submitted for class. They demonstrate the most important methods for mineral classification, mining identification, and environmental correlation.

\subsection{Mineral Classification}

The \verb!MineralClassification.classifyImage! method in \verb!mineral.py! takes an orthocorrected, scaled reflectance imaging spectrometer file and classifies each pixel using spectral angle mapper classification.

\begin{lstlisting}[firstnumber=57]
def classifyImage(self, imageFilename, classifiedFilename):
    """
    Classify minerals in an AVIRIS image using spectral angle mapper (SAM)
    classification and save the results to a file.

    Args:
        imageFilename (str):      filename of the image to be classified
        classifiedFilename (str): filename of the classified image

    Returns:
        None
    """

    # open the image
    image = spectral.open_image(imageFilename)
    if self.inMemory:
        data = image.load()
    else:
        data = image.asarray()
    M = image.shape[0]
    N = image.shape[1]

    # define a resampler
    # TODO detect and scale units
    # TODO band resampler should do this
    resample = spectral.BandResampler([x/1000 for x in image.bands.centers],
                                      self.library.bands.centers)

    # allocate a zero-initialized MxN array for the classified image
    classified = numpy.zeros(shape=(M,N), dtype=numpy.uint16)

    # for each pixel in the image
    for x in range(M):

        for y in range(N):

            # read the pixel from the file
            pixel = data[x,y]

            # if it is not a no data pixel
            if not numpy.isclose(pixel[0], -0.005) and not pixel[0]==-50:

                # resample the pixel ignoring NaNs from target bands that don't overlap
                # TODO fix spectral library so that bands are in order
                resampledPixel = numpy.nan_to_num(resample(pixel))

                # calculate spectral angles
                angles = spectral.spectral_angles(resampledPixel[numpy.newaxis,
                                                                 numpy.newaxis,
                                                                 ...],
                                                  self.library.spectra)

                # normalize confidence values from [pi,0] to [0,1]
                for z in range(angles.shape[2]):
                    angles[0,0,z] = 1-angles[0,0,z]/math.pi

                # get index of class with largest confidence value
                indexOfMax = numpy.argmax(angles)

                # classify pixel if confidence above threshold
                if angles[0,0,indexOfMax] > self.threshold:

                    # index from one (after zero for no data)
                    classified[x,y] = indexOfMax + 1

    # save the classified image to a file
    spectral.io.envi.save_classification(
        classifiedFilename,
        classified,
        class_names=['No data']+self.library.names,
        metadata={
            'data ignore value': 0,
            'description': 'PyCOAL '+pycoal.version+' mineral classified image.',
            'map info': image.metadata.get('map info')
        })

    # remove unused classes from the image
    pycoal.mineral.MineralClassification.filterClasses(classifiedFilename)
\end{lstlisting}

\subsection{Mining Identification}

The \verb!MiningClassification.classifyImage! method in \verb!mining.py! takes a mineral classified image and selects pixels with classes that are proxies for coal mining waste.

\begin{lstlisting}[firstnumber=36]
def classifyImage(self, imageFilename, classifiedFilename):

    """
    Classify mines or other features in a PyCOAL mineral classified image by
    copying relevant pixels and discarding the rest in a new file.

    Args:
        imageFilename (str):      filename of the image to be classified
        classifiedFilename (str): filename of the classified image

    Returns:
        None
    """

    # open the image
    image = spectral.open_image(imageFilename)
    data = image.asarray()
    M = image.shape[0]
    N = image.shape[1]

    # allocate a zero-initialized MxN array for the classified image
    classified = numpy.zeros(shape=(M,N), dtype=numpy.uint16)

    # get class numbers from names
    classList = image.metadata.get('class names')
    classNums = [classList.index(className) if className in classList else -1 for className in self.classNames]

    # copy pixels of the desired classes
    for y in range(N):
        for x in range(M):
            pixel = data[x,y]
            if pixel[0] in classNums:
                classified[x,y] = 1 + classNums.index(pixel[0])

    # save the classified image to a file
    spectral.io.envi.save_classification(
        classifiedFilename,
        classified,
        class_names=['No data']+self.classNames,
        metadata={
            'data ignore value': 0,
            'description': 'PyCOAL '+pycoal.version+' mining classified image.',
            'map info': image.metadata.get('map info')
        })
\end{lstlisting}

\subsection{Environmental Correlation}

The \verb!EnvironmentalCorrelation.intersectProximity! method in \verb!environment.py! takes a mining classified file and a GIS file such as hydrography data to find mining waste classifications a given distance from streams and water bodies.

\begin{lstlisting}[firstnumber=27]
def intersectProximity(self, miningFilename, vectorFilename, proximity, correlatedFilename):
    """
    Generate an environmental correlation image containing pixels from the
    mining classified image detected within a given distance of features
    within a vector layer.

    Args:
        miningImage (str):     filename of the mining classified image
        vectorLayer (str):     filename of vector layer
        proximity (float):     distance in meters
        correlatedImage (str): filename of the correlated image
    """

    # get path and file names
    outputDirectory = dirname(abspath(correlatedFilename))
    miningName = splitext(basename(abspath(miningFilename)))[0]
    vectorName = splitext(basename(abspath(vectorFilename)))[0]

    # rasterize the vector features to the same dimensions as the mining image
    featureHeaderName = outputDirectory + '/' + miningName + '_' + vectorName + '.hdr'
    self.createEmptyCopy(miningFilename, featureHeaderName)
    featureImageName = featureHeaderName[:-4] + '.img'
    self.rasterize(vectorFilename, featureImageName)

    # generate a proximity map from the features
    proximityHeaderName = outputDirectory + '/' + miningName + '_' + vectorName + '_proximity.hdr'
    proximityImageName = proximityHeaderName[:-4] + '.img'
    self.proximity(featureImageName, proximityImageName)

    # load mining and proximity images and initialize environmental correlation array
    miningImage = spectral.open_image(miningFilename)
    proximityImage = spectral.open_image(proximityHeaderName)
    correlatedImage = numpy.zeros(shape=miningImage.shape, dtype=numpy.uint16)

    # get average pixel size
    if miningImage.metadata.get('map info')[10][-6:].lower() == 'meters':
        xPixelSize = float(miningImage.metadata.get('map info')[5])
        yPixelSize = float(miningImage.metadata.get('map info')[6])
        pixelSize = (xPixelSize + yPixelSize) / 2
    else:
        raise ValueError('Mining image units not in meters.')

    # intersect features within proximity
    for x in range(miningImage.shape[0]):
        for y in range(miningImage.shape[1]):
            if miningImage[x,y,0]==1 and proximityImage[x,y,0]*pixelSize<=proximity:
                correlatedImage[x,y,0] = miningImage[x,y,0]

    # save the environmental correlation image
    spectral.io.envi.save_classification(
        correlatedFilename,
        correlatedImage,
        class_names=miningImage.metadata.get('class names'),
        metadata={
            'data ignore value': 0,
            'description': 'PyCOAL '+pycoal.version+' environmental correlation image.',
            'map info': miningImage.metadata.get('map info')
        })
\end{lstlisting}

\section{Appendix II: Images}

\begin{figure}[H]
  \begin{center}
    \includegraphics[height=0.4\textheight]{2017-03-04-055905_1920x1080_scrot_crop.png}
    \caption{Three-dimensional visualization of hyperspectral imagery displaying spectral bands.}
  \end{center}
\end{figure}

\begin{figure}[H]
  \begin{center}
    \includegraphics[height=0.4\textheight]{2017-04-26-031405_1920x1080_scrot.png}
    \caption{Map of coal mine, waste classifications, water resources, and surrounding geography.}
  \end{center}
\end{figure}

\begin{figure}[H]
  \begin{center}
    \includegraphics[height=0.4\textheight]{2017-04-28-023139_1920x1080_scrot_900x600.png}
    \caption{Distribution of coal mining waste classifications along streams and water bodies.}
  \end{center}
\end{figure}

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=\textwidth]{visible-light.png}
    \caption{Visible-light image.}
  \end{center}
\end{figure}

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=\textwidth]{mineral-classified.png}
    \caption{Mineral classified image.}
  \end{center}
\end{figure}

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=\textwidth]{mining-classified.png}
    \caption{Mining classified image.}
  \end{center}
\end{figure}

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=\textwidth]{environmental-correlation.png}
    \caption{Environmental correlation image.}
  \end{center}
\end{figure}

\begin{thebibliography}{16}

\bibitem{aviris}
  J. A. Benediktsson \emph{et al}, ``\href{https://notendur.hi.is//~benedikt/aviris.pdf}{Classification and Feature Extraction of AVIRIS Data},'' IEEE Trans. on Geoscience and Remote Sensing, Vol. 33, No. 5, September 1995.

\bibitem{autoscaleprinc}
  T. Lindeberg, ``\href{https://people.eecs.berkeley.edu/~malik/cs294/cvap222.pdf}{Principles for Automatic Scale Selection},'' Handbook on Computer Vision and Applications, B. J{\"a}hne, et. all, eds., Academic Press, 1998.
  
\bibitem{lapofgaus}
  D. Matthys. (2001, March 21). \emph{Laplacian of Gaussian Filter} [Online]. Available: \url{http://academic.mu.edu/phys/matthysd/web226/Lab02.htm}

\bibitem{ppt}
  N. Snavely. (2012). \emph{Features 2: Invariance and blob detection} [PowerPoint slides]. Available: \url{www.cs.cornell.edu/courses/cs4670/2012fa/lectures/lec07_features2.ppt}

\bibitem{introarcmap}
  W. Amidon. (2014, March 17). \emph{v25 introduction to classification in ArcMap and ENVI} [YouTube video]. Available: \url{https://www.youtube.com/watch?v=OBJKGWocM6Q}
  
\bibitem{separability}
  W. Amidon. (2014, March 17). \emph{v26 evaluating spectral separability in ENVI} [YouTube video]. Available: \url{https://www.youtube.com/watch?v=y685Km9njys}

\bibitem{requirements}
  T. A. Brown, H. Clayton, and X. Wang, Requirements document, Fall 2016.
  
\bibitem{blobdetection}
  A. Kaspers, ``\href{http://dspace.library.uu.nl/handle/1874/204781}{Blob Detection},'' Master Thesis, Biomedical Image Sci., Image Sci. Inst., UMC Utrecht, Utrecht, Netherlands, 2011.

\bibitem{leadville}
  G. A. Swayze \emph{et al}, ``Using Imaging Spectroscopy to Cost-Effectively Locate Acid-Generating Minerals at Mine Sites: An Example from the California Gulch Superfund Site in Leadville, Colorado,'' paper presented at JPL Airborne Geoscience Workshop, Leadville, Colorado, 1998.

\bibitem{raymine}
  R. N. Clark, \emph{et al}, ``\href{http://speclab.cr.usgs.gov/PAPERS/ray.mine.1.1998/ray.mine.avproc.html}{Mineral Mapping with Imaging Spectroscopy: The Ray Mine, AZ},'' Summaries of the 7th Annual JPL Airborne Earth Science Workshop, R.O. Green, Ed., JPL Publication 97-21. Jan 12-14, pp67-75, 1998.

\bibitem{movingmountaintops}
  D. Nally, ``Moving Mountaintops: Monitoring Surface Mine Expansion and Reclamation Using Landsat Imagery,'' Tufts Univ., Spring 2011.

\bibitem{nomarkdown}
  E. Holscher. (2015, March 15). \emph{Why You Shouldn't Use ``Markdown'' for Documentation} [Online]. Available: \url{http://ericholscher.com/blog/2016/mar/15/dont-use-markdown-for-technical-docs/}

\bibitem{staticsite}
  M. Christensen. (2015, November 16). \emph{Static Website Generators Reviewed: Jekyll, Middleman, Roots, Hugo} [Online]. Available: \url{https://www.smashingmagazine.com/2015/11/static-website-generators-jekyll-middleman-roots-hugo-review/}
  
\bibitem{bootfoundskeleton}  
\emph{Best CSS Frameworks - Bootstrap vs Foundation vs Skeleton?} [Online]. Available: \url{http://customwebsitedevelopement.blogspot.com/2016/01/best-css-frameworks-bootstrap-vs-foundation-vs-skeleton.html}

\bibitem{bootstrapvsfoundation}
  M. Schenker. (2014, September 15). \emph{Bootstrap vs. Foundation: Which Framework is Better?} [Online]. Available: \url{https://bootstrapbay.com/blog/bootstrap-vs-foundation/}
  
\bibitem{dimensionality}
  V. Catterson. (2013, November 27). \emph{Understanding data science: dimensionality reduction with R} [Online]. Available: \url{http://cowlet.org/2013/11/27/understanding-data-science-dimensionality-reduction-with-r.html}

\end{thebibliography}

\end{document}
